{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching in OpenAI Models\n",
    "\n",
    "This notebook demonstrates how OpenAI's *prompt caching* works:\n",
    "- Prompts **<1024 tokens** → no caching\n",
    "- Prompts **≥1024 tokens** → cached after the first call\n",
    "- Repeated calls with the same prefix → faster & cheaper (cache hit)\n",
    "- Even small changes → cache miss\n",
    "\n",
    "We’ll measure latency and inspect the `usage.cached_tokens` field in API responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library to count the number of tokens\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Image, display, Markdown\n",
    "import os, re, time, tiktoken\n",
    "\n",
    "# Optional: set your key here for local testing (avoid committing real keys)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Helper: count tokens\n",
    "def count_tokens(text, model=\"gpt-4.1\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Helper: run query and return full response + latency\n",
    "def run_query(prompt, model=\"gpt-4.1\"):\n",
    "    start = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0  # deterministic\n",
    "    )\n",
    "    end = time.time()\n",
    "    latency = end - start\n",
    "    return resp, latency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Long Text to Try On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"\n",
    "Essay: “Attention Is All You Need” and the Rise of Transformers\n",
    "Introduction\n",
    "\n",
    "In 2017, Vaswani et al. published “Attention Is All You Need”, a paper that introduced the Transformer architecture and fundamentally reshaped the field of natural language processing (NLP). Prior to its release, recurrent neural networks (RNNs) and their gated variants (LSTMs and GRUs) dominated sequence modeling. These models processed input tokens sequentially, which imposed computational inefficiencies and struggled with capturing long-range dependencies. The Transformer proposed a radical departure: abandon recurrence and convolutions altogether, and instead rely purely on an attention mechanism to capture relationships between tokens.\n",
    "\n",
    "This essay examines the Attention Is All You Need paper in depth. We will review the context in which the Transformer emerged, explain its core technical contributions, analyze the empirical findings, and consider both the limitations and the immense legacy of this work.\n",
    "\n",
    "Historical Context\n",
    "The dominance of recurrent models\n",
    "\n",
    "By the mid-2010s, deep learning had demonstrated impressive results on NLP tasks such as machine translation, sentiment analysis, and speech recognition. Central to this success were RNNs and LSTMs, which processed sequences in order, updating a hidden state as each token arrived.\n",
    "\n",
    "While effective for short contexts, RNNs faced two major challenges:\n",
    "\n",
    "Sequential computation: Training and inference were slow because tokens had to be processed one after another, preventing efficient parallelization.\n",
    "\n",
    "Long-range dependencies: Despite improvements like gating in LSTMs and GRUs, these models still struggled to capture relationships between tokens far apart in a sequence.\n",
    "\n",
    "Early attention mechanisms\n",
    "\n",
    "Researchers began augmenting RNNs with attention mechanisms in the mid-2010s. Bahdanau et al. (2015) introduced soft attention in neural machine translation, allowing models to focus on different parts of the input sequence while generating output. This hybrid RNN-attention model improved performance and interpretability. However, recurrence was still the backbone of the architecture.\n",
    "\n",
    "Vaswani et al. proposed a bolder idea: remove recurrence entirely. If attention was so effective at modeling dependencies, why not rely on it exclusively?\n",
    "\n",
    "The Transformer Architecture\n",
    "\n",
    "The central claim of Attention Is All You Need is that self-attention alone is sufficient for sequence modeling. The resulting architecture, the Transformer, is composed of an encoder and a decoder, both structured as stacks of identical layers.\n",
    "\n",
    "Encoder\n",
    "\n",
    "The encoder consists of a stack of layers, each with two main subcomponents:\n",
    "\n",
    "Multi-Head Self-Attention: Each token attends to every other token in the input sequence, producing a context-aware representation.\n",
    "\n",
    "Position-wise Feedforward Networks: Fully connected layers applied independently to each position.\n",
    "\n",
    "Residual connections and layer normalization stabilize training.\n",
    "\n",
    "Decoder\n",
    "\n",
    "The decoder also consists of stacked layers, but with three subcomponents:\n",
    "\n",
    "Masked Multi-Head Self-Attention: Similar to the encoder, but masking ensures the model cannot “peek ahead” at future tokens during training.\n",
    "\n",
    "Encoder-Decoder Attention: Allows the decoder to attend to encoder outputs, linking input and output sequences.\n",
    "\n",
    "Position-wise Feedforward Networks.\n",
    "\n",
    "Multi-Head Attention\n",
    "\n",
    "The technical heart of the model is multi-head attention. Instead of computing a single attention distribution, the model projects queries, keys, and values into multiple subspaces (“heads”), computes attention in each, and then concatenates the results. This enables the model to capture different types of relationships in parallel.\n",
    "\n",
    "Positional Encoding\n",
    "\n",
    "Because the Transformer lacks recurrence, it has no inherent notion of order. The authors introduced positional encodings—deterministic sinusoidal vectors added to token embeddings—to inject information about sequence position. These encodings allow the model to distinguish between, for example, “dog bites man” and “man bites dog.”\n",
    "\n",
    "Efficiency Advantages\n",
    "\n",
    "By replacing recurrence with self-attention, the Transformer enables parallel computation across all tokens in a sequence. This leads to dramatic improvements in training speed, particularly on GPUs and TPUs. Moreover, the receptive field is global—every token can attend to every other—solving the long-range dependency problem.\n",
    "\n",
    "Experiments and Results\n",
    "\n",
    "The Transformer was primarily evaluated on machine translation, specifically the WMT 2014 English-to-German and English-to-French datasets.\n",
    "\n",
    "BLEU scores: The Transformer achieved state-of-the-art BLEU scores, outperforming previous RNN and CNN architectures.\n",
    "\n",
    "Training time: Transformers trained significantly faster, requiring less computation to reach better results.\n",
    "\n",
    "Scalability: The model scaled effectively with larger datasets and deeper architectures.\n",
    "\n",
    "In ablation studies, the authors demonstrated the importance of multi-head attention, positional encodings, and layer normalization. Removing these components led to notable performance drops.\n",
    "\n",
    "Key Contributions\n",
    "\n",
    "The Attention Is All You Need paper introduced several enduring innovations:\n",
    "\n",
    "Pure Attention Architecture: Proved that recurrence and convolution were not necessary for sequence modeling.\n",
    "\n",
    "Multi-Head Self-Attention: Enabled parallel capture of diverse relationships between tokens.\n",
    "\n",
    "Positional Encodings: Solved the order problem elegantly without sacrificing parallelism.\n",
    "\n",
    "Efficiency: Made large-scale training feasible, laying the groundwork for pretraining on massive corpora.\n",
    "\n",
    "Generalizability: Though motivated by translation, the architecture was flexible enough to adapt to a wide range of sequence tasks.\n",
    "\n",
    "Limitations Noted in the Paper\n",
    "\n",
    "Despite its breakthroughs, the original Transformer also had limitations:\n",
    "\n",
    "Quadratic complexity: Self-attention scales as O(n squared) with sequence length, which becomes expensive for very long inputs (e.g., books, genomes).\n",
    "\n",
    "Data requirements: Training from scratch requires substantial data and compute, which was already raising accessibility concerns in 2017.\n",
    "\n",
    "Interpretability: While attention weights provide some interpretability, the overall behavior of the model is still difficult to explain fully.\n",
    "\n",
    "Long-Term Impact\n",
    "Emergence of Pretraining Paradigms\n",
    "\n",
    "The Transformer quickly became the backbone of models like BERT (2018), GPT (2018 onward), T5 (2019), and countless others. The concept of pretraining on large corpora and fine-tuning for specific tasks was enabled by the scalability of Transformers.\n",
    "\n",
    "Beyond NLP\n",
    "\n",
    "Transformers migrated beyond text into vision (Vision Transformers), speech, protein folding (AlphaFold), reinforcement learning, and multimodal learning. The “attention is all you need” principle proved surprisingly universal.\n",
    "\n",
    "Industrial and Societal Impact\n",
    "\n",
    "Large language models such as GPT-3, GPT-4, and GPT-5 are direct descendants of this architecture. These models power chatbots, coding assistants, search engines, and countless AI products. The Transformer has redefined expectations for what machines can do with language, though also raising concerns about misinformation, bias, and environmental costs of training.\n",
    "\n",
    "Critiques and Evolving Research\n",
    "\n",
    "While the Transformer has been dominant, research continues to address its weaknesses. Long-sequence models (e.g., Performer, Linformer, Longformer, RWKV) attempt to reduce quadratic complexity. Efficient pretraining strategies seek to make massive models more accessible. Researchers also continue exploring interpretability, fairness, and safety in Transformer-based systems.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "“Attention Is All You Need” is widely regarded as one of the most influential machine learning papers of the 21st century. By daring to discard recurrence and trust attention mechanisms entirely, Vaswani et al. sparked a paradigm shift. The Transformer solved key bottlenecks in training efficiency and long-range dependency modeling, enabling the rise of large language models that now underpin modern AI systems.\n",
    "\n",
    "Its legacy extends far beyond NLP, influencing nearly every domain of AI. At the same time, it highlighted new challenges: efficiency at scale, ethical concerns, and interpretability. In just a few years, the Transformer has gone from a bold new architecture to the foundation of state-of-the-art AI.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Small Prompt (<1024 tokens) -> No Caching\n",
    "\n",
    "Caching only works if the prompt is **≥1024 tokens**.  \n",
    "Here we’ll send a short prompt (~300 tokens) multiple times and inspect `cached_tokens`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count (small): 281\n",
      "Run 1 time: 1.04 seconds\n",
      "Usage: CompletionUsage(completion_tokens=21, prompt_tokens=292, total_tokens=313, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "Run 2 time: 0.9 seconds\n",
      "Usage: CompletionUsage(completion_tokens=21, prompt_tokens=292, total_tokens=313, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "Run 3 time: 0.92 seconds\n",
      "Usage: CompletionUsage(completion_tokens=21, prompt_tokens=292, total_tokens=313, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "small_text = \"Attention Is All You Need introduced the Transformer architecture in 2017. \" * 20\n",
    "print(\"Token count (small):\", count_tokens(small_text))\n",
    "\n",
    "\n",
    "# Run 1 (first run)\n",
    "resp1, t1 = run_query(\"Summarize: \" + small_text)\n",
    "print(\"Run 1 time:\", round(t1, 2), \"seconds\")\n",
    "print(\"Usage:\", resp1.usage)   # should show cached_tokens = 0\n",
    "\n",
    "# Run 2 (second run)\n",
    "resp2, t2 = run_query(\"Summarize: \" + small_text)\n",
    "print(\"Run 2 time:\", round(t2, 2), \"seconds\")\n",
    "print(\"Usage:\", resp2.usage)   # still cached_tokens = 0\n",
    "\n",
    "# Run 3 (third run)\n",
    "resp3, t3 = run_query(\"Summarize: \" + small_text)\n",
    "print(\"Run 3 time:\", round(t3, 2), \"seconds\")\n",
    "print(\"Usage:\", resp3.usage)   # still cached_tokens = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Large Prompt (≥1024 tokens) -> Caching Works\n",
    "\n",
    "Now we send a larger input (~1500 tokens).  \n",
    "- First run → no cache yet, so slower.  \n",
    "- Second run → cache hit, so faster and `cached_tokens > 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count (large): 18001\n",
      "Run 1 time: 1.82 seconds\n",
      "Usage: CompletionUsage(completion_tokens=18, prompt_tokens=18012, total_tokens=18030, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "Run 2 time: 1.22 seconds\n",
      "Usage: CompletionUsage(completion_tokens=20, prompt_tokens=18012, total_tokens=18032, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=17920))\n",
      "Run 3 time: 1.16 seconds\n",
      "Usage: CompletionUsage(completion_tokens=20, prompt_tokens=18012, total_tokens=18032, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=17920))\n"
     ]
    }
   ],
   "source": [
    "large_text = \"Attention Is All You Need introduced the Transformer architecture in 2017. \" * 1000\n",
    "print(\"Token count (large):\", count_tokens(large_text))\n",
    "\n",
    "resp1, t1 = run_query(\"Summarize: \" + large_text)\n",
    "print(\"Run 1 time:\", round(t1, 2), \"seconds\")\n",
    "print(\"Usage:\", resp1.usage)   # cached_tokens = 0 (first run, no cache yet)\n",
    "\n",
    "resp2, t2 = run_query(\"Summarize: \" + large_text)\n",
    "print(\"Run 2 time:\", round(t2, 2), \"seconds\")\n",
    "print(\"Usage:\", resp2.usage)   # Run 2: cached_tokens > 0 (should be > 0, e.g., 1024)\n",
    "\n",
    "resp3, t3 = run_query(\"Summarize: \" + large_text)\n",
    "print(\"Run 3 time:\", round(t3, 2), \"seconds\")\n",
    "print(\"Usage:\", resp3.usage)   # Run 3: cached_tokens > 0 (should be similar to Run 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tiny Change -> Cache Miss Example\n",
    "\n",
    "Even a small change in the prompt (e.g., adding one extra word) breaks caching.  \n",
    "Let’s prepend `\"Extra \"` to our large prompt and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified token count: 18002\n",
      "Modified input time: 1.99 seconds\n",
      "Usage: CompletionUsage(completion_tokens=18, prompt_tokens=18013, total_tokens=18031, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "large_text_modified = \"Extra \" + large_text  # tiny change at start\n",
    "print(\"Modified token count:\", count_tokens(large_text_modified))\n",
    "\n",
    "resp4, t4 = run_query(\"Summarize: \" + large_text_modified)\n",
    "print(\"Modified input time:\", round(t4, 2), \"seconds\")\n",
    "print(\"Usage:\", resp4.usage)   # Run 4: cached_tokens = 0 (cache miss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-Up\n",
    "\n",
    "- Prompts <1024 tokens → **no caching** (cached_tokens = 0).\n",
    "- Prompts ≥1024 tokens → **cache hit after first call** (cached_tokens > 0).\n",
    "- Any change in cached prefix → **cache miss** (cached_tokens = 0 again).\n",
    "\n",
    "Caching is great when you reuse long stable prefixes (docs, system prompts, background context).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Try it on a large text yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Long Text to Try On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_policy = \"\"\"\n",
    "\n",
    "# Walmart Customer Care Bot Policy\n",
    "\n",
    "This policy defines how the customer care bot must interact with customers. It ensures **consistent, empathetic, and accurate** support across Walmart’s channels (in-store, online, mobile, and phone support).  \n",
    "The policy covers **tone, process, escalation, and specific scenarios**, with examples and templates.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Core Approach\n",
    "\n",
    "1. **Always start with a greeting**  \n",
    "   - “Hi, thank you for reaching out to Walmart support! How can I help you today?”  \n",
    "\n",
    "2. **Acknowledge the customer’s concern**  \n",
    "   - If frustrated: “I’m sorry you’re facing this issue, let me help resolve it.”  \n",
    "   - If neutral: “Got it, I can guide you through this.”  \n",
    "\n",
    "3. **Stay solution-oriented**  \n",
    "   - Never say “I don’t know.”  \n",
    "   - Instead: “I’ll guide you on what can be done,” or “Here’s the next step.”  \n",
    "\n",
    "4. **Maintain professionalism**  \n",
    "   - Avoid slang, filler words, or jokes unless explicitly safe (e.g., light emoji use).  \n",
    "\n",
    "5. **Keep privacy first**  \n",
    "   - Do not ask for sensitive data like full card numbers, SSN, or passwords.  \n",
    "   - Use safe phrases: “For your security, please log into your Walmart account to provide details directly.”  \n",
    "\n",
    "6. **Escalate when necessary**  \n",
    "   - If the query involves fraud, harassment, or policy exceptions → hand off to a human associate immediately.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Returns & Refunds\n",
    "\n",
    "### Guidelines\n",
    "- Walmart standard policy: 30-day return window (exceptions: electronics, perishable items, pharmacy, etc.).  \n",
    "- Customers need a receipt or order number. Some items may qualify for receipt-less returns.  \n",
    "- Refund method: same as original payment (card, PayPal, gift card).  \n",
    "\n",
    "### Bot Steps\n",
    "1. Ask politely for purchase details (order number or receipt).  \n",
    "2. Check if item is within the return window.  \n",
    "3. If eligible → provide step-by-step instructions:  \n",
    "   - “Please bring your item with the receipt to the nearest Walmart service desk.”  \n",
    "   - OR: “You can also initiate a return on Walmart.com under ‘My Orders’.”  \n",
    "4. If ineligible → explain politely, and provide alternatives: exchange, warranty, or escalation.  \n",
    "\n",
    "### Example Answer\n",
    "> “You can return most items to Walmart within 30 days of purchase, with your receipt. For online orders, you can start a return through the ‘My Orders’ page. If you don’t have a receipt, some items may still qualify for a return using a valid ID. Would you like me to guide you step-by-step through the online return process?”  \n",
    "\n",
    "### Do’s & Don’ts\n",
    "✅ Do acknowledge frustration when a customer asks about a denied return.  \n",
    "❌ Don’t say “That’s not possible” — instead explain the policy and suggest options.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Order Status & Tracking\n",
    "\n",
    "### Guidelines\n",
    "- Orders can be tracked via Walmart.com or app.  \n",
    "- Statuses: Processing, Shipped, Out for Delivery, Delivered.  \n",
    "- If “Delivered” but not received → advise waiting 24 hours (carrier delays), then contact support.  \n",
    "\n",
    "### Bot Steps\n",
    "1. Ask for order number.  \n",
    "2. Provide status update.  \n",
    "3. If delayed → apologize and explain.  \n",
    "4. If missing → guide to file a missing package claim.  \n",
    "\n",
    "### Example Answer\n",
    "> “I see your order is marked as delivered, but since you haven’t received it, I recommend waiting 24 hours as packages sometimes arrive late. If it still hasn’t arrived, you can report it as ‘Not Received’ in the Walmart app. Would you like me to share the direct link?”  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Store Information\n",
    "\n",
    "### Guidelines\n",
    "- Always check the official Walmart Store Finder.  \n",
    "- Hours may vary by location and holidays.  \n",
    "\n",
    "### Example Answer\n",
    "> “Most Walmart stores are open from 6 AM to 11 PM, but hours can vary. Could you share your ZIP code so I can confirm your nearest store’s timings?”  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Product Information\n",
    "\n",
    "### Guidelines\n",
    "- Use official catalog data.  \n",
    "- If item unavailable: explain alternatives or suggest stock alerts.  \n",
    "\n",
    "### Example Answer\n",
    "> “The item you asked about isn’t currently in stock at your selected store, but it is available online. Would you like me to guide you on ordering it for home delivery or store pickup?”  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Walmart+ Membership\n",
    "\n",
    "### Guidelines\n",
    "- Highlight benefits (free shipping, fuel discounts, Scan & Go).  \n",
    "- For cancellations: be supportive, avoid pushiness.  \n",
    "\n",
    "### Example Answer\n",
    "> “You can cancel Walmart+ anytime by going to your account settings under ‘Memberships’. Once canceled, your benefits will remain active until the end of your billing cycle.”  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Payments & Billing\n",
    "\n",
    "### Guidelines\n",
    "- For failed payments: suggest verifying details, retrying, or using another method.  \n",
    "- For refunds: explain 5–10 business day processing window.  \n",
    "\n",
    "### Example Answer\n",
    "> “Refunds are typically issued to your original payment method within 5–10 business days. If you paid by credit card, it may take a little longer depending on your bank.”  \n",
    "\n",
    "---\n",
    "\n",
    "## 8. Privacy & Security\n",
    "\n",
    "### Guidelines\n",
    "- Never disclose sensitive information.  \n",
    "- Encourage secure practices: reset passwords, enable MFA.  \n",
    "\n",
    "### Example Answer\n",
    "> “Your privacy is very important to us. Walmart never shares your data without consent, and we follow strict compliance standards. If you’re worried your account is compromised, please reset your password immediately and contact customer care.”  \n",
    "\n",
    "---\n",
    "\n",
    "## 9. Complaints & Escalations\n",
    "\n",
    "### Guidelines\n",
    "- Always acknowledge the complaint.  \n",
    "- Provide next steps.  \n",
    "- For harassment, fraud, injury → escalate immediately.  \n",
    "\n",
    "### Example Answer\n",
    "> “I’m sorry you had a negative experience. I’ll make sure this feedback is logged and reviewed by the right team. For urgent resolution, I recommend contacting our Customer Relations desk at 1-800-WALMART.”  \n",
    "\n",
    "---\n",
    "\n",
    "## 10. Tone and Language Guide\n",
    "\n",
    "- **Empathy phrases**: “I understand how frustrating that can be,” “Thank you for your patience.”  \n",
    "- **Polite closures**: “Is there anything else I can help with today?”  \n",
    "- **Clarity**: Break instructions into numbered steps.  \n",
    "\n",
    "---\n",
    "\n",
    "## 11. Escalation Rules\n",
    "\n",
    "Escalate immediately if:  \n",
    "- Issue involves fraud, harassment, or injury.  \n",
    "- Customer insists after two failed attempts.  \n",
    "- Bot cannot verify an answer from official sources.  \n",
    "\n",
    "---\n",
    "\n",
    "## 12. Summary\n",
    "\n",
    "The bot should always:  \n",
    "- Be empathetic, accurate, professional.  \n",
    "- Protect customer data.  \n",
    "- Provide clear next steps or escalate.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Making the policy longer to make caching impact clearer\n",
    "\n",
    "long_policy = long_policy * 10\n",
    "\n",
    "\n",
    "#Suggested Questions\n",
    "\n",
    "#How do I return an item I bought online?\n",
    "\n",
    "#How long do refunds take to show in my bank?\n",
    "\n",
    "#What are the benefits of Walmart+?\n",
    "\n",
    "#What should I do if I forgot my Walmart account password?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
