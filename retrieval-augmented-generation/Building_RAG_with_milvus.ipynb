{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Building a Retrieval-Augmented Generation (RAG) System with Milvus\n",
        "\n",
        "This repository demonstrates how to build a **Retrieval-Augmented Generation (RAG)** pipeline using **Milvus**, a popular open-source vector database.  \n",
        "\n",
        "## What you‚Äôll learn\n",
        "- How to create and use embeddings for unstructured text.\n",
        "- How to store and query vectors efficiently with Milvus.\n",
        "- How to connect a Large Language Model (LLM) with Milvus to build a RAG system.\n",
        "- How to run simple queries and generate answers grounded in retrieved context.\n",
        "\n",
        "## Why RAG?\n",
        "LLMs are powerful, but they **hallucinate** and have limited knowledge (cutoff dates).  \n",
        "RAG overcomes these challenges by combining:\n",
        "- **Retrieval** ‚Üí pull relevant facts from an external knowledge base.  \n",
        "- **Generation** ‚Üí let the LLM generate natural, contextual answers.  \n",
        "---\n",
        "\n",
        "> ‚ö°Ô∏è By the end, you‚Äôll have a working RAG pipeline that you can adapt for use cases like product search, customer support, or knowledge management.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Installation\n",
        "\n",
        "Install the required dependencies:  \n",
        "- `pymilvus` for interacting with Milvus  \n",
        "- `openai` for embeddings and LLMs  \n",
        "-  supporting libraries for dataset handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NvHPrkL1-3P",
        "outputId": "d606d211-5115-4a55-9e37-7cc1e539f3c5",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# Install necessary packages for Milvus and OpenAI\n",
        "\n",
        "! pip install --upgrade pymilvus openai requests tqdm\n",
        "! pip install 'pymilvus[milvus_lite]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYo5bif_1-3P"
      },
      "source": [
        "Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "81xLEuUg1-3P",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jKI24vV1-3P"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We use the FAQ pages from the [Milvus Documentation 2.4.x](https://github.com/milvus-io/milvus-docs/releases/download/v2.4.6-preview/milvus_docs_2.4.x_en.zip) as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline.\n",
        "\n",
        "These are present in repository at  `data/milvus_docs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZfrnCQs1-3Q"
      },
      "source": [
        "We load all markdown files from the folder. \n",
        "\n",
        "For each document, we just simply use \"# \" to separate the content in the file, which can separate each question-pair. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MO_Do5U11-3Q"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "\n",
        "text_lines = []\n",
        "\n",
        "for file_path in glob(\"data/milvus_docs/*.md\"):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        file_text = file.read()\n",
        "\n",
        "    text_lines += file_text.split(\"# \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Fl4MpW1-3Q"
      },
      "source": [
        "### Prepare the Embedding Model\n",
        "\n",
        "We initialize the OpenAI client to prepare the embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QpmjAwjL1-3Q"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßÆ Creating Embeddings\n",
        "\n",
        "We use OpenAI‚Äôs embedding model to convert each text/document into a high-dimensional vector.  \n",
        "These vectors capture semantic meaning and make similarity search possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O85iLush1-3Q"
      },
      "source": [
        "Define a function to generate text embeddings using OpenAI client. We use the [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) model as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mIRKLxoY1-3Q"
      },
      "outputs": [],
      "source": [
        "def emb_text(text):\n",
        "    return (\n",
        "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
        "        .data[0]\n",
        "        .embedding\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iHcSnsN1-3Q"
      },
      "source": [
        "Generate a test embedding and print its dimension and first few elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFHcunF31-3Q",
        "outputId": "09f7d48c-887c-464e-fe40-ca5e643244e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1536\n",
            "[0.009889289736747742, -0.005578675772994757, 0.00683477520942688, -0.03805781528353691, -0.01824733428657055, -0.04121600463986397, -0.007636285852640867, 0.03225184231996536, 0.018949154764413834, 9.352207416668534e-05]\n"
          ]
        }
      ],
      "source": [
        "test_embedding = emb_text(\"This is a test\")\n",
        "embedding_dim = len(test_embedding)\n",
        "print(embedding_dim)\n",
        "print(test_embedding[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0XtbIZl1-3Q"
      },
      "source": [
        "## üîó Connecting to Milvus\n",
        "\n",
        "Milvus will act as our **vector database**, storing and searching embeddings efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L1JWK6M1-3Q"
      },
      "source": [
        "## üóÇÔ∏è Creating a Collection in Milvus\n",
        "\n",
        "A collection in Milvus is like a table in a relational database.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K1asT8xA1-3R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import DistributionNotFound, get_distribution\n"
          ]
        }
      ],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
        "\n",
        "collection_name = \"my_rag_collection\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "F3f_Icwm1-3R"
      },
      "source": [
        "> As for the argument of `MilvusClient`:\n",
        "> - Setting the `uri` as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
        "> - If you have large scale of data, you can set up a more performant Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
        "> - Alternatively, Element offers a cloud-hosted version of Milvus (note: this option is not available in the Sandbox environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ihboFw1-3R"
      },
      "source": [
        "Check if the collection already exists and drop it if it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OrwQlSVJ1-3R"
      },
      "outputs": [],
      "source": [
        "if milvus_client.has_collection(collection_name):\n",
        "    milvus_client.drop_collection(collection_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcnSoCIM1-3R"
      },
      "source": [
        "Create a new collection with specified parameters.\n",
        "\n",
        "If we don't specify any field information, Milvus will automatically create a default `id` field for primary key, and a `vector` field to store the vector data. A reserved JSON field is used to store non-schema-defined fields and their values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jBDcRPoP1-3R"
      },
      "outputs": [],
      "source": [
        "milvus_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=embedding_dim,\n",
        "    metric_type=\"COSINE\",  # Cosine similarity\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW7BDg3R1-3R"
      },
      "source": [
        "### Generate embeddings and insert data into Milvus\n",
        "Iterate through the text lines, create embeddings, and then insert the data into Milvus.\n",
        "This builds our searchable knowledge base.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFSU3IEk1-3R",
        "outputId": "fb3f7fb9-04b8-4364-a9cc-997c7d468c7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [00:40<00:00,  1.78it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'insert_count': 72, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], 'cost': 0}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n",
        "    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n",
        "\n",
        "milvus_client.insert(collection_name=collection_name, data=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4-Ei9vX1-3R"
      },
      "source": [
        "## Build RAG\n",
        "\n",
        "### Step 1 - Retrieve data for a query\n",
        "\n",
        "Let's specify a frequent question about Milvus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "onqNievq1-3R"
      },
      "outputs": [],
      "source": [
        "question = \"How is data stored in milvus?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTiml8UF1-3R"
      },
      "source": [
        "\n",
        "## üîç Querying Milvus\n",
        "\n",
        "To answer user questions, we:  \n",
        "1. Convert the query into an embedding  \n",
        "2. Search Milvus for the top-k similar vectors  \n",
        "3. Retrieve the most relevant documents  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yak8LzTx1-3R"
      },
      "outputs": [],
      "source": [
        "# üîç Perform similarity search in Milvus to retrieve top-3 matches\n",
        "\n",
        "search_res = milvus_client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[\n",
        "        emb_text(question)\n",
        "    ],  # Use the `emb_text` function to convert the question to an embedding vector\n",
        "    limit=3,  # Return top 3 results\n",
        "    search_params={\"metric_type\": \"COSINE\", \"params\": {}},  # Inner product distance\n",
        "    output_fields=[\"text\"],  # Return the text field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI5x5_vI1-3R"
      },
      "source": [
        "Let's take a look at the search results of the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnfup6cb1-3R",
        "outputId": "df25ae44-4dcd-46d6-c20c-4d4de95e452f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    [\n",
            "        \" Where does Milvus store data?\\n\\nMilvus deals with two types of data, inserted data and metadata. \\n\\nInserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\\n\\nMetadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\\n\\n###\",\n",
            "        0.7826632261276245\n",
            "    ],\n",
            "    [\n",
            "        \"How does Milvus handle vector data types and precision?\\n\\nMilvus supports Binary, Float32, Float16, and BFloat16 vector types.\\n\\n- Binary vectors: Store binary data as sequences of 0s and 1s, used in image processing and information retrieval.\\n- Float32 vectors: Default storage with a precision of about 7 decimal digits. Even Float64 values are stored with Float32 precision, leading to potential precision loss upon retrieval.\\n- Float16 and BFloat16 vectors: Offer reduced precision and memory usage. Float16 is suitable for applications with limited bandwidth and storage, while BFloat16 balances range and efficiency, commonly used in deep learning to reduce computational requirements without significantly impacting accuracy.\\n\\n###\",\n",
            "        0.6772843599319458\n",
            "    ],\n",
            "    [\n",
            "        \"How much does Milvus cost?\\n\\nMilvus is a 100% free open-source project.\\n\\nPlease adhere to [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0) when using Milvus for production or distribution purposes.\\n\\nZilliz, the company behind Milvus, also offers a fully managed cloud version of the platform for those that don't want to build and maintain their own distributed instance. [Zilliz Cloud](https://zilliz.com/cloud) automatically maintains data reliability and allows users to pay only for what they use.\\n\\n###\",\n",
            "        0.6466938853263855\n",
            "    ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "retrieved_lines_with_distances = [\n",
        "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
        "]\n",
        "print(json.dumps(retrieved_lines_with_distances, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjvEX8n61-3R"
      },
      "source": [
        "\n",
        "### Step 2 - Augment original user query with additional context\n",
        "\n",
        "Convert the retrieved documents into a string format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R5mLf6m51-3R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Where does Milvus store data?\n",
            "\n",
            "Milvus deals with two types of data, inserted data and metadata. \n",
            "\n",
            "Inserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\n",
            "\n",
            "Metadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\n",
            "\n",
            "###\n",
            "How does Milvus handle vector data types and precision?\n",
            "\n",
            "Milvus supports Binary, Float32, Float16, and BFloat16 vector types.\n",
            "\n",
            "- Binary vectors: Store binary data as sequences of 0s and 1s, used in image processing and information retrieval.\n",
            "- Float32 vectors: Default storage with a precision of about 7 decimal digits. Even Float64 values are stored with Float32 precision, leading to potential precision loss upon retrieval.\n",
            "- Float16 and BFloat16 vectors: Offer reduced precision and memory usage. Float16 is suitable for applications with limited bandwidth and storage, while BFloat16 balances range and efficiency, commonly used in deep learning to reduce computational requirements without significantly impacting accuracy.\n",
            "\n",
            "###\n",
            "How much does Milvus cost?\n",
            "\n",
            "Milvus is a 100% free open-source project.\n",
            "\n",
            "Please adhere to [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0) when using Milvus for production or distribution purposes.\n",
            "\n",
            "Zilliz, the company behind Milvus, also offers a fully managed cloud version of the platform for those that don't want to build and maintain their own distributed instance. [Zilliz Cloud](https://zilliz.com/cloud) automatically maintains data reliability and allows users to pay only for what they use.\n",
            "\n",
            "###\n"
          ]
        }
      ],
      "source": [
        "context = \"\\n\".join(\n",
        "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
        ")\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia72x7b_1-3R"
      },
      "source": [
        "Define system and user prompts for the Language Model. This prompt is assembled with the retrieved documents from Milvus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nGDjjXzY1-3R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
            "<context>\n",
            " Where does Milvus store data?\n",
            "\n",
            "Milvus deals with two types of data, inserted data and metadata. \n",
            "\n",
            "Inserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\n",
            "\n",
            "Metadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\n",
            "\n",
            "###\n",
            "How does Milvus handle vector data types and precision?\n",
            "\n",
            "Milvus supports Binary, Float32, Float16, and BFloat16 vector types.\n",
            "\n",
            "- Binary vectors: Store binary data as sequences of 0s and 1s, used in image processing and information retrieval.\n",
            "- Float32 vectors: Default storage with a precision of about 7 decimal digits. Even Float64 values are stored with Float32 precision, leading to potential precision loss upon retrieval.\n",
            "- Float16 and BFloat16 vectors: Offer reduced precision and memory usage. Float16 is suitable for applications with limited bandwidth and storage, while BFloat16 balances range and efficiency, commonly used in deep learning to reduce computational requirements without significantly impacting accuracy.\n",
            "\n",
            "###\n",
            "How much does Milvus cost?\n",
            "\n",
            "Milvus is a 100% free open-source project.\n",
            "\n",
            "Please adhere to [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0) when using Milvus for production or distribution purposes.\n",
            "\n",
            "Zilliz, the company behind Milvus, also offers a fully managed cloud version of the platform for those that don't want to build and maintain their own distributed instance. [Zilliz Cloud](https://zilliz.com/cloud) automatically maintains data reliability and allows users to pay only for what they use.\n",
            "\n",
            "###\n",
            "</context>\n",
            "<question>\n",
            "How is data stored in milvus?\n",
            "</question>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ü§ñ Combine retrieved context with LLM to generate final answer\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
        "\"\"\"\n",
        "USER_PROMPT = f\"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\"\n",
        "\n",
        "# Final augmented query to be sent to the LLM\n",
        "print(USER_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6wGgkL1-3R"
      },
      "source": [
        "### Step 3 - Generate response using a Large Language Model\n",
        "Use OpenAI LLM to generate a response based on the prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bW47ieK1-3R",
        "outputId": "b866d56a-4349-4921-9ccc-557bba65a613",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Milvus stores two types of data: inserted data and metadata. \n",
            "\n",
            "Inserted data, which includes vector data, scalar data, and collection-specific schema, is stored in persistent storage as incremental logs. Milvus supports multiple object storage backends for this purpose, including MinIO, AWS S3, Google Cloud Storage, Azure Blob Storage, Alibaba Cloud OSS, and Tencent Cloud Object Storage.\n",
            "\n",
            "Metadata is generated within Milvus and stored in etcd, with each Milvus module having its own specific metadata.\n"
          ]
        }
      ],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "In this notebook, we built a simple yet powerful RAG pipeline:\n",
        "- Stored embeddings in Milvus  \n",
        "- Queried for relevant context  \n",
        "- Used LLM to generate grounded answers  \n",
        "\n",
        "This foundation can be extended to real-world use cases like:\n",
        "- Product search  \n",
        "- Customer support  \n",
        "- Internal knowledge management  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
