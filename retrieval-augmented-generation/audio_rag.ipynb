{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039c00d1",
   "metadata": {},
   "source": [
    "# Audio RAG System with Whisper and Milvus Lite\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system for audio files using:\n",
    "- **Whisper** for audio transcription\n",
    "- **Milvus Lite** for vector storage and retrieval\n",
    "- **OpenAI text-embedding-3-small** for embeddings\n",
    "- **OpenAI GPT** for generation\n",
    "\n",
    "## Features:\n",
    "- Transcribe MP3 audio files using Whisper\n",
    "- Store transcriptions with embeddings in Milvus Lite\n",
    "- Query audio content using RAG\n",
    "- Retrieve relevant audio segments based on queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55cee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (20250625)\n",
      "Requirement already satisfied: pymilvus in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (2.6.2)\n",
      "Requirement already satisfied: openai in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: pydub in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (0.25.1)\n",
      "Requirement already satisfied: ffmpeg in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (1.4)\n",
      "Requirement already satisfied: more-itertools in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai-whisper) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai-whisper) (0.11.0)\n",
      "Requirement already satisfied: torch in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai-whisper) (2.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: setuptools>69 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pymilvus) (80.9.0)\n",
      "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pymilvus) (1.75.0)\n",
      "Requirement already satisfied: protobuf>=5.27.2 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pymilvus) (5.29.5)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pymilvus) (1.0.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pymilvus) (5.11.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pymilvus) (2.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2025.8.29)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: filelock in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from torch->openai-whisper) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from torch->openai-whisper) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Note: ffmpeg must be installed at system level (not via pip)\n",
    "# On macOS: brew install ffmpeg\n",
    "# On Linux: sudo apt-get install ffmpeg\n",
    "# On Windows: Download from https://ffmpeg.org/download.html\n",
    "%pip install openai-whisper pymilvus openai pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc589f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ffmpeg is installed at system level\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def check_ffmpeg():\n",
    "    \"\"\"Check if ffmpeg binary is available in system PATH.\"\"\"\n",
    "    ffmpeg_path = shutil.which(\"ffmpeg\")\n",
    "    if ffmpeg_path:\n",
    "        # Get version to confirm it's working\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ffmpeg\", \"-version\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=5\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                version_line = result.stdout.split('\\n')[0]\n",
    "                print(f\"✓ ffmpeg is installed: {ffmpeg_path}\")\n",
    "                print(f\"  {version_line}\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"✗ ffmpeg found but not working: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"✗ ffmpeg is NOT installed at system level\")\n",
    "        print(\"\\nTo install ffmpeg on macOS:\")\n",
    "        print(\"  1. Install Homebrew if you don't have it: /bin/bash -c \\\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\\\"\")\n",
    "        print(\"  2. Install ffmpeg: brew install ffmpeg\")\n",
    "        print(\"\\nOr on Linux:\")\n",
    "        print(\"  sudo apt-get update && sudo apt-get install -y ffmpeg\")\n",
    "        print(\"\\nOr on Windows:\")\n",
    "        print(\"  Download from https://ffmpeg.org/download.html or use: choco install ffmpeg\")\n",
    "        return False\n",
    "\n",
    "# Check ffmpeg availability\n",
    "has_ffmpeg = check_ffmpeg()\n",
    "if not has_ffmpeg:\n",
    "    print(\"\\n⚠️  Please install ffmpeg before proceeding with audio transcription!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import whisper\n",
    "from pymilvus import MilvusClient\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set up OpenAI API key (set this as an environment variable or replace with your key)\n",
    "# Option 1: Set environment variable before running: export OPENAI_API_KEY=\"your-key-here\"\n",
    "# Option 2: Uncomment and set your API key below:\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9238d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Whisper model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Whisper model\n",
    "print(\"Loading Whisper model...\")\n",
    "whisper_model = whisper.load_model(\"base\")  # Options: tiny, base, small, medium, large\n",
    "print(\"Whisper model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd4b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe audio file\n",
    "def transcribe_audio(audio_path: str, include_timestamps: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribe an audio file using Whisper.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file (MP3, WAV, etc.)\n",
    "        include_timestamps: Whether to include word-level timestamps\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with transcription and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Transcribing: {audio_path}\")\n",
    "    \n",
    "    # Transcribe audio\n",
    "    result = whisper_model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=include_timestamps\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"text\": result[\"text\"],\n",
    "        \"segments\": result.get(\"segments\", []),\n",
    "        \"language\": result.get(\"language\", \"unknown\"),\n",
    "        \"audio_path\": audio_path\n",
    "    }\n",
    "\n",
    "# Test transcription (replace with your audio file path)\n",
    "# audio_file = \"path/to/your/audio.mp3\"\n",
    "# transcription = transcribe_audio(audio_file)\n",
    "# print(f\"Transcription: {transcription['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48525271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text with overlap\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split text into chunks with overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        chunk_size: Size of each chunk in characters\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk_text = text[start:end].strip()\n",
    "        \n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"start_char\": start,\n",
    "                \"end_char\": end,\n",
    "                \"chunk_id\": len(chunks)\n",
    "            })\n",
    "        \n",
    "        # Move start position accounting for overlap\n",
    "        start = end - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example\n",
    "# text = \"This is a long text that needs to be chunked...\"\n",
    "# chunks = chunk_text(text, chunk_size=100, overlap=20)\n",
    "# print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9038e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embeddings using OpenAI\n",
    "def create_embeddings(texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Create embeddings for a list of texts using OpenAI text-embedding-3-small.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        batch_size: Number of texts to process in each batch\n",
    "    \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        print(f\"Creating embeddings for batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Example\n",
    "# texts = [\"Hello world\", \"How are you?\"]\n",
    "# embeddings = create_embeddings(texts)\n",
    "# print(f\"Created {len(embeddings)} embeddings, each with {len(embeddings[0])} dimensions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a304a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for batch 1/1\n",
      "Embedding dimension: 1536\n",
      "Collection audio_rag_collection already exists\n"
     ]
    }
   ],
   "source": [
    "# Initialize Milvus Lite client\n",
    "collection_name = \"audio_rag_collection\"\n",
    "milvus_client = MilvusClient(uri=\"./audio_rag_milvus.db\")\n",
    "\n",
    "# Get embedding dimension (text-embedding-3-small has 1536 dimensions)\n",
    "# Let's create a test embedding to get the dimension\n",
    "test_embedding = create_embeddings([\"test\"])[0]\n",
    "embedding_dim = len(test_embedding)\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "if not milvus_client.has_collection(collection_name):\n",
    "    milvus_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        dimension=embedding_dim,\n",
    "        metric_type=\"COSINE\"\n",
    "    )\n",
    "    print(f\"Created collection: {collection_name}\")\n",
    "else:\n",
    "    print(f\"Collection {collection_name} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89c76387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process and store audio file in Milvus\n",
    "def process_audio_file(audio_path: str, chunk_size: int = 500, overlap: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio, chunk text, create embeddings, and store in Milvus.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        chunk_size: Size of text chunks\n",
    "        overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    # Step 1: Transcribe audio\n",
    "    # Convert to Path object and resolve to absolute path for better reliability\n",
    "    audio_path_obj = Path(audio_path).expanduser().resolve()\n",
    "    \n",
    "    if not audio_path_obj.exists():\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path_obj}\")\n",
    "    \n",
    "    transcription = transcribe_audio(str(audio_path_obj))\n",
    "    print(f\"Transcription completed. Length: {len(transcription['text'])} characters\")\n",
    "    \n",
    "    # Step 2: Chunk the transcription\n",
    "    chunks = chunk_text(transcription['text'], chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Step 3: Create embeddings for chunks\n",
    "    chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = create_embeddings(chunk_texts)\n",
    "    print(f\"Created {len(embeddings)} embeddings\")\n",
    "    \n",
    "    # Step 4: Prepare data for Milvus\n",
    "    audio_filename = audio_path_obj.name\n",
    "    \n",
    "    # Get current collection count to generate unique IDs\n",
    "    # Note: In production, consider using UUID or hash-based IDs\n",
    "    try:\n",
    "        collection_stats = milvus_client.get_collection_stats(collection_name)\n",
    "        next_id = collection_stats.get('row_count', 0)\n",
    "    except:\n",
    "        next_id = 0\n",
    "    \n",
    "    data_to_insert = []\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        data_to_insert.append({\n",
    "            \"id\": next_id + i,\n",
    "            \"vector\": embedding,\n",
    "            \"text\": chunk['text'],\n",
    "            \"audio_file\": audio_filename,\n",
    "            \"chunk_id\": chunk['chunk_id'],\n",
    "            \"start_char\": chunk['start_char'],\n",
    "            \"end_char\": chunk['end_char']\n",
    "        })\n",
    "    \n",
    "    # Step 5: Insert into Milvus\n",
    "    insert_result = milvus_client.insert(\n",
    "        collection_name=collection_name,\n",
    "        data=data_to_insert\n",
    "    )\n",
    "    \n",
    "    print(f\"Inserted {insert_result['insert_count']} chunks into Milvus\")\n",
    "    \n",
    "    return {\n",
    "        \"audio_file\": audio_filename,\n",
    "        \"chunks_count\": len(chunks),\n",
    "        \"insert_count\": insert_result['insert_count'],\n",
    "        \"transcription\": transcription['text']\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = process_audio_file(\"path/to/your/audio.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99ca6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for relevant chunks\n",
    "def search_chunks(query: str, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for relevant chunks using semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant chunks with metadata\n",
    "    \"\"\"\n",
    "    # Create embedding for query\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # Search in Milvus\n",
    "    results = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"text\", \"audio_file\", \"chunk_id\", \"start_char\", \"end_char\"]\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    retrieved_chunks = []\n",
    "    for hits in results:\n",
    "        for hit in hits:\n",
    "            retrieved_chunks.append({\n",
    "                \"text\": hit['entity']['text'],\n",
    "                \"audio_file\": hit['entity']['audio_file'],\n",
    "                \"chunk_id\": hit['entity']['chunk_id'],\n",
    "                \"similarity\": 1 - hit['distance'],  # Convert distance to similarity\n",
    "                \"metadata\": {\n",
    "                    \"start_char\": hit['entity']['start_char'],\n",
    "                    \"end_char\": hit['entity']['end_char']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "# Example\n",
    "# query = \"What was discussed about AI?\"\n",
    "# results = search_chunks(query, top_k=3)\n",
    "# for i, result in enumerate(results, 1):\n",
    "#     print(f\"\\nResult {i} (Similarity: {result['similarity']:.3f}):\")\n",
    "#     print(f\"Audio: {result['audio_file']}\")\n",
    "#     print(f\"Text: {result['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b9a8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to answer questions using RAG\n",
    "def rag_query(question: str, top_k: int = 3, model: str = \"gpt-4o-mini\") -> Dict:\n",
    "    \"\"\"\n",
    "    Answer a question using Retrieval-Augmented Generation.\n",
    "    \n",
    "    Args:\n",
    "        question: Question to answer\n",
    "        top_k: Number of relevant chunks to retrieve\n",
    "        model: OpenAI model to use for generation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and sources\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    retrieved_chunks = search_chunks(question, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Build context from retrieved chunks\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[From {chunk['audio_file']}, Chunk {chunk['chunk_id']}]:\\n{chunk['text']}\"\n",
    "        for chunk in retrieved_chunks\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    prompt = f\"\"\"Use the following pieces of context from audio transcriptions to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using OpenAI\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on audio transcriptions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Step 5: Return answer with sources\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": retrieved_chunks,\n",
    "        \"num_sources\": len(retrieved_chunks)\n",
    "    }\n",
    "\n",
    "# Example\n",
    "# result = rag_query(\"What was the main topic discussed?\")\n",
    "# print(f\"Question: {result['question']}\")\n",
    "# print(f\"\\nAnswer: {result['answer']}\")\n",
    "# print(f\"\\nSources ({result['num_sources']}):\")\n",
    "# for i, source in enumerate(result['sources'], 1):\n",
    "#     print(f\"{i}. {source['audio_file']} (Similarity: {source['similarity']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725dd53",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51357dd",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Now let's use the system to process audio files and query them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "887adb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in /Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages (1.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8597473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: /Users/tanmaydhote/Downloads/1.mp3\n",
      "File exists: True\n",
      "File size: 12.60 MB\n",
      "File is readable: True\n",
      "Transcribing: /Users/tanmaydhote/Downloads/1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription completed. Length: 10261 characters\n",
      "Created 23 chunks\n",
      "Creating embeddings for batch 1/1\n",
      "Created 23 embeddings\n",
      "Inserted 23 chunks into Milvus\n",
      "\n",
      "Processed: 1.mp3\n",
      "Chunks created: 23\n",
      "\n",
      "First 500 characters of transcription:\n",
      " The neat thing about working in machine learning is that every few years, somebody invents something crazy that makes you totally reconsider what's possible. Like models that can play go or generate hyper-realistic faces. And today, the mind-blowing discovery that's rocking everyone's world is a type of neural network called a transformer. Transformers are models that can translate text, write poems and op-eds, and even generate computer code. These are going to be used in biology to solve the \n"
     ]
    }
   ],
   "source": [
    "# First, let's verify the file exists\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "audio_file_path = '/Users/tanmaydhote/Downloads/1.mp3'\n",
    "\n",
    "# Check if file exists\n",
    "audio_path_obj = Path(audio_file_path).expanduser().resolve()\n",
    "print(f\"Checking file: {audio_path_obj}\")\n",
    "print(f\"File exists: {audio_path_obj.exists()}\")\n",
    "if audio_path_obj.exists():\n",
    "    print(f\"File size: {audio_path_obj.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"File is readable: {os.access(audio_path_obj, os.R_OK)}\")\n",
    "    \n",
    "    # Now process it\n",
    "    result = process_audio_file(audio_file_path)\n",
    "    print(f\"\\nProcessed: {result['audio_file']}\")\n",
    "    print(f\"Chunks created: {result['chunks_count']}\")\n",
    "    print(f\"\\nFirst 500 characters of transcription:\")\n",
    "    print(result['transcription'][:500])\n",
    "else:\n",
    "    print(f\"\\n❌ File not found! Please check the path.\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "109fa442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for batch 1/1\n",
      "================================================================================\n",
      "Question: What are transformers?\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "Transformers are a type of neural network architecture that are very effective for analyzing complicated data types like images, videos, audio, and text. They were developed in 2017 by researchers at Google and the University of Toronto, initially designed for translation tasks. Unlike recurrent neural networks, transformers can be efficiently parallelized, allowing for the training of very large models. They are the basis for popular machine learning models like BERT, GPT-3, and T5.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sources (3):\n",
      "================================================================================\n",
      "\n",
      "[1] 1.mp3 (Similarity: 0.392)\n",
      "    hese are going to be used in biology to solve the protein-folding problem. Transformers are like this magical machine learning hammer that seems to make every problem into an owl. If you've heard of t...\n",
      "\n",
      "[2] 1.mp3 (Similarity: 0.395)\n",
      "    going to tell you about what transformers are, how they work, and why they've been so impactful. Let's get to it. So what is a transformer? It's a type of neural network architecture. To recap, neural...\n",
      "\n",
      "[3] 1.mp3 (Similarity: 0.488)\n",
      "    it on all that much data. This is where the transformer changed everything. They were a model developed in 2017 by researchers at Google and the University of Toronto, and they were initially designed...\n"
     ]
    }
   ],
   "source": [
    "# Query the audio RAG system\n",
    "question = \"What are transformers?\"\n",
    "\n",
    "result = rag_query(question, top_k=3)\n",
    "# \n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nSources ({result['num_sources']}):\")\n",
    "print(\"=\" * 80)\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "     print(f\"\\n[{i}] {source['audio_file']} (Similarity: {source['similarity']:.3f})\")\n",
    "     print(f\"    {source['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple audio files from a directory\n",
    "def process_audio_directory(directory_path: str, chunk_size: int = 500, overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Process all audio files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing audio files\n",
    "        chunk_size: Size of text chunks\n",
    "        overlap: Overlap between chunks\n",
    "    \"\"\"\n",
    "    audio_extensions = {'.mp3', '.wav', '.m4a', '.ogg', '.flac', '.mp4'}\n",
    "    directory = Path(directory_path)\n",
    "    \n",
    "    audio_files = [f for f in directory.iterdir() if f.suffix.lower() in audio_extensions]\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} audio files\")\n",
    "    \n",
    "    results = []\n",
    "    for audio_file in audio_files:\n",
    "        try:\n",
    "            result = process_audio_file(str(audio_file), chunk_size=chunk_size, overlap=overlap)\n",
    "            results.append(result)\n",
    "            print(f\"✓ Processed: {audio_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {audio_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal processed: {len(results)}/{len(audio_files)}\")\n",
    "    return results\n",
    "\n",
    "# Example\n",
    "# audio_directory = \"/path/to/audio/files\"\n",
    "# results = process_audio_directory(audio_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59569d1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete Audio RAG system:\n",
    "\n",
    "### Workflow:\n",
    "1. **Transcription**: Use Whisper to transcribe MP3 (or other audio) files to text\n",
    "2. **Chunking**: Split transcriptions into smaller, overlapping chunks for better retrieval\n",
    "3. **Embedding**: Generate embeddings using OpenAI `text-embedding-3-small` model\n",
    "4. **Storage**: Store embeddings and metadata in Milvus Lite vector database\n",
    "5. **Retrieval**: Search for relevant chunks using cosine similarity\n",
    "6. **Generation**: Use OpenAI GPT to generate answers based on retrieved context\n",
    "\n",
    "### Key Functions:\n",
    "- `transcribe_audio()`: Transcribe audio files using Whisper\n",
    "- `chunk_text()`: Split text into chunks with overlap\n",
    "- `create_embeddings()`: Generate embeddings using OpenAI\n",
    "- `process_audio_file()`: Complete pipeline to process and store audio\n",
    "- `search_chunks()`: Retrieve relevant chunks for a query\n",
    "- `rag_query()`: Answer questions using RAG\n",
    "\n",
    "### Features:\n",
    "- Supports multiple audio formats (MP3, WAV, M4A, OGG, FLAC, MP4)\n",
    "- Batch processing of multiple audio files\n",
    "- Semantic search with similarity scores\n",
    "- Source attribution for answers\n",
    "- No LangChain dependency - pure OpenAI and Milvus Lite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dc01d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1105ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
