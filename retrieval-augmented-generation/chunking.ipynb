{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649555f0",
   "metadata": {},
   "source": [
    "# Different types of chunking methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240eaae5",
   "metadata": {},
   "source": [
    "## Level 1 - Character Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f39d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2643794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba6c97",
   "metadata": {},
   "source": [
    "## Level 1.1 - Character Splitting with Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997d9128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'like to chunk up. It is the example',\n",
       " 'he example text for this exercise',\n",
       " 'exercise']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "overlap = 10    # Overlap in characters\n",
    "\n",
    "# Run through the range with the length of your text and iterate every (chunk_size - overlap)\n",
    "for i in range(0, len(text), chunk_size - overlap):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0195c0",
   "metadata": {},
   "source": [
    "## Level 2 - Fixed-length chunks (word level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8668789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I',\n",
       " 'would like to chunk up.',\n",
       " 'It is the example text',\n",
       " 'for this exercise']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# Split the text into units (words, in this case)\n",
    "def word_splitter(source_text: str) -> List[str]:\n",
    "    import re\n",
    "    source_text = re.sub(\"\\s+\", \" \", source_text)  # Replace multiple whitespces\n",
    "    return re.split(\"\\s\", source_text)  # Split by single whitespace\n",
    "\n",
    "def get_chunks_fixed_size(text: str, chunk_size: int) -> List[str]:\n",
    "    text_words = word_splitter(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(text_words), chunk_size):\n",
    "        chunk_words = text_words[i: i + chunk_size]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "get_chunks_fixed_size(text, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0823b",
   "metadata": {},
   "source": [
    "## Level 2 - Fixed-length chunks (word level) with overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022e1e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I',\n",
       " 'text I would like to',\n",
       " 'like to chunk up. It',\n",
       " 'up. It is the example',\n",
       " 'the example text for this',\n",
       " 'for this exercise']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# Split the text into units (words, in this case)\n",
    "def word_splitter(source_text: str) -> List[str]:\n",
    "    import re\n",
    "    source_text = re.sub(\"\\s+\", \" \", source_text)  # Replace multiple whitespaces\n",
    "    return re.split(\"\\s\", source_text)  # Split by single whitespace\n",
    "\n",
    "def get_chunks_fixed_size_with_overlap(text: str, chunk_size: int, overlap_fraction: float) -> List[str]:\n",
    "    text_words = word_splitter(text)\n",
    "    overlap_int = int(chunk_size * overlap_fraction)\n",
    "    if overlap_int >= chunk_size:\n",
    "        raise ValueError(\"overlap must be less than chunk_size\")\n",
    "    step = chunk_size - overlap_int\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"overlap_fraction too large for given chunk_size\")\n",
    "    chunks = []\n",
    "    for i in range(0, len(text_words), step):\n",
    "        chunk_words = text_words[i: i + chunk_size]\n",
    "        if chunk_words:  # Only add non-empty chunks\n",
    "            chunk = \" \".join(chunk_words)\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "get_chunks_fixed_size_with_overlap(text, 5, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc563d7",
   "metadata": {},
   "source": [
    "## Level 3 - Sentence Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1722bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: This is the first sentence. This is the second sentence. This is the third sentence.\n",
      "Chunk 2: This is the third sentence. This is the fourth sentence. This is the fifth sentence.\n",
      "Chunk 3: This is the fifth sentence.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sentence_chunker(text, max_sentences=3, overlap_sentences=1):\n",
    "    \"\"\"\n",
    "    Split text into chunks based on sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        max_sentences (int): Maximum sentences per chunk\n",
    "        overlap_sentences (int): Number of sentences to overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    if max_sentences <= 0:\n",
    "        raise ValueError(\"max_sentences must be positive\")\n",
    "    if overlap_sentences < 0:\n",
    "        raise ValueError(\"overlap_sentences must be non-negative\")\n",
    "    if overlap_sentences >= max_sentences:\n",
    "        raise ValueError(\"overlap_sentences must be less than max_sentences\")\n",
    "\n",
    "    # Split text into sentences using regex\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = max_sentences - overlap_sentences\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"overlap_sentences must be less than max_sentences\")\n",
    "\n",
    "    while start < len(sentences):\n",
    "        end = min(start + max_sentences, len(sentences))\n",
    "        chunk_sentences = sentences[start:end]\n",
    "        chunk = '. '.join(chunk_sentences) + '.'\n",
    "        chunks.append(chunk)\n",
    "        start += step  # Move start forward by step size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "text = \"This is the first sentence. This is the second sentence. This is the third sentence. This is the fourth sentence. This is the fifth sentence.\"\n",
    "chunks = sentence_chunker(text, max_sentences=3, overlap_sentences=1)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ededfd",
   "metadata": {},
   "source": [
    "## Level 4 - Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12367d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Artificial intelligence (AI) is transforming industries worldwide. From healthcare to finance, AI-driven solutions are improving efficiency and accuracy.\n",
      "\n",
      "Chunk 2:\n",
      "However, ethical considerations remain crucial. As technology evolves, so must our understanding of its impact. This text is provided to test recursive chunking functions.\n",
      "\n",
      "Chunk 3:\n",
      "In recent years, machine learning models have achieved remarkable results in natural language processing, computer vision, and robotics. Despite these advances, challenges such as data privacy, algorithmic bias, and transparency persist.\n",
      "\n",
      "Chunk 4:\n",
      "Addressing these issues is essential for building trust in AI systems and ensuring their responsible deployment across society.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def recursive_chunking(text: str, max_chunk_size: int = 1000) -> List[str]:\n",
    "    # Base case: if text is small enough, return as single chunk\n",
    "    if len(text) <= max_chunk_size:\n",
    "        return [text.strip()] if text.strip() else []\n",
    "    \n",
    "    # Try separators in priority order\n",
    "    separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    \n",
    "    for separator in separators:\n",
    "        if separator in text:\n",
    "            parts = text.split(separator)\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for part in parts:\n",
    "                # Check if adding this part would exceed the limit\n",
    "                test_chunk = current_chunk + separator + part if current_chunk else part\n",
    "                \n",
    "                if len(test_chunk) <= max_chunk_size:\n",
    "                    current_chunk = test_chunk\n",
    "                else:\n",
    "                    # Save current chunk and start new one\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    current_chunk = part\n",
    "            \n",
    "            # Add the final chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Recursively process any chunks that are still too large\n",
    "            final_chunks = []\n",
    "            for chunk in chunks:\n",
    "                if len(chunk) > max_chunk_size:\n",
    "                    final_chunks.extend(recursive_chunking(chunk, max_chunk_size))\n",
    "                else:\n",
    "                    final_chunks.append(chunk)\n",
    "            \n",
    "            return [chunk for chunk in final_chunks if chunk]\n",
    "    \n",
    "    # Fallback: split by character limit if no separators work\n",
    "    return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming industries worldwide. From healthcare to finance, AI-driven solutions are improving efficiency and accuracy.\n",
    "\n",
    "However, ethical considerations remain crucial. As technology evolves, so must our understanding of its impact. This text is provided to test recursive chunking functions.\n",
    "\n",
    "In recent years, machine learning models have achieved remarkable results in natural language processing, computer vision, and robotics. Despite these advances, challenges such as data privacy, algorithmic bias, and transparency persist.\n",
    "\n",
    "Addressing these issues is essential for building trust in AI systems and ensuring their responsible deployment across society.\n",
    "\"\"\"\n",
    "\n",
    "chunks = recursive_chunking(sample_text, max_chunk_size=500)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6fd6d6",
   "metadata": {},
   "source": [
    "## Level 5 - Document-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36bd1bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction\\nThis is the introduction section.',\n",
       " '## Background\\nSome background information.',\n",
       " '### Details\\nMore detailed information.',\n",
       " '# Conclusion\\nFinal thoughts.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def markdown_document_chunking(text: str) -> List[str]:\n",
    "    # Split by markdown headers (# ## ### etc.)\n",
    "    header_pattern = r'^#{1,6}\\s+.+$'\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if this line is a header\n",
    "        if re.match(header_pattern, line, re.MULTILINE):\n",
    "            # Save previous chunk if it has content\n",
    "            if current_chunk:\n",
    "                chunk_text = '\\n'.join(current_chunk).strip()\n",
    "                if chunk_text:\n",
    "                    chunks.append(chunk_text)\n",
    "            \n",
    "            # Start new chunk with this header\n",
    "            current_chunk = [line]\n",
    "        else:\n",
    "            # Add line to current chunk\n",
    "            current_chunk.append(line)\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = '\\n'.join(current_chunk).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "# Introduction\n",
    "This is the introduction section.\n",
    "\n",
    "## Background\n",
    "Some background information.\n",
    "\n",
    "### Details\n",
    "More detailed information.\n",
    "\n",
    "# Conclusion\n",
    "Final thoughts.\n",
    "\"\"\"\n",
    "\n",
    "markdown_document_chunking(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45994f9b",
   "metadata": {},
   "source": [
    "# Exercises: Advanced Chunking Techniques\n",
    "\n",
    "## Exercise 1: LLM-based Chunking\n",
    "\n",
    "Use an LLM to determine optimal chunk boundaries in a text.\n",
    "\n",
    "**Example Prompt:**\n",
    "\n",
    "```python\n",
    "boundary_prompt = f\"\"\"\n",
    "Analyze the following text and identify the best places to split it into chunks of approximately {chunk_size} characters.\n",
    "\n",
    "Consider:\n",
    "- Semantic coherence (keep related ideas together)\n",
    "- Context preservation (maintain meaning across chunks)\n",
    "- Natural boundaries (paragraphs, topics, concepts)\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Return only the character positions where splits should occur, separated by commas.\n",
    "Example: 150, 300, 450\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Semantic Chunking\n",
    "\n",
    "Implement a function that splits a text into chunks based on semantic similarity.  \n",
    "For example, you can use sentence embeddings to group sentences into semantically coherent chunks.\n",
    "\n",
    "**Example (pseudo-code):**\n",
    "\n",
    "```python\n",
    "def semantic_chunking(text, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Split the text into semantically coherent chunks, each up to max_chunk_size characters.\n",
    "    Use sentence embeddings to group similar sentences together.\n",
    "    Return a list of text chunks.\n",
    "    \"\"\"\n",
    "    # 1. Split text into sentences\n",
    "    # 2. Compute embeddings for each sentence\n",
    "    # 3. Group sentences into chunks based on similarity and size constraint\n",
    "    # 4. Return list of chunks\n",
    "    pass\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
