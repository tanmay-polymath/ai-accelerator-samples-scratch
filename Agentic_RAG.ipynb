{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68VkzKGGOeTc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5bN4DHj3k_8",
        "outputId": "5462a728-6024-4b65-847f-9535ea4604df"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVcOVZ53z_gD"
      },
      "outputs": [],
      "source": [
        "#Setting up LLM Provider\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"add your key here\"\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P3FqSgo2aYH"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Download the transformer paper - #!wget \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" -O transformer.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOCRhgPq2bXe"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"transformer.pdf\"]).load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dqEOG4K4DVD"
      },
      "source": [
        "## Define the LLM and Embedding Model\n",
        "Discuss how to plug in models from Element Gateway here.\n",
        "Supported providers - https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu5yQ5l04HHD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4o\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYVm4X3r7wrz"
      },
      "source": [
        "## Define Summary Index and Vector Index on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dPj7U4P76du"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96TsuP8OaNl"
      },
      "source": [
        "#Simple RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFM2TgqNPeTL",
        "outputId": "01d1f3dd-680e-4a5d-a5ca-999ab0c380bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-attention, also known as intra-attention, is a mechanism that relates different positions of a single sequence to compute a representation of that sequence. It allows the model to focus on different parts of the input sequence when computing a representation for a particular position, enabling the model to capture dependencies regardless of their distance in the sequence. This mechanism is integral to the Transformer model, which relies entirely on self-attention to compute input and output representations without using recurrent or convolutional layers.\n",
            "\n",
            "Regarding the training data, the Transformer model was evaluated on machine translation tasks, specifically the WMT 2014 English-to-German and English-to-French translation tasks. The model achieved state-of-the-art results on these tasks, demonstrating its effectiveness in sequence transduction.\n"
          ]
        }
      ],
      "source": [
        "query_engine_simple = vector_index.as_query_engine(\n",
        "    similarity_top_k=5,          # tweak how many chunks come back\n",
        "    # any other kwargsâ€¦\n",
        ")\n",
        "\n",
        "resp = query_engine_simple.query(\"Tell me what self attention is and then tell me about the training data also\")\n",
        "print(resp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X0PgFEtz91g"
      },
      "source": [
        "# Agentic RAG - Going beyond simple retrieval and generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaqeiqA98MEM"
      },
      "source": [
        "## Define Query Engine and Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOQYBzo88IDo"
      },
      "outputs": [],
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to the Transformer paper\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for answering specific questions from the Transformer paper.\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMKgVA79jzj"
      },
      "source": [
        "## Define Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4UmT9Q99oSW"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWfLehuG9q9Q",
        "outputId": "a28614cd-0de6-4f08-ca58-911b119bf798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which aligns with the purpose of choice 1, as it is useful for summarization questions..\n",
            "\u001b[0mThe document introduces the Transformer, a novel neural network architecture for sequence transduction tasks, which relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional networks. The Transformer model demonstrates superior performance in machine translation tasks, achieving state-of-the-art results with improved parallelization and reduced training time. The architecture consists of an encoder-decoder structure with multi-head self-attention and feed-forward layers. The document details the model's components, training process, and advantages over traditional models, highlighting its efficiency and effectiveness in handling long-range dependencies. The Transformer sets new benchmarks in translation quality while significantly reducing computational costs.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUbY1IGl93qf",
        "outputId": "829c0d21-1030-4bf7-a3bb-af9561d0d5da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        }
      ],
      "source": [
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmnoKVka97JU",
        "outputId": "f317d17c-287c-4b98-a903-6e0bab4831e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'What is the training data?' is specific and likely pertains to details found in the MetaGPT paper, making choice 2 the most relevant..\n",
            "\u001b[0mThe training data consists of the WMT 2014 English-German dataset with about 4.5 million sentence pairs and the WMT 2014 English-French dataset with 36 million sentences. The English-German sentences were encoded using byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset used a 32,000 word-piece vocabulary.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"What is the training data?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287Iy3UD_o90"
      },
      "source": [
        "## Adding Reasoning Loop to the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZuO96dL_tGI",
        "outputId": "a764eb88-e06f-41b3-9bbb-784e7e74f4b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-16-2277309365.py:8: DeprecationWarning: Call to deprecated class AgentRunner. (AgentRunner has been deprecated and is not maintained.\n",
            "\n",
            "This implementation will be removed in a v0.13.0.\n",
            "\n",
            "See the docs for more information on updated agent usage: https://docs.llamaindex.ai/en/stable/understanding/agent/)\n",
            "  agent = AgentRunner(agent_worker)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [vector_tool, summary_tool],\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgZfCEhB_8Lj",
        "outputId": "12fd1772-078f-4787-8038-5b42ae8d2294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me what self attention is\n",
            "    and then why is it important.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"What is self-attention in the context of the Transformer model?\"}\n",
            "=== Function Output ===\n",
            "Self-attention, in the context of the Transformer model, is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution. This mechanism enables the model to attend to all positions in the sequence simultaneously, facilitating parallelization and improving the ability to learn long-range dependencies.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"Why is self-attention important in the Transformer model?\"}\n",
            "=== Function Output ===\n",
            "Self-attention is crucial in the Transformer model because it allows the model to draw global dependencies between input and output sequences without relying on recurrence or convolution. This mechanism enables the model to process sequences in parallel, significantly improving computational efficiency and reducing training time. Additionally, self-attention facilitates learning long-range dependencies by connecting all positions in a sequence with a constant number of operations, making it easier to capture relationships between distant elements in the input and output sequences. This contributes to the superior performance of the Transformer in tasks like machine translation.\n",
            "=== LLM Response ===\n",
            "Self-attention, in the context of the Transformer model, is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution. This mechanism enables the model to attend to all positions in the sequence simultaneously, facilitating parallelization and improving the ability to learn long-range dependencies.\n",
            "\n",
            "Self-attention is crucial in the Transformer model because it allows the model to draw global dependencies between input and output sequences without relying on recurrence or convolution. This mechanism enables the model to process sequences in parallel, significantly improving computational efficiency and reducing training time. Additionally, self-attention facilitates learning long-range dependencies by connecting all positions in a sequence with a constant number of operations, making it easier to capture relationships between distant elements in the input and output sequences. This contributes to the superior performance of the Transformer in tasks like machine translation.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"\"\"Tell me what self attention is\n",
        "    and then why is it important.\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrtBx1lnAYoF",
        "outputId": "65a2f442-e1e7-4b9d-8103-f10d4992070b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the training data used.\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"What training data is used for the Transformer model?\"}\n",
            "=== Function Output ===\n",
            "The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which consists of 36 million sentences.\n",
            "=== LLM Response ===\n",
            "The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which consists of 36 million sentences.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"Tell me about the training data used.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPb9NKlzA5yQ",
        "outputId": "b130c54c-148f-4320-ac92-c118ddfb35d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: How was the batching done?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"How is batching done in the training of the Transformer model?\"}\n",
            "=== Function Output ===\n",
            "In the training of the Transformer model, sentence pairs are batched together by approximate sequence length. Each training batch contains a set of sentence pairs with approximately 25,000 source tokens and 25,000 target tokens.\n",
            "=== LLM Response ===\n",
            "In the training of the Transformer model, sentence pairs are batched together by approximate sequence length. Each training batch contains a set of sentence pairs with approximately 25,000 source tokens and 25,000 target tokens.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"How was the batching done?\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
