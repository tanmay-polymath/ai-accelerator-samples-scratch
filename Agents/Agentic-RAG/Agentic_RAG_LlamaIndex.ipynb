{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68VkzKGGOeTc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w5bN4DHj3k_8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.14.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index-core<0.15.0,>=0.14.8 (from llama-index)\n",
            "  Downloading llama_index_core-0.14.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.6.9-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (3.12.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.6.7)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting eval-type-backport<0.3,>=0.2.0 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.27.2)\n",
            "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading llama_index_workflows-2.11.2-py3-none-any.whl.metadata (766 bytes)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (3.2.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (10.4.0)\n",
            "Requirement already satisfied: platformdirs in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (2.9.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib/python3.9/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.8->llama-index) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.9/site-packages (from llama-index-core<0.15.0,>=0.14.8->llama-index) (0.9.0)\n",
            "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading wrapt-2.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (3.1.6)\n",
            "Requirement already satisfied: openai>=1.1.0 in ./.venv/lib/python3.9/site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (2.2.0)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in ./.venv/lib/python3.9/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.14.2)\n",
            "Collecting defusedxml>=0.7.1 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting pandas<2.3.0 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
            "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.9/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pydantic>=2.8.0 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (3.10)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.8->llama-index) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.8->llama-index) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (0.7.0)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading pydantic_core-2.41.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting typing-extensions>=4.5.0 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading wrapt-1.17.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.81-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.81 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.81-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click<9,>=8.1.7 in ./.venv/lib/python3.9/site-packages (from llama-cloud-services>=0.6.81->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.1.8)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.80-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.80 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.80-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.79-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.79 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.79-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.78-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.78 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.78-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.77-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.77 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.77-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in ./.venv/lib/python3.9/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
            "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.9/site-packages (from nltk>3.8.1->llama-index) (2025.8.29)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (2.5.0)\n",
            "Collecting greenlet>=1 (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Downloading greenlet-3.2.4-cp39-cp39-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.8->llama-index) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.8->llama-index) (24.2)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.8->llama-index)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.8->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.14.8-py3-none-any.whl (7.4 kB)\n",
            "Downloading llama_index_core-0.14.8-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_openai-0.6.9-py3-none-any.whl (26 kB)\n",
            "Downloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
            "Downloading llama_index_workflows-2.11.2-py3-none-any.whl (89 kB)\n",
            "Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m924.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m388.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "Downloading wrapt-1.17.3-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
            "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
            "Downloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp39-cp39-macosx_11_0_universal2.whl (269 kB)\n",
            "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: striprtf, dirtyjson, wrapt, typing-extensions, nltk, greenlet, eval-type-backport, defusedxml, colorama, typing-inspection, pypdf, pydantic-core, pandas, griffe, deprecated, aiosqlite, pydantic, llama-index-instrumentation, banks, llama-index-workflows, llama-cloud, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-readers-llama-parse, llama-index\n",
            "\u001b[2K  Attempting uninstall: typing-extensions\n",
            "\u001b[2K    Found existing installation: typing_extensions 4.14.0\n",
            "\u001b[2K    Uninstalling typing_extensions-4.14.0:\n",
            "\u001b[2K      Successfully uninstalled typing_extensions-4.14.0━━━━━━━━━━━\u001b[0m \u001b[32m 3/30\u001b[0m [typing-extensions]\n",
            "\u001b[2K  Attempting uninstall: typing-inspection━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]ions]\n",
            "\u001b[2K    Found existing installation: typing-inspection 0.4.1━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K    Uninstalling typing-inspection-0.4.1:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K      Successfully uninstalled typing-inspection-0.4.1━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K  Attempting uninstall: pydantic-core━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K    Found existing installation: pydantic_core 2.23.4━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K    Uninstalling pydantic_core-2.23.4:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K      Successfully uninstalled pydantic_core-2.23.4━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [colorama]\n",
            "\u001b[2K  Attempting uninstall: pandasm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/30\u001b[0m [pydantic-core]\n",
            "\u001b[2K    Found existing installation: pandas 2.3.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/30\u001b[0m [pydantic-core]\n",
            "\u001b[2K    Uninstalling pandas-2.3.0:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/30\u001b[0m [pandas]re]\n",
            "\u001b[2K      Successfully uninstalled pandas-2.3.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/30\u001b[0m [pandas]\n",
            "\u001b[2K  Attempting uninstall: pydantic0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/30\u001b[0m [griffe]\n",
            "\u001b[2K    Found existing installation: pydantic 2.9.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/30\u001b[0m [griffe]\n",
            "\u001b[2K    Uninstalling pydantic-2.9.2:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/30\u001b[0m [griffe]\n",
            "\u001b[2K      Successfully uninstalled pydantic-2.9.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/30\u001b[0m [griffe]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [llama-index]\u001b[0m [llama-index-readers-file]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiosqlite-0.21.0 banks-2.2.0 colorama-0.4.6 defusedxml-0.7.1 deprecated-1.2.18 dirtyjson-1.0.8 eval-type-backport-0.2.2 greenlet-3.2.4 griffe-1.14.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.8 llama-index-core-0.14.8 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.9 llama-index-readers-file-0.5.4 llama-index-readers-llama-parse-0.5.1 llama-index-workflows-2.11.2 llama-parse-0.6.54 nltk-3.9.2 pandas-2.2.3 pydantic-2.12.4 pydantic-core-2.41.5 pypdf-6.3.0 striprtf-0.0.26 typing-extensions-4.15.0 typing-inspection-0.4.2 wrapt-1.17.3\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVcOVZ53z_gD"
      },
      "outputs": [],
      "source": [
        "#Setting up LLM Provider\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" #Add your OpenAI API key here\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P3FqSgo2aYH"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Download the transformer paper - #!wget \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" -O transformer.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Logging in LlamaIndex\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# Method 1: Configure Python logging module (recommended)\n",
        "logging.basicConfig(\n",
        "    stream=sys.stdout,\n",
        "    level=logging.INFO,  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Method 2: Set specific logger levels for llama-index components\n",
        "# Get the llama_index logger\n",
        "llama_logger = logging.getLogger(\"llama_index\")\n",
        "llama_logger.setLevel(logging.DEBUG)  # Set to DEBUG for more detailed logs\n",
        "\n",
        "# Optional: Set level for specific components\n",
        "# logging.getLogger(\"llama_index.core.embeddings\").setLevel(logging.DEBUG)\n",
        "# logging.getLogger(\"llama_index.core.llms\").setLevel(logging.DEBUG)\n",
        "# logging.getLogger(\"llama_index.core.agent\").setLevel(logging.DEBUG)\n",
        "\n",
        "# Method 3: Enable verbose mode in components (already using this in some places)\n",
        "# verbose=True parameter in agents, query engines, etc.\n",
        "\n",
        "print(\"Logging configured! You'll now see detailed logs from llama-index.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oOCRhgPq2bXe"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf\"]).load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dqEOG4K4DVD"
      },
      "source": [
        "## Define the LLM and Embedding Model\n",
        "Discuss how to plug in models from Element Gateway here.\n",
        "Supported providers - https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/\n",
        "\n",
        "We will have to plug in models from the Element Gateway here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mu5yQ5l04HHD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tanmaydhote/Documents/src/ai-accelerator-walmart-scratch/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4o\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYVm4X3r7wrz"
      },
      "source": [
        "## Define Summary Index and Vector Index on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0dPj7U4P76du"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 18:54:20,551 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96TsuP8OaNl"
      },
      "source": [
        "# Simple RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFM2TgqNPeTL",
        "outputId": "01d1f3dd-680e-4a5d-a5ca-999ab0c380bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 18:54:28,593 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-11-18 18:54:34,575 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation of that sequence. It allows the model to focus on different parts of the input sequence when computing a representation, which is particularly useful for capturing dependencies regardless of their distance in the sequence.\n",
            "\n",
            "Regarding the training data, the model was trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens. For English-French, the significantly larger WMT 2014 English-French dataset was used, consisting of 36 million sentences, with tokens split into a 32,000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length, with each training batch containing approximately 25,000 source tokens and 25,000 target tokens.\n"
          ]
        }
      ],
      "source": [
        "query_engine_simple = vector_index.as_query_engine(\n",
        "    similarity_top_k=5,          # tweak how many chunks come back\n",
        "    # any other kwargs…\n",
        ")\n",
        "\n",
        "resp = query_engine_simple.query(\"Tell me what self attention is and then tell me about the training data also\")\n",
        "print(resp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X0PgFEtz91g"
      },
      "source": [
        "# Agentic RAG - Going beyond simple retrieval and generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaqeiqA98MEM"
      },
      "source": [
        "## Define Query Engine and Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TOQYBzo88IDo"
      },
      "outputs": [],
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to the Transformer paper\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for answering specific questions from the Transformer paper.\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMKgVA79jzj"
      },
      "source": [
        "## Define Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m4UmT9Q99oSW"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWfLehuG9q9Q",
        "outputId": "a28614cd-0de6-4f08-ca58-911b119bf798"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 18:54:46,605 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-18 18:54:46,615 - INFO - Selecting query engine 0: The question asks for a summary of the document, which aligns with choice 1, as it is useful for summarization questions related to the Transformer paper..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which aligns with choice 1, as it is useful for summarization questions related to the Transformer paper..\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 18:54:53,479 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The document introduces the Transformer, a novel neural network architecture for sequence transduction tasks, which relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional networks. The Transformer model is shown to be more efficient and parallelizable, achieving superior performance in machine translation tasks compared to previous models. It establishes new state-of-the-art BLEU scores on the WMT 2014 English-to-German and English-to-French translation tasks. The architecture consists of an encoder-decoder structure using multi-head self-attention and feed-forward layers, allowing for significant parallelization and improved learning of long-range dependencies. The document also discusses the advantages of self-attention over traditional recurrent and convolutional layers, including computational efficiency and the ability to model dependencies regardless of their distance in the input or output sequences. The paper concludes with a discussion on the potential of attention-based models for various tasks beyond text translation.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUbY1IGl93qf",
        "outputId": "829c0d21-1030-4bf7-a3bb-af9561d0d5da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        }
      ],
      "source": [
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmnoKVka97JU",
        "outputId": "f317d17c-287c-4b98-a903-6e0bab4831e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 09:13:08,783 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 09:13:08,807 - INFO - Selecting query engine 1: The question 'What is the training data?' is a specific question related to the Transformer paper, making choice 2 the most relevant..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'What is the training data?' is a specific question related to the Transformer paper, making choice 2 the most relevant..\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 09:13:09,746 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 09:13:12,197 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The training data consists of the WMT 2014 English-German dataset with about 4.5 million sentence pairs and the WMT 2014 English-French dataset with 36 million sentences. The English-German sentences were encoded using byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset used a 32,000 word-piece vocabulary.\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"What is the training data?\"\n",
        ")\n",
        "print(str(response))\n",
        "\n",
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287Iy3UD_o90"
      },
      "source": [
        "## Adding Reasoning Loop to the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nZuO96dL_tGI"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionAgent\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[vector_tool, summary_tool],\n",
        "    verbose=True,\n",
        ")\n",
        "chat_memory = ChatMemoryBuffer.from_defaults()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 12:02:06,558 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 12:02:08,600 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 12:02:09,840 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 12:02:10,741 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESPONSE CONTENT:\n",
            "============================================================\n",
            "**Self-Attention:**\n",
            "\n",
            "Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution.\n",
            "\n",
            "**Importance:**\n",
            "\n",
            "This mechanism is important because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Additionally, self-attention helps in learning long-range dependencies more effectively, as it connects all positions with a constant number of sequential operations, reducing the path length between dependencies. This makes it particularly advantageous for tasks like machine translation, where understanding the context across the entire sequence is crucial.\n",
            "\n",
            "**Training Data:**\n",
            "\n",
            "The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which is significantly larger with 36 million sentences. The English-German dataset uses byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.\n",
            "\n",
            "============================================================\n",
            "RESPONSE OBJECT PROPERTIES:\n",
            "============================================================\n",
            "Response type: <class 'llama_index.core.agent.workflow.workflow_events.AgentOutput'>\n",
            "Agent name: Agent\n",
            "Response message: assistant: **Self-Attention:**\n",
            "\n",
            "Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution.\n",
            "\n",
            "**Importance:**\n",
            "\n",
            "This mechanism is important because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Additionally, self-attention helps in learning long-range dependencies more effectively, as it connects all positions with a constant number of sequential operations, reducing the path length between dependencies. This makes it particularly advantageous for tasks like machine translation, where understanding the context across the entire sequence is crucial.\n",
            "\n",
            "**Training Data:**\n",
            "\n",
            "The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which is significantly larger with 36 million sentences. The English-German dataset uses byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.\n",
            "Response content: **Self-Attention:**\n",
            "\n",
            "Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution.\n",
            "\n",
            "**Importance:**\n",
            "\n",
            "This mechanism is important because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Additionally, self-attention helps in learning long-range dependencies more effectively, as it connects all positions with a constant number of sequential operations, reducing the path length between dependencies. This makes it particularly advantageous for tasks like machine translation, where understanding the context across the entire sequence is crucial.\n",
            "\n",
            "**Training Data:**\n",
            "\n",
            "The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which is significantly larger with 36 million sentences. The English-German dataset uses byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.\n",
            "============================================================\n",
            "Number of tool calls: 2\n",
            "Tool calls: [ToolCallResult(tool_name='query_engine_tool', tool_kwargs={'input': 'What training data is used in the Transformer model?'}, tool_id='call_07Y5V2Q0sg3NuPRxuZMb0h8e', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which is significantly larger with 36 million sentences. The English-German dataset uses byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.')], tool_name='query_engine_tool', raw_input={'input': 'What training data is used in the Transformer model?'}, raw_output=Response(response='The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which is significantly larger with 36 million sentences. The English-German dataset uses byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.', source_nodes=[NodeWithScore(node=TextNode(id_='0c073a71-88b8-4d34-8fa0-5f6cb07aeb75', embedding=None, metadata={'page_label': '1', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e27e7b3f-db6d-4885-b4cd-d3094116290a', node_type='4', metadata={'page_label': '1', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='a293f1ac26e9d7c7d77c783e66ee0c81dfeb7fbfcd0b2c0369623597582c9c72')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', mimetype='text/plain', start_char_idx=0, end_char_idx=2909, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='5b7422bd-27b3-4754-b256-f22a8e2368f3', embedding=None, metadata={'page_label': '2', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cfb8e353-7c13-4ce6-9fef-5251c6f6e4f9', node_type='4', metadata={'page_label': '2', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='535a410e626a4a2506b2e25992dcf2b82cece743bc6b0944f8a4d5ea8f41c370')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2', mimetype='text/plain', start_char_idx=0, end_char_idx=4251, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='7403fe4c-a856-4053-bb59-132c8be59e46', embedding=None, metadata={'page_label': '3', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f9c9f7e-3cc4-4650-82cf-21b233e914f3', node_type='4', metadata={'page_label': '3', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='c8b571d729dd799f784c4afcf30a1873d6e14175d56e02aa501e550ee73963d9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=1753, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='41ab7da7-24f9-4f37-a82e-4e17e197ee56', embedding=None, metadata={'page_label': '4', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a4b00977-40cf-49d3-a660-02a8a5751529', node_type='4', metadata={'page_label': '4', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='9ca9bf72b4e59b7b5d3e308635bc85ba4f9c15a97dac6c2b7f5085f962063236')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4', mimetype='text/plain', start_char_idx=0, end_char_idx=2442, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='cc5d08d5-4b51-4b85-a4f1-c56d098ec42e', embedding=None, metadata={'page_label': '5', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b57134cf-7502-459c-b8f8-da36c2c46207', node_type='4', metadata={'page_label': '5', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='d542bbb710ad00bd6f543c7b10aa4b69af02fbdd223d00140418e7a63c3074e4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5', mimetype='text/plain', start_char_idx=0, end_char_idx=3194, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='aa344e3e-7acd-4f32-8482-4934760a6bbb', embedding=None, metadata={'page_label': '6', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8e3d41ab-a95a-471f-b797-c99d4f883348', node_type='4', metadata={'page_label': '6', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='64b7609b6ce0923e260622c893a15f45c14ee05462fa804a5ea2d217896738c7')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', mimetype='text/plain', start_char_idx=0, end_char_idx=3523, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='e3ca3ad8-14f4-489a-b586-bbf98c8f9a15', embedding=None, metadata={'page_label': '7', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2e05aff-8c34-4ea7-990c-cc888a1c6a85', node_type='4', metadata={'page_label': '7', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='011c174370c15f8e65d7771e542b4299e9d912c6e7ee77e5b5106bfcdda36bbe')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7', mimetype='text/plain', start_char_idx=0, end_char_idx=3213, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='c89736df-fe4c-4049-baf7-3462cee98f76', embedding=None, metadata={'page_label': '8', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d9d37f6b-efa4-4a14-9c14-c02a7fa43309', node_type='4', metadata={'page_label': '8', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='cce76a141dbd27ea747063c4bb60e153ccf9623d65acb77eed40f6d318d22053')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', mimetype='text/plain', start_char_idx=0, end_char_idx=3313, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='f07502c5-0c7f-49ff-a6f3-33c33b94ce50', embedding=None, metadata={'page_label': '9', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='763884bd-45d3-41db-a901-73e66f5abc65', node_type='4', metadata={'page_label': '9', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='32333432e933783fd1527e90f854d09192bbcb240bbea505c9e349bae027bbee')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', mimetype='text/plain', start_char_idx=0, end_char_idx=2619, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='6dd0d448-e931-416e-b4a0-1ee78c013949', embedding=None, metadata={'page_label': '10', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8e05738a-ce7f-444f-a701-09e4819ca952', node_type='4', metadata={'page_label': '10', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='acdfa79d633de8f35ba356bf7a7c9aeb3710424b97bc649ef3a24821772a0c91')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10', mimetype='text/plain', start_char_idx=0, end_char_idx=3089, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='30456b5f-9c0a-4fec-84d8-6864b163d43d', embedding=None, metadata={'page_label': '11', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2b6c34ff-e877-4a9d-a00d-ae2aaae5f182', node_type='4', metadata={'page_label': '11', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='348fdf40e814ded61c7431337e0faf2017e23b14f94e00bd765aa113d2640c56')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11', mimetype='text/plain', start_char_idx=0, end_char_idx=2325, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)], metadata={'0c073a71-88b8-4d34-8fa0-5f6cb07aeb75': {'page_label': '1', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '5b7422bd-27b3-4754-b256-f22a8e2368f3': {'page_label': '2', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '7403fe4c-a856-4053-bb59-132c8be59e46': {'page_label': '3', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '41ab7da7-24f9-4f37-a82e-4e17e197ee56': {'page_label': '4', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'cc5d08d5-4b51-4b85-a4f1-c56d098ec42e': {'page_label': '5', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'aa344e3e-7acd-4f32-8482-4934760a6bbb': {'page_label': '6', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'e3ca3ad8-14f4-489a-b586-bbf98c8f9a15': {'page_label': '7', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'c89736df-fe4c-4049-baf7-3462cee98f76': {'page_label': '8', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'f07502c5-0c7f-49ff-a6f3-33c33b94ce50': {'page_label': '9', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '6dd0d448-e931-416e-b4a0-1ee78c013949': {'page_label': '10', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '30456b5f-9c0a-4fec-84d8-6864b163d43d': {'page_label': '11', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}}), is_error=False), return_direct=False), ToolCallResult(tool_name='query_engine_tool', tool_kwargs={'input': 'What is self-attention in the Transformer model and why is it important?'}, tool_id='call_ZROcCbD4kcxI87NqdQwoNzeu', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution. This mechanism is important because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Additionally, self-attention helps in learning long-range dependencies more effectively, as it connects all positions with a constant number of sequential operations, reducing the path length between dependencies. This makes it particularly advantageous for tasks like machine translation, where understanding the context across the entire sequence is crucial.')], tool_name='query_engine_tool', raw_input={'input': 'What is self-attention in the Transformer model and why is it important?'}, raw_output=Response(response='Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution. This mechanism is important because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Additionally, self-attention helps in learning long-range dependencies more effectively, as it connects all positions with a constant number of sequential operations, reducing the path length between dependencies. This makes it particularly advantageous for tasks like machine translation, where understanding the context across the entire sequence is crucial.', source_nodes=[NodeWithScore(node=TextNode(id_='0c073a71-88b8-4d34-8fa0-5f6cb07aeb75', embedding=None, metadata={'page_label': '1', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e27e7b3f-db6d-4885-b4cd-d3094116290a', node_type='4', metadata={'page_label': '1', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='a293f1ac26e9d7c7d77c783e66ee0c81dfeb7fbfcd0b2c0369623597582c9c72')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', mimetype='text/plain', start_char_idx=0, end_char_idx=2909, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='5b7422bd-27b3-4754-b256-f22a8e2368f3', embedding=None, metadata={'page_label': '2', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cfb8e353-7c13-4ce6-9fef-5251c6f6e4f9', node_type='4', metadata={'page_label': '2', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='535a410e626a4a2506b2e25992dcf2b82cece743bc6b0944f8a4d5ea8f41c370')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2', mimetype='text/plain', start_char_idx=0, end_char_idx=4251, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='7403fe4c-a856-4053-bb59-132c8be59e46', embedding=None, metadata={'page_label': '3', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f9c9f7e-3cc4-4650-82cf-21b233e914f3', node_type='4', metadata={'page_label': '3', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='c8b571d729dd799f784c4afcf30a1873d6e14175d56e02aa501e550ee73963d9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=1753, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='41ab7da7-24f9-4f37-a82e-4e17e197ee56', embedding=None, metadata={'page_label': '4', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a4b00977-40cf-49d3-a660-02a8a5751529', node_type='4', metadata={'page_label': '4', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='9ca9bf72b4e59b7b5d3e308635bc85ba4f9c15a97dac6c2b7f5085f962063236')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4', mimetype='text/plain', start_char_idx=0, end_char_idx=2442, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='cc5d08d5-4b51-4b85-a4f1-c56d098ec42e', embedding=None, metadata={'page_label': '5', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b57134cf-7502-459c-b8f8-da36c2c46207', node_type='4', metadata={'page_label': '5', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='d542bbb710ad00bd6f543c7b10aa4b69af02fbdd223d00140418e7a63c3074e4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5', mimetype='text/plain', start_char_idx=0, end_char_idx=3194, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='aa344e3e-7acd-4f32-8482-4934760a6bbb', embedding=None, metadata={'page_label': '6', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8e3d41ab-a95a-471f-b797-c99d4f883348', node_type='4', metadata={'page_label': '6', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='64b7609b6ce0923e260622c893a15f45c14ee05462fa804a5ea2d217896738c7')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', mimetype='text/plain', start_char_idx=0, end_char_idx=3523, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='e3ca3ad8-14f4-489a-b586-bbf98c8f9a15', embedding=None, metadata={'page_label': '7', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2e05aff-8c34-4ea7-990c-cc888a1c6a85', node_type='4', metadata={'page_label': '7', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='011c174370c15f8e65d7771e542b4299e9d912c6e7ee77e5b5106bfcdda36bbe')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7', mimetype='text/plain', start_char_idx=0, end_char_idx=3213, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='c89736df-fe4c-4049-baf7-3462cee98f76', embedding=None, metadata={'page_label': '8', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d9d37f6b-efa4-4a14-9c14-c02a7fa43309', node_type='4', metadata={'page_label': '8', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='cce76a141dbd27ea747063c4bb60e153ccf9623d65acb77eed40f6d318d22053')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', mimetype='text/plain', start_char_idx=0, end_char_idx=3313, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='f07502c5-0c7f-49ff-a6f3-33c33b94ce50', embedding=None, metadata={'page_label': '9', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='763884bd-45d3-41db-a901-73e66f5abc65', node_type='4', metadata={'page_label': '9', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='32333432e933783fd1527e90f854d09192bbcb240bbea505c9e349bae027bbee')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', mimetype='text/plain', start_char_idx=0, end_char_idx=2619, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='6dd0d448-e931-416e-b4a0-1ee78c013949', embedding=None, metadata={'page_label': '10', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8e05738a-ce7f-444f-a701-09e4819ca952', node_type='4', metadata={'page_label': '10', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='acdfa79d633de8f35ba356bf7a7c9aeb3710424b97bc649ef3a24821772a0c91')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10', mimetype='text/plain', start_char_idx=0, end_char_idx=3089, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='30456b5f-9c0a-4fec-84d8-6864b163d43d', embedding=None, metadata={'page_label': '11', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2b6c34ff-e877-4a9d-a00d-ae2aaae5f182', node_type='4', metadata={'page_label': '11', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, hash='348fdf40e814ded61c7431337e0faf2017e23b14f94e00bd765aa113d2640c56')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11', mimetype='text/plain', start_char_idx=0, end_char_idx=2325, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)], metadata={'0c073a71-88b8-4d34-8fa0-5f6cb07aeb75': {'page_label': '1', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '5b7422bd-27b3-4754-b256-f22a8e2368f3': {'page_label': '2', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '7403fe4c-a856-4053-bb59-132c8be59e46': {'page_label': '3', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '41ab7da7-24f9-4f37-a82e-4e17e197ee56': {'page_label': '4', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'cc5d08d5-4b51-4b85-a4f1-c56d098ec42e': {'page_label': '5', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'aa344e3e-7acd-4f32-8482-4934760a6bbb': {'page_label': '6', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'e3ca3ad8-14f4-489a-b586-bbf98c8f9a15': {'page_label': '7', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'c89736df-fe4c-4049-baf7-3462cee98f76': {'page_label': '8', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, 'f07502c5-0c7f-49ff-a6f3-33c33b94ce50': {'page_label': '9', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '6dd0d448-e931-416e-b4a0-1ee78c013949': {'page_label': '10', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}, '30456b5f-9c0a-4fec-84d8-6864b163d43d': {'page_label': '11', 'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_path': '/Users/tanmaydhote/Downloads/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2025-11-18', 'last_modified_date': '2025-11-18'}}), is_error=False), return_direct=False)]\n",
            "  Tool 1: query_engine_tool with args: {'input': 'What training data is used in the Transformer model?'}\n",
            "  Tool output: The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which is significantly larger with 36 million sentences. The English-German dataset uses byte-pair encoding with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.\n",
            "  Tool 2: query_engine_tool with args: {'input': 'What is self-attention in the Transformer model and why is it important?'}\n",
            "  Tool output: Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution. This mechanism is important because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Additionally, self-attention helps in learning long-range dependencies more effectively, as it connects all positions with a constant number of sequential operations, reducing the path length between dependencies. This makes it particularly advantageous for tasks like machine translation, where understanding the context across the entire sequence is crucial.\n",
            "\n",
            "============================================================\n",
            "HANDLER INFORMATION:\n",
            "============================================================\n",
            "Handler type: <class 'workflows.handler.WorkflowHandler'>\n",
            "Handler context available: True\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# You can also access memory to see the full conversation\\nprint(\"=\" * 60)\\nprint(\"CONVERSATION HISTORY (from memory):\")\\nprint(\"=\" * 60)\\nmessages = await chat_memory.aget()\\nfor i, msg in enumerate(messages):\\n    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\\n'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"Tell me what self attention is and then why is it important. Also what is the training data?\",\n",
        "    memory=chat_memory,\n",
        ")\n",
        "response = await handler\n",
        "\n",
        "# Explore what's available in the response object\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE CONTENT:\")\n",
        "print(\"=\" * 60)\n",
        "print(str(response))  # or response.response.content\n",
        "print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE OBJECT PROPERTIES:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Response type: {type(response)}\")\n",
        "print(f\"Agent name: {response.current_agent_name}\")\n",
        "print(f\"Response message: {response.response}\")\n",
        "print(f\"Response content: {response.response.content}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Number of tool calls: {len(response.tool_calls)}\")\n",
        "if response.tool_calls:\n",
        "    print(f\"Tool calls: {response.tool_calls}\")\n",
        "    for i, tool_call in enumerate(response.tool_calls):\n",
        "        print(f\"  Tool {i+1}: {tool_call.tool_name} with args: {tool_call.tool_kwargs}\")\n",
        "        if hasattr(tool_call, \"tool_output\"):\n",
        "            print(f\"  Tool output: {tool_call.tool_output}\")\n",
        "print()\n",
        "\n",
        "# Access the handler to see execution details\n",
        "print(\"=\" * 60)\n",
        "\"\"\"\n",
        "print(\"HANDLER INFORMATION:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Handler type: {type(handler)}\")\n",
        "print(f\"Handler context available: {hasattr(handler, 'ctx')}\")\n",
        "print()\n",
        "\n",
        "\n",
        "# You can also access memory to see the full conversation\n",
        "print(\"=\" * 60)\n",
        "print(\"CONVERSATION HISTORY (from memory):\")\n",
        "print(\"=\" * 60)\n",
        "messages = await chat_memory.aget()\n",
        "for i, msg in enumerate(messages):\n",
        "    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgZfCEhB_8Lj",
        "outputId": "12fd1772-078f-4787-8038-5b42ae8d2294"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 11:55:30,431 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 11:55:34,209 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 11:55:36,820 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Self-Attention:**\n",
            "\n",
            "Self-attention in the Transformer model is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It allows the model to draw global dependencies between input and output without relying on sequence-aligned recurrence or convolution. \n",
            "\n",
            "**Importance:**\n",
            "\n",
            "This mechanism is crucial because it enables the model to process all positions in the sequence simultaneously, which significantly enhances parallelization and computational efficiency. Self-attention also facilitates learning long-range dependencies by connecting all positions with a constant number of operations, making it easier to capture relationships between distant elements in the sequence. This contributes to the model's superior performance in tasks like machine translation.\n"
          ]
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"\"\"Tell me what self attention is\n",
        "    and then why is it important.\"\"\",\n",
        "    memory=chat_memory,\n",
        "    verbose=True\n",
        ")\n",
        "response = await handler\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrtBx1lnAYoF",
        "outputId": "65a2f442-e1e7-4b9d-8103-f10d4992070b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 19:02:40,132 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-18 19:02:43,183 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-18 19:02:43,689 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Transformer model is trained on the WMT 2014 English-German dataset, which consists of about 4.5 million sentence pairs, and the WMT 2014 English-French dataset, which consists of 36 million sentences. For the English-German dataset, byte-pair encoding is used with a shared source-target vocabulary of about 37,000 tokens, while the English-French dataset uses a 32,000 word-piece vocabulary.\n"
          ]
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"Tell me about the training data used.\",\n",
        "    memory=chat_memory,\n",
        ")\n",
        "response = await handler\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPb9NKlzA5yQ",
        "outputId": "b130c54c-148f-4320-ac92-c118ddfb35d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 19:02:57,544 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-18 19:03:00,279 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-18 19:03:00,872 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the Transformer model, batching is done by grouping sentence pairs together based on approximate sequence length. Each training batch contains a set of sentence pairs with approximately 25,000 source tokens and 25,000 target tokens. This approach helps in efficiently utilizing computational resources during training.\n"
          ]
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"How was the batching done?\",\n",
        "    memory=chat_memory,\n",
        ")\n",
        "response = await handler\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: abstract all of this into a function that takes in a PDF file name \n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "# def get_doc_tools(\n",
        "#     file_path: str,\n",
        "#     name: str,\n",
        "# ) -> str:\n",
        "#     \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "    \n",
        "#     # load documents\n",
        "#     documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "#     splitter = SentenceSplitter(chunk_size=1024)\n",
        "#     nodes = splitter.get_nodes_from_documents(documents)\n",
        "#     vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "#     def vector_query(\n",
        "#         query: str, \n",
        "#         filter_key_list: List[str],\n",
        "#         filter_value_list: List[str]\n",
        "#     ) -> str:\n",
        "#         \"\"\"Perform a vector search over an index.\n",
        "\n",
        "#         query (str): the string query to be embedded.\n",
        "#         filter_key_list (List[str]): A list of metadata filter field names\n",
        "#             Must specify ['page_label'] or empty list. Please leave empty\n",
        "#             if there are no explicit filters to specify.\n",
        "#         filter_value_list (List[str]): List of metadata filter field values \n",
        "#             (corresponding to names specified in filter_key_list) \n",
        "\n",
        "#         \"\"\"\n",
        "#         metadata_dicts = [\n",
        "#             {\"key\": k, \"value\": v} for k, v in zip(filter_key_list, filter_value_list)\n",
        "#         ]\n",
        "\n",
        "#         query_engine = vector_index.as_query_engine(\n",
        "#             similarity_top_k=2,\n",
        "#             filters=MetadataFilters.from_dicts(metadata_dicts)\n",
        "#         )\n",
        "#         response = query_engine.query(query)\n",
        "#         return response\n",
        "\n",
        "#     vector_query_tool = FunctionTool.from_defaults(\n",
        "#         fn=vector_query,\n",
        "#         name=f\"vector_query_{name}\"\n",
        "#     )\n",
        "\n",
        "#     summary_index = SummaryIndex(nodes)\n",
        "#     summary_query_engine = summary_index.as_query_engine(\n",
        "#         response_mode=\"tree_summarize\",\n",
        "#         use_async=True,\n",
        "#     )\n",
        "#     summary_tool = QueryEngineTool.from_defaults(\n",
        "#         query_engine=summary_query_engine,\n",
        "#         name=f\"summary_query_{name}\",\n",
        "#         description=(\n",
        "#             f\"Useful for summarization questions related to {name}\"\n",
        "#         ),\n",
        "#     )\n",
        "#     return vector_query_tool, summary_tool\n",
        "\n",
        "\n",
        "\n",
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "\n",
        "    # load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "    \n",
        "    def vector_query(\n",
        "        query: str,\n",
        "    ) -> str:\n",
        "        \n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "        return response\n",
        "        \n",
        "    \n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{name}\",\n",
        "        fn=vector_query\n",
        "    )\n",
        "    \n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "papers = [\n",
        "    \"/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf\",\n",
        "    \"/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf\",\n",
        "    \"/Users/tanmaydhote/Downloads/5488_MetaGPT_Meta_Programming_.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: /Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:13:42,136 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: /Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:13:45,250 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: /Users/tanmaydhote/Downloads/5488_MetaGPT_Meta_Programming_.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:13:46,729 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: /Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:13:49,254 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: /Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:13:51,461 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: /Users/tanmaydhote/Downloads/5488_MetaGPT_Meta_Programming_.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:13:53,799 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionAgent\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=initial_tools,\n",
        "    verbose=True,\n",
        ")\n",
        "chat_memory = ChatMemoryBuffer.from_defaults()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:20:03,573 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 16:20:14,895 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESPONSE CONTENT:\n",
            "============================================================\n",
            "The evaluation dataset used in LongLoRA is the proof-pile test set.\n",
            "\n",
            "Regarding the evaluation results, LongLoRA demonstrates strong empirical performance on various tasks using Llama2 models, ranging from 7B/13B to 70B. It effectively extends the context length of these models while maintaining their original architectures. Specifically, LongLoRA extends the Llama2 7B model from a 4k context to 100k and the Llama2 70B model to 32k on a single 8× A100 machine. It achieves this with up to 1.8× lower memory cost and improves the training speed by up to 1.8× compared to full fine-tuning. The evaluation on the proof-pile test set shows that LongLoRA closes the accuracy gap between conventional LoRA and full fine-tuning.\n",
            "\n",
            "============================================================\n",
            "RESPONSE OBJECT PROPERTIES:\n",
            "============================================================\n",
            "Response type: <class 'llama_index.core.agent.workflow.workflow_events.AgentOutput'>\n",
            "Agent name: Agent\n",
            "Response message: assistant: The evaluation dataset used in LongLoRA is the proof-pile test set.\n",
            "\n",
            "Regarding the evaluation results, LongLoRA demonstrates strong empirical performance on various tasks using Llama2 models, ranging from 7B/13B to 70B. It effectively extends the context length of these models while maintaining their original architectures. Specifically, LongLoRA extends the Llama2 7B model from a 4k context to 100k and the Llama2 70B model to 32k on a single 8× A100 machine. It achieves this with up to 1.8× lower memory cost and improves the training speed by up to 1.8× compared to full fine-tuning. The evaluation on the proof-pile test set shows that LongLoRA closes the accuracy gap between conventional LoRA and full fine-tuning.\n",
            "Response content: The evaluation dataset used in LongLoRA is the proof-pile test set.\n",
            "\n",
            "Regarding the evaluation results, LongLoRA demonstrates strong empirical performance on various tasks using Llama2 models, ranging from 7B/13B to 70B. It effectively extends the context length of these models while maintaining their original architectures. Specifically, LongLoRA extends the Llama2 7B model from a 4k context to 100k and the Llama2 70B model to 32k on a single 8× A100 machine. It achieves this with up to 1.8× lower memory cost and improves the training speed by up to 1.8× compared to full fine-tuning. The evaluation on the proof-pile test set shows that LongLoRA closes the accuracy gap between conventional LoRA and full fine-tuning.\n",
            "============================================================\n",
            "Number of tool calls: 2\n",
            "Tool calls: [ToolCallResult(tool_name='vector_tool_1602_LongLoRA_Efficient_Fine_t', tool_kwargs={'query': 'evaluation dataset used in LongLoRA'}, tool_id='call_fZ2zdlm8WODxrWd8NHBGOBQB', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='The evaluation dataset used in LongLoRA is the proof-pile test set.')], tool_name='vector_tool_1602_LongLoRA_Efficient_Fine_t', raw_input={'args': (), 'kwargs': {'query': 'evaluation dataset used in LongLoRA'}}, raw_output=Response(response='The evaluation dataset used in LongLoRA is the proof-pile test set.', source_nodes=[NodeWithScore(node=TextNode(id_='410c763c-6e84-414d-9732-eb4fb9fc03de', embedding=None, metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1ac18f89-684d-4dfc-a802-6aa4e2008b81', node_type='4', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1b450c75cbac43004d8a12f479b9d3de5e2139a5ac89dc6136f6a6b417fddd62'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0d3d588d-2b84-432e-af83-d9c92a883507', node_type='1', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='e9f5e5be2c4c457928163242e872ebb72a6ae703a038a8120024d61e719429c9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='These settings share similar computational costs. We visualize\\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\\n5 C ONCLUSION\\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\\nattention architecture during inference, making most pre-existing infrastructure and optimization\\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\\nnormalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B\\nmodel to 32k context length, on a single 8 × A100 machine. We also present a long instruction-\\nfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA. We believe\\nthat LongLoRA is a general method that could be compatible with more types of LLMs and position\\nencodings. We plan to investigate these in future work.\\nAcknowledgement We would like to thank Xiuyu Li and Bohao Peng for the helpful discussions.\\n9', mimetype='text/plain', start_char_idx=2865, end_char_idx=4637, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6312443794871967), NodeWithScore(node=TextNode(id_='a13f0794-9f31-4eb4-a623-4c24fc4ebe36', embedding=None, metadata={'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3801b43c-54ef-4fb7-a0ac-c7ab87993302', node_type='4', metadata={'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='03f5fcaa685d62d20d2a10b7598bc833c212c7905a8ddf8da1371d4262b4e1f8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nLONG LORA: E FFICIENT FINE -TUNING OF LONG -\\nCONTEXT LARGE LANGUAGE MODELS\\nYukang Chen 1 Shengju Qian 1 Haotian Tang 2 Xin Lai 1\\nZhijian Liu 2 Song Han 2,3 Jiaya Jia 1\\n1CUHK 2MIT 3NVIDIA\\nABSTRACT\\nWe present LongLoRA, an efficient fine-tuning approach that extends the context\\nsizes of pre-trained large language models (LLMs), with limited computation cost.\\nTypically, training LLMs with long context sizes is computationally expensive,\\nrequiring extensive training hours and GPU resources. For example, training on\\nthe context length of 8192 needs 16× computational costs in self-attention layers\\nas that of 2048. In this paper, we speed up the context extension of LLMs in\\ntwo aspects. On the one hand, although dense global attention is needed during\\ninference, fine-tuning the model can be effectively and efficiently done by sparse\\nlocal attention. The proposed shifted sparse attention (S2-Attn) effectively enables\\ncontext extension, leading to non-trivial computation saving with similar perfor-\\nmance to fine-tuning with vanilla attention. Particularly, it can be implemented\\nwith only two lines of code in training, while being optional in inference. On\\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\\nexpansion. Notably, we find that LoRA for context extension works well under\\nthe premise of trainable embedding and normalization. LongLoRA combines this\\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8× A100\\nmachine. LongLoRA extends models’ context while retaining their original archi-\\ntectures, and is compatible with most existing techniques, like Flash-Attention2.\\nIn addition, we further conduct supervised fine-tuning with LongLoRA and our\\nlong instruction-following LongAlpaca dataset. All our code, models, dataset, and\\ndemo are available at github.com/dvlab-research/LongLoRA.\\n2.662.522.482.782.942.98\\n3.73\\n2.42.93.43.9\\n8192163643276865536\\nPerplexity\\n46.357.468.8\\n25.634.646.369.8\\n20406080\\n8192163643276865536\\nGPU memory\\nFull FTLoRALongLoRA\\n7.416.339.8\\n5.2 11.324.652.4\\n0306090\\n8192163643276865536\\nTraining hours\\nContext\\nhoursGB\\nContextContext\\nOOM OOM92.5\\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\\nwhile still maintaining up to 1.8× lower memory cost than full fine-tuning. Furthermore, LongLoRA\\nimproves the training speed of LoRA by up to 1.8 × with S2-Attn. Llama2-7B are fine-tuned to\\nvarious context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage\\n2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity.\\n1 I NTRODUCTION\\nLarge language models (LLMs) are typically trained with a pre-defined context size, such as 2048\\ntokens for LLaMA (Touvron et al., 2023a) and 4096 tokens for Llama2 (Touvron et al., 2023b).\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=3002, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6154358914118058)], metadata={'410c763c-6e84-414d-9732-eb4fb9fc03de': {'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'a13f0794-9f31-4eb4-a623-4c24fc4ebe36': {'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}}), is_error=False), return_direct=False), ToolCallResult(tool_name='vector_tool_1602_LongLoRA_Efficient_Fine_t', tool_kwargs={'query': 'evaluation results of LongLoRA'}, tool_id='call_pbPyTrHGRNK6Q4H3vkpIsz21', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='LongLoRA demonstrates strong empirical results on various tasks using Llama2 models, ranging from 7B/13B to 70B. It effectively extends the context length of these models while maintaining their original architectures. Specifically, LongLoRA extends the Llama2 7B model from a 4k context to 100k and the Llama2 70B model to 32k on a single 8× A100 machine. It achieves this with up to 1.8× lower memory cost and improves the training speed by up to 1.8× compared to full fine-tuning. The evaluation on the proof-pile test set shows that LongLoRA closes the accuracy gap between conventional LoRA and full fine-tuning.')], tool_name='vector_tool_1602_LongLoRA_Efficient_Fine_t', raw_input={'args': (), 'kwargs': {'query': 'evaluation results of LongLoRA'}}, raw_output=Response(response='LongLoRA demonstrates strong empirical results on various tasks using Llama2 models, ranging from 7B/13B to 70B. It effectively extends the context length of these models while maintaining their original architectures. Specifically, LongLoRA extends the Llama2 7B model from a 4k context to 100k and the Llama2 70B model to 32k on a single 8× A100 machine. It achieves this with up to 1.8× lower memory cost and improves the training speed by up to 1.8× compared to full fine-tuning. The evaluation on the proof-pile test set shows that LongLoRA closes the accuracy gap between conventional LoRA and full fine-tuning.', source_nodes=[NodeWithScore(node=TextNode(id_='410c763c-6e84-414d-9732-eb4fb9fc03de', embedding=None, metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1ac18f89-684d-4dfc-a802-6aa4e2008b81', node_type='4', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1b450c75cbac43004d8a12f479b9d3de5e2139a5ac89dc6136f6a6b417fddd62'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0d3d588d-2b84-432e-af83-d9c92a883507', node_type='1', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='e9f5e5be2c4c457928163242e872ebb72a6ae703a038a8120024d61e719429c9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='These settings share similar computational costs. We visualize\\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\\n5 C ONCLUSION\\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\\nattention architecture during inference, making most pre-existing infrastructure and optimization\\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\\nnormalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B\\nmodel to 32k context length, on a single 8 × A100 machine. We also present a long instruction-\\nfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA. We believe\\nthat LongLoRA is a general method that could be compatible with more types of LLMs and position\\nencodings. We plan to investigate these in future work.\\nAcknowledgement We would like to thank Xiuyu Li and Bohao Peng for the helpful discussions.\\n9', mimetype='text/plain', start_char_idx=2865, end_char_idx=4637, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6601325414319625), NodeWithScore(node=TextNode(id_='a13f0794-9f31-4eb4-a623-4c24fc4ebe36', embedding=None, metadata={'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3801b43c-54ef-4fb7-a0ac-c7ab87993302', node_type='4', metadata={'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='03f5fcaa685d62d20d2a10b7598bc833c212c7905a8ddf8da1371d4262b4e1f8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nLONG LORA: E FFICIENT FINE -TUNING OF LONG -\\nCONTEXT LARGE LANGUAGE MODELS\\nYukang Chen 1 Shengju Qian 1 Haotian Tang 2 Xin Lai 1\\nZhijian Liu 2 Song Han 2,3 Jiaya Jia 1\\n1CUHK 2MIT 3NVIDIA\\nABSTRACT\\nWe present LongLoRA, an efficient fine-tuning approach that extends the context\\nsizes of pre-trained large language models (LLMs), with limited computation cost.\\nTypically, training LLMs with long context sizes is computationally expensive,\\nrequiring extensive training hours and GPU resources. For example, training on\\nthe context length of 8192 needs 16× computational costs in self-attention layers\\nas that of 2048. In this paper, we speed up the context extension of LLMs in\\ntwo aspects. On the one hand, although dense global attention is needed during\\ninference, fine-tuning the model can be effectively and efficiently done by sparse\\nlocal attention. The proposed shifted sparse attention (S2-Attn) effectively enables\\ncontext extension, leading to non-trivial computation saving with similar perfor-\\nmance to fine-tuning with vanilla attention. Particularly, it can be implemented\\nwith only two lines of code in training, while being optional in inference. On\\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\\nexpansion. Notably, we find that LoRA for context extension works well under\\nthe premise of trainable embedding and normalization. LongLoRA combines this\\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8× A100\\nmachine. LongLoRA extends models’ context while retaining their original archi-\\ntectures, and is compatible with most existing techniques, like Flash-Attention2.\\nIn addition, we further conduct supervised fine-tuning with LongLoRA and our\\nlong instruction-following LongAlpaca dataset. All our code, models, dataset, and\\ndemo are available at github.com/dvlab-research/LongLoRA.\\n2.662.522.482.782.942.98\\n3.73\\n2.42.93.43.9\\n8192163643276865536\\nPerplexity\\n46.357.468.8\\n25.634.646.369.8\\n20406080\\n8192163643276865536\\nGPU memory\\nFull FTLoRALongLoRA\\n7.416.339.8\\n5.2 11.324.652.4\\n0306090\\n8192163643276865536\\nTraining hours\\nContext\\nhoursGB\\nContextContext\\nOOM OOM92.5\\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\\nwhile still maintaining up to 1.8× lower memory cost than full fine-tuning. Furthermore, LongLoRA\\nimproves the training speed of LoRA by up to 1.8 × with S2-Attn. Llama2-7B are fine-tuned to\\nvarious context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage\\n2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity.\\n1 I NTRODUCTION\\nLarge language models (LLMs) are typically trained with a pre-defined context size, such as 2048\\ntokens for LLaMA (Touvron et al., 2023a) and 4096 tokens for Llama2 (Touvron et al., 2023b).\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=3002, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6509754955885584)], metadata={'410c763c-6e84-414d-9732-eb4fb9fc03de': {'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'a13f0794-9f31-4eb4-a623-4c24fc4ebe36': {'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}}), is_error=False), return_direct=False)]\n",
            "  Tool 1: vector_tool_1602_LongLoRA_Efficient_Fine_t with args: {'query': 'evaluation dataset used in LongLoRA'}\n",
            "  Tool output: The evaluation dataset used in LongLoRA is the proof-pile test set.\n",
            "  Tool 2: vector_tool_1602_LongLoRA_Efficient_Fine_t with args: {'query': 'evaluation results of LongLoRA'}\n",
            "  Tool output: LongLoRA demonstrates strong empirical results on various tasks using Llama2 models, ranging from 7B/13B to 70B. It effectively extends the context length of these models while maintaining their original architectures. Specifically, LongLoRA extends the Llama2 7B model from a 4k context to 100k and the Llama2 70B model to 32k on a single 8× A100 machine. It achieves this with up to 1.8× lower memory cost and improves the training speed by up to 1.8× compared to full fine-tuning. The evaluation on the proof-pile test set shows that LongLoRA closes the accuracy gap between conventional LoRA and full fine-tuning.\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nprint(\"HANDLER INFORMATION:\")\\nprint(\"=\" * 60)\\nprint(f\"Handler type: {type(handler)}\")\\nprint(f\"Handler context available: {hasattr(handler, \\'ctx\\')}\")\\nprint()\\n\\n\\n# You can also access memory to see the full conversation\\nprint(\"=\" * 60)\\nprint(\"CONVERSATION HISTORY (from memory):\")\\nprint(\"=\" * 60)\\nmessages = await chat_memory.aget()\\nfor i, msg in enumerate(messages):\\n    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\\n'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\",\n",
        "    memory=chat_memory,\n",
        ")\n",
        "response = await handler\n",
        "\n",
        "# Explore what's available in the response object\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE CONTENT:\")\n",
        "print(\"=\" * 60)\n",
        "print(str(response))  # or response.response.content\n",
        "print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE OBJECT PROPERTIES:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Response type: {type(response)}\")\n",
        "print(f\"Agent name: {response.current_agent_name}\")\n",
        "print(f\"Response message: {response.response}\")\n",
        "print(f\"Response content: {response.response.content}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Number of tool calls: {len(response.tool_calls)}\")\n",
        "if response.tool_calls:\n",
        "    print(f\"Tool calls: {response.tool_calls}\")\n",
        "    for i, tool_call in enumerate(response.tool_calls):\n",
        "        print(f\"  Tool {i+1}: {tool_call.tool_name} with args: {tool_call.tool_kwargs}\")\n",
        "        if hasattr(tool_call, \"tool_output\"):\n",
        "            print(f\"  Tool output: {tool_call.tool_output}\")\n",
        "print()\n",
        "\n",
        "# Access the handler to see execution details\n",
        "print(\"=\" * 60)\n",
        "\"\"\"\n",
        "print(\"HANDLER INFORMATION:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Handler type: {type(handler)}\")\n",
        "print(f\"Handler context available: {hasattr(handler, 'ctx')}\")\n",
        "print()\n",
        "\n",
        "\n",
        "# You can also access memory to see the full conversation\n",
        "print(\"=\" * 60)\n",
        "print(\"CONVERSATION HISTORY (from memory):\")\n",
        "print(\"=\" * 60)\n",
        "messages = await chat_memory.aget()\n",
        "for i, msg in enumerate(messages):\n",
        "    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:22:37,104 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 16:22:42,848 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 16:22:45,357 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-11-19 16:22:46,065 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESPONSE CONTENT:\n",
            "============================================================\n",
            "**Self-RAG (Self-Reflective Retrieval-Augmented Generation):**  \n",
            "Self-RAG is a framework designed to enhance the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection. It trains an LLM to retrieve relevant passages on-demand and generate outputs while reflecting on the quality and factual support of these outputs using special tokens called reflection tokens. This approach allows the model to adaptively decide when retrieval is necessary and to critique its own outputs, improving overall generation quality, factuality, and citation accuracy. Self-RAG outperforms other models, including ChatGPT, in various tasks by providing more accurate and verifiable outputs.\n",
            "\n",
            "**LongLoRA:**  \n",
            "LongLoRA is an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It addresses the challenge of training LLMs with long context sizes, which is typically resource-intensive. LongLoRA achieves this by employing shifted sparse attention (S2-Attn) during training, which allows for context extension while maintaining computational efficiency. This method retains the original attention architecture during inference, ensuring compatibility with existing optimization techniques. Additionally, LongLoRA incorporates an improved version of low-rank adaptation (LoRA) by making embedding and normalization layers trainable, which is crucial for effective long context adaptation. This approach allows models like Llama2 to be extended to significantly larger context lengths on standard hardware setups.\n",
            "\n",
            "============================================================\n",
            "RESPONSE OBJECT PROPERTIES:\n",
            "============================================================\n",
            "Response type: <class 'llama_index.core.agent.workflow.workflow_events.AgentOutput'>\n",
            "Agent name: Agent\n",
            "Response message: assistant: **Self-RAG (Self-Reflective Retrieval-Augmented Generation):**  \n",
            "Self-RAG is a framework designed to enhance the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection. It trains an LLM to retrieve relevant passages on-demand and generate outputs while reflecting on the quality and factual support of these outputs using special tokens called reflection tokens. This approach allows the model to adaptively decide when retrieval is necessary and to critique its own outputs, improving overall generation quality, factuality, and citation accuracy. Self-RAG outperforms other models, including ChatGPT, in various tasks by providing more accurate and verifiable outputs.\n",
            "\n",
            "**LongLoRA:**  \n",
            "LongLoRA is an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It addresses the challenge of training LLMs with long context sizes, which is typically resource-intensive. LongLoRA achieves this by employing shifted sparse attention (S2-Attn) during training, which allows for context extension while maintaining computational efficiency. This method retains the original attention architecture during inference, ensuring compatibility with existing optimization techniques. Additionally, LongLoRA incorporates an improved version of low-rank adaptation (LoRA) by making embedding and normalization layers trainable, which is crucial for effective long context adaptation. This approach allows models like Llama2 to be extended to significantly larger context lengths on standard hardware setups.\n",
            "Response content: **Self-RAG (Self-Reflective Retrieval-Augmented Generation):**  \n",
            "Self-RAG is a framework designed to enhance the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection. It trains an LLM to retrieve relevant passages on-demand and generate outputs while reflecting on the quality and factual support of these outputs using special tokens called reflection tokens. This approach allows the model to adaptively decide when retrieval is necessary and to critique its own outputs, improving overall generation quality, factuality, and citation accuracy. Self-RAG outperforms other models, including ChatGPT, in various tasks by providing more accurate and verifiable outputs.\n",
            "\n",
            "**LongLoRA:**  \n",
            "LongLoRA is an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It addresses the challenge of training LLMs with long context sizes, which is typically resource-intensive. LongLoRA achieves this by employing shifted sparse attention (S2-Attn) during training, which allows for context extension while maintaining computational efficiency. This method retains the original attention architecture during inference, ensuring compatibility with existing optimization techniques. Additionally, LongLoRA incorporates an improved version of low-rank adaptation (LoRA) by making embedding and normalization layers trainable, which is crucial for effective long context adaptation. This approach allows models like Llama2 to be extended to significantly larger context lengths on standard hardware setups.\n",
            "============================================================\n",
            "Number of tool calls: 2\n",
            "Tool calls: [ToolCallResult(tool_name='summary_tool_6283_Self_RAG_Learning_to_Retr', tool_kwargs={'input': 'Self-RAG'}, tool_id='call_N2lquPKbf0z1uMgGCYxP3Pfj', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='Self-Reflective Retrieval-Augmented Generation (SELF-RAG) is a framework designed to enhance the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection. It trains an LLM to retrieve relevant passages on-demand and generate outputs while reflecting on the quality and factual support of these outputs using special tokens called reflection tokens. This approach allows the model to adaptively decide when retrieval is necessary and to critique its own outputs, improving overall generation quality, factuality, and citation accuracy. SELF-RAG outperforms other models, including ChatGPT, in various tasks by providing more accurate and verifiable outputs.')], tool_name='summary_tool_6283_Self_RAG_Learning_to_Retr', raw_input={'input': 'Self-RAG'}, raw_output=Response(response='Self-Reflective Retrieval-Augmented Generation (SELF-RAG) is a framework designed to enhance the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection. It trains an LLM to retrieve relevant passages on-demand and generate outputs while reflecting on the quality and factual support of these outputs using special tokens called reflection tokens. This approach allows the model to adaptively decide when retrieval is necessary and to critique its own outputs, improving overall generation quality, factuality, and citation accuracy. SELF-RAG outperforms other models, including ChatGPT, in various tasks by providing more accurate and verifiable outputs.', source_nodes=[NodeWithScore(node=TextNode(id_='87fbbc3a-9cbc-4040-b177-ced283a8b851', embedding=None, metadata={'page_label': '1', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='de8d3ec6-0d92-4677-b6d2-a461d98678a7', node_type='4', metadata={'page_label': '1', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='4f5b7ddc9ad93a83899537ca9475dc199d1895a701df4141457f795ae6f9e944')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nSELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND\\nCRITIQUE THROUGH SELF -REFLECTION\\nAkari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§\\n†University of Washington §Allen Institute for AI ‡IBM Research AI\\n{akari,zeqiuwu1,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration (SELF -RAG) that enhances an LM’s quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF -\\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages.\\nThis work introduces Self-Reflective Retrieval-augmented Generation (SELF -RAG) to improve an\\nLLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF -RAG first determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\\nretrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/.\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=4087, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='c7cccd92-ec6b-4643-b309-32a691f4f1a5', embedding=None, metadata={'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a954e93-610c-49f2-a5d2-69a04a8befcd', node_type='4', metadata={'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='a4c7aaa92f83a080f17befbd24bc4cf70ae304fe52ca4d30eddf90dc54f1cb00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='49962080-5e6f-4676-848b-37eb1b0dfd27', node_type='1', metadata={}, hash='ec75d82c44f654107e5539a76021f80549b0a50315f2d1757e0ea3889b66390e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Published as a conference paper at ICLR 2024\\nStep 1: Retrieve K documents\\nCalifornia was named after a ﬁctional \\nisland in a Spanish book. \\nPrompt How did US states get their names? \\nUS states got their names from a variety of \\nsources. Eleven states are named after an \\nindividual person  (e.g, California was named \\nafter Christopher Columbus) . Some states \\nincluding Texas and Utah, are named after \\nNative American tribe.\\nRetrieval-Augmented Generation (RAG) Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG) \\nPopular names by states. In Texas, \\nEmma is a popular baby name. \\nOf the ﬁfty states, eleven are named \\nafter an individual person. \\nPrompt How did US states get their names? + \\nStep 2: Prompt LM with K docs and generate\\nRetriever\\nLM\\nPrompt How did US states get their names? \\nUS states got their names from a variety of sources. Retrieve\\nStep 1: Retrieve on demand  \\nPrompt +  \\n11 of 50 state namesRelevant\\nStep 2: Generate segment in parallel \\ncome from persons. Supported\\nIrrelevant Texas is named\\nafter a Native American tribe. \\nStep 3: Critique outputs and select best segment\\norigins in a 16th-century novel \\nLas Sergas de Esplandián. \\nCalifornia's name has itsRelevant\\nPartially\\nUS states got their names from a variety of sources. 11 of 50 \\nstates names are come from persons.    26 states are named \\nafter Native Americans, including Utah. \\nPrompt: Write an essay of your best summer vacation\\n Prompt: Write an essay of your best summer vacation\\nNo Retrieval My best summer vacation is when my family and I embarked on a road trip along …My best… \\n>\\nRepeat.…\\nNo information in passagesContradictory\\n>\\nPrompt +  \\n Prompt +  \\nRetrieve\\nFigure 1: Overview of SELF -RAG. SELF -RAG learns to retrieve, critique, and generate text passages\\nto enhance overall generation quality, factuality, and verifiability.\\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\\ngeneration quality. Moreover, SELF -RAG provides citations for each segment with its self-assessment\\nof whether the output is supported by the passage, leading to easier fact verification.\\nSELF -RAG trains an arbitrary LM to generate text with reflection tokens by unifying them as the\\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\\nassess its own predictions after each generated segment as an integral part of the generation output.\\nSELF -RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,\\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\\nmodels’ behaviors to user preferences by leveraging reflection tokens through segment-level beam\\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF -\\nRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF -RAG outperforms\\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\\nreflection tokens for overall performance improvements as well as test-time model customizations\\n(e.g., balancing the trade-off between citation previsions and completeness).\", mimetype='text/plain', start_char_idx=0, end_char_idx=4398, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='49962080-5e6f-4676-848b-37eb1b0dfd27', embedding=None, metadata={'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a954e93-610c-49f2-a5d2-69a04a8befcd', node_type='4', metadata={'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='a4c7aaa92f83a080f17befbd24bc4cf70ae304fe52ca4d30eddf90dc54f1cb00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c7cccd92-ec6b-4643-b309-32a691f4f1a5', node_type='1', metadata={'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='851611b476914e5993be49eea8a5d51bf8422c3e4c4f33fd22d764aea17acbcf')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF -\\nRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF -RAG outperforms\\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\\nreflection tokens for overall performance improvements as well as test-time model customizations\\n(e.g., balancing the trade-off between citation previsions and completeness).\\n2 R ELATED WORK\\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\\n2', mimetype='text/plain', start_char_idx=3725, end_char_idx=4796, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='8e711daf-51fd-4257-9b4c-f1e067c723c6', embedding=None, metadata={'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='136a4421-7f6d-483c-9fa6-8dc5b00b0de6', node_type='4', metadata={'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6abffc0f2774bb5439290551a287604263539b456c8338042e2416644aec0a20'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='05ad53fe-0179-4214-b167-74ce89bb160e', node_type='1', metadata={}, hash='7529dd0f97cca452e72fda4ca1152763ec569c3cda76f25d7104cbc86da85302')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nType Input Output Definitions\\nRetrieve x / x, y {yes, no, continue} Decides when to retrieve with R\\nISREL x, d {relevant, irrelevant} d provides useful information to solve x.\\nISSUP x, d, y {fully supported, partially\\nsupported, no support}\\nAll of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.\\nTable 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.\\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\\nentities. Yet, the improved task performance of such approaches often comes at the expense of\\nrun-time efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack\\nof attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\\ngeneration guided by reflections tokens to further improve generation quality and attributions.\\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\\neffective in aligning LLMs with human preferences (Ouyang et al., 2022; Wu et al., 2023). Though\\nour work also studies fine-grained critique on retrieval and generation, we train our target LM on task\\nexamples augmented with reflection tokens from a critic model offline, with a far lower training cost\\ncompared to RLHF. Compared to prior work using control tokens to guide LM generation (Lu et al.,\\n2022; Korbak et al., 2023), SELF -RAG uses reflection tokens to decide the need for retrieval and to\\nself-evaluate generation quality.\\n3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\\nSELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training\\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the output’s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.2 Generated\\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassage’s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage.', mimetype='text/plain', start_char_idx=0, end_char_idx=3966, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='05ad53fe-0179-4214-b167-74ce89bb160e', embedding=None, metadata={'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='136a4421-7f6d-483c-9fa6-8dc5b00b0de6', node_type='4', metadata={'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6abffc0f2774bb5439290551a287604263539b456c8338042e2416644aec0a20'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8e711daf-51fd-4257-9b4c-f1e067c723c6', node_type='1', metadata={'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='fcce0c9757bae6c3345bdf5c2318dab58487dfbf7785e16fcc21bc3062164172')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='. . , yT ], where yt indicates a sequence of tokens for the t-th segment.2 Generated\\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassage’s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.3 To generate each segment, SELF -RAG processes multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n2In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3We follow Liu et al. (2023a) in using a “perceived” utility value that is independent of retrieved passages.\\n3', mimetype='text/plain', start_char_idx=3231, end_char_idx=4480, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='da8d9514-9752-4e14-8014-8de420292cf8', embedding=None, metadata={'page_label': '4', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='595ef5f8-0d4f-43cc-990c-7e7a74dd2101', node_type='4', metadata={'page_label': '4', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='f7b5f8572566ef6e7eafb98bedcf9d0ec6df3843b6712bd68b445e35dec575e7')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nAlgorithm 1 SELF -RAG Inference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }\\n1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\\n2: M predicts Retrieve given (x, y<t)\\n3: if Retrieve == Yes then\\n4: Retrieve relevant text passages D using R given (x, yt−1) ▷ Retrieve\\n5: M predicts ISREL given x, dand yt given x, d, y<t for each d ∈ D ▷ Generate\\n6: M predicts ISSUP and ISUSE given x, yt, dfor each d ∈ D ▷ Critique\\n7: Rank yt based on ISREL , ISSUP , ISUSE ▷ Detailed in Section 3.3\\n8: else if Retrieve == No then\\n9: Mgen predicts yt given x ▷ Generate\\n10: Mgen predicts ISUSE given x, yt ▷ Critique\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\\nand d3 output is only partially supported while d1 are fully supported.\\nTraining overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\\nM to generate reflection tokens by itself without relying on the critic at inference time.\\n3.2 S ELF -RAG TRAINING\\nHere, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)\\nand the generator M (Section 3.2.2).\\n3.2.1 T RAINING THE CRITIC MODEL\\nData collection for critic model. Manual annotation of reflection tokens for each segment is\\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\\nused to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\\ncan raise API costs and diminish reproducibility (Chen et al., 2023). Our method requires fine-\\ngrained evaluations on multiple passages as well as segments for each input-output instance from the\\ntraining dataset, increasing the number of evaluations required to generate SELF -RAG training data\\nexponentially. To overcome those issues, we create supervised data by prompting GPT-4 to generate\\nreflection tokens and then distill their knowledge into an in-house C. For each group of reflection\\ntokens, we randomly sample instances from the original training data: {Xsample, Ysample} ∼\\n{X, Y}. As different reflection token groups have their definitions and input, as shown in Table 1,\\nwe use different instruction prompts for them. Here, we use Retrieve as an example. We prompt\\nGPT-4 with a type-specific instruction (“Given an instruction, make a judgment on whether finding\\nsome external documents from the web helps to generate a better response.”) followed by few-shot\\ndemonstrations I the original task input x and output y to predict an appropriate reflection token\\nas text: p(r|I, x, y). Manual assessment reveals that GPT-4 reflection token predictions show high\\nagreement with human evaluations. We collect 4k-20k supervised training data for each type and\\ncombine them to form training data for C. Appendix Section D shows the full list of instructions, and\\nA.1 contains more details and our analysis.\\nCritic learning. After we collect training data Dcritic, we initialize C with a pre-trained LM and\\ntrain it on Dcritic using a standard conditional language modeling objective, maximizing likelihood:\\nmax\\nC\\nE((x,y),r)∼Dcritic log pC(r|x, y), rfor reflection tokens. (1)\\n4', mimetype='text/plain', start_char_idx=0, end_char_idx=4054, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='dc273803-4e08-4d19-b4f4-a920b4a1e353', embedding=None, metadata={'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fa0aa725-1004-4d2f-96d4-a8cd8f95e617', node_type='4', metadata={'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='853b149d018b7a0a69c79e132d60b2af8e5c2a4b38f687118cc518da2e648fa7'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='97ffd9d2-c627-41e2-99cf-91998fba9b69', node_type='1', metadata={}, hash='30430c69ed6c38ce870263f3899f1186c102c0fd441af401049bfe7ee0e83106')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInput: How did US states get their names? Input: Write an essay of your best summer vacation\\nOutput: My best summer vacation was a magical escape \\nto the coastal town of Santorini. The azure waters, \\ncharming white-washed building are unforgettable. \\nCritic LM\\nOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor \\nof King Louis XIV of France and Georgia was named after King George II. \\nRetrieve\\nPartially\\nAugmented Output:                Retrieve\\n<p>LOUISIANA: Named in\\n<p>Of the ﬁfty states, eleven are named after an individual person</p>.  \\n             11 of 50 states’ names come from person. Relevant Supported\\nhonor of Louis XIV of France.</p>.  Relevant For instance, Louisiana was named after King Louis XIV, and\\nUtil: 5Georgia was named after King George II. \\nUtil: 5\\nAugmented Output:                     My best summer \\nvacation was a magical escape to the coastal town of \\nSantorini.                     The azure waters, charming white-\\nwashed building are unforgettable experience.\\nNo Retrieval\\nNo Retrieval\\nRetriever\\nFigure 2: SELF -RAG training examples. The left example does not require retrieval while the right\\none requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\\nThough the initial model can be any pre-trained LM, we use the same one as the generator LM\\n(i.e., Llama 2-7B; Touvron et al. 2023) for C initialization. The critic achieves a higher than 90%\\nagreement with GPT-4-based predictions on most reflection token categories (Appendix Table 4).\\n3.2.2 T RAINING THE GENERATOR MODEL\\nData collection for generator. Given an input-output pair (x, y), we augment the original output\\ny using the retrieval and critic models to create supervised data that precisely mimics the SELF -\\nRAG inference-time process (Section 3.1). For each segment yt ∈ y, we run C to assess whether\\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\\ntoken Retrieve =Yes is added, and R retrieves the top K passages, D. For each passage, C further\\nevaluates whether the passage is relevant and predicts ISREL . If a passage is relevant, C further\\nevaluates whether the passage supports the model generation and predicts ISSUP . Critique tokens\\nISREL and ISSUP are appended after the retrieved passage or generations. At the end of the output, y\\n(or yT ), C predicts the overall utility token ISUSE , and an augmented output with reflection tokens\\nand the original input pair is added to Dgen. See the example training data in Figure 2.\\nGenerator learning. We train the generator model M by training on the curated corpus augmented\\nwith reflection tokens Dgen using the standard next token objective:\\nmax\\nM\\nE(x,y,r)∼Dgen log pM(y, r|x). (2)\\nUnlike C training (Eq. 1), M learns to predict the target output as well as the reflection tokens. During\\ntraining, we mask out the retrieved text chunks (surrounded by <p> and </p> in Figure 2) for loss\\ncalculation and expand the original vocabulary V with a set of reflection tokens { Critique , Retrieve }.\\nConnections to prior work on learning with critique. Recent work incorporates additional\\ncritique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\\nseparate reward models during training, we compute critique offline and directly insert them into the\\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\\nlearns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\\n3.3 S ELF -RAG INFERENCE\\nGenerating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\\ninference phase, enabling it to tailor its behavior to diverse task requirements.', mimetype='text/plain', start_char_idx=0, end_char_idx=4117, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='97ffd9d2-c627-41e2-99cf-91998fba9b69', embedding=None, metadata={'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fa0aa725-1004-4d2f-96d4-a8cd8f95e617', node_type='4', metadata={'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='853b149d018b7a0a69c79e132d60b2af8e5c2a4b38f687118cc518da2e648fa7'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='dc273803-4e08-4d19-b4f4-a920b4a1e353', node_type='1', metadata={'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='67e9d0c33c3b5b2d0020be1d67b261d94b7c612dc42eb443eca85a728ea9d646')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2022) via PPO. While PPO relies on\\nseparate reward models during training, we compute critique offline and directly insert them into the\\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\\nlearns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\\n3.3 S ELF -RAG INFERENCE\\nGenerating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\\ncontrol to meet these distinct objectives during the inference process.\\nAdaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by\\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\\n5', mimetype='text/plain', start_char_idx=3275, end_char_idx=4817, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='c6bc7ff8-c1c1-468d-bf6e-6c27c96a1c82', embedding=None, metadata={'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='58c6b8d4-62eb-4c1f-a80d-c67bd897c56d', node_type='4', metadata={'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d230bc7258efc5db4515fb4d42a1a193a48558eb8d0bb5dc37f3c1041e9e30b0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d9fb8ec3-ec18-4bf7-b491-1fbe3f237d24', node_type='1', metadata={}, hash='ca3063cb2fde786e16301a9783328962b38e3508307c5cdff22be0735b9c463c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\\ndesignated threshold, we trigger retrieval (details in Appendix Section A.4).\\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\\nCritique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp\\nt as sG\\nt , and we compute a segment score as follows:\\nf(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)\\nS( Critique ) =\\nX\\nG∈G\\nwGsG\\nt for G = { ISREL , ISSUP , ISUSE }, (4)\\nwhere sG\\nt = pt(ˆr)PNG\\ni=1 pt(ri)\\nstands for the generation probability of the most desirable reflection token\\nˆr (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent\\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\\ny is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while\\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\\nduring decoding using Critique e.g., filtering out a segment continuation when the model generates an\\nundesirable token (e.g., ISSUP =No support).\\n4 E XPERIMENTS\\n4.1 T ASKS AND DATASETS\\nWe conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\\nour experiments’ settings, including test-time instructions, are available in the Appendix Section B.1.\\nClosed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;\\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\\nChallenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\\naggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\\nShort-form generations tasks include two open-domain question answering (QA) datasets,\\nPopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\\nto answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work’s validation and\\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\\nperformance based on whether gold answers are included in the model generations instead of strictly\\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA.', mimetype='text/plain', start_char_idx=0, end_char_idx=3952, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='d9fb8ec3-ec18-4bf7-b491-1fbe3f237d24', embedding=None, metadata={'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='58c6b8d4-62eb-4c1f-a80d-c67bd897c56d', node_type='4', metadata={'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d230bc7258efc5db4515fb4d42a1a193a48558eb8d0bb5dc37f3c1041e9e30b0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c6bc7ff8-c1c1-468d-bf6e-6c27c96a1c82', node_type='1', metadata={'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='0bedee6a050a9e85f62e3ba0060aa3a1f2690bee44a99eaea6f3c23c4ea51ebb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='We evaluate\\nperformance based on whether gold answers are included in the model generations instead of strictly\\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. 4\\n4.2 B ASELINES\\nBaselines without retrievals. We evaluate strong publicly available pre-trained LLMs,\\nLlama27B,13B (Touvron et al., 2023), instruction-tuned models, Alpaca7B,13B (Dubois et al., 2023)\\n4https://github.com/princeton-nlp/ALCE\\n6', mimetype='text/plain', start_char_idx=3377, end_char_idx=4194, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='4792f0a8-2e10-42ac-a003-a76567972fe0', embedding=None, metadata={'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d90b082e-c862-4f45-ae5f-2309f813c977', node_type='4', metadata={'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6b52d61cecf425bda253fbc457b39c84aa8bb2c0138d6f75c787aa866ca7584d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f09fc7f6-d8d2-4a91-a471-460aaa9df0a6', node_type='1', metadata={}, hash='f381001652f6e71136221ee992bf3b47601e4be60a61968f8991d69e8591fd8f')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\n(our replication based on Llama2); and models trained and reinforced using private data, Chat-\\nGPT (Ouyang et al., 2022) and Llama2-chat 13B. For instruction-tuned LMs, we use the official\\nsystem prompt or instruction format used during training if publicly available. We also compare our\\nmethod to concurrent work, CoVE65B (Dhuliawala et al., 2023), which introduces iterative prompt\\nengineering to improve the factuality of LLM generations.\\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\\nsystem. The second category includes concurrent methods that are trained with retrieved text\\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\\nto pre-train an LM with API calls (e.g., Wikipedia APIs).5\\n4.3 E XPERIMENTAL SETTINGS\\nTraining data and settings. Our training data consists of diverse instruction-following input-output\\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\\ndocuments for each input. More training details are in the Appendix Section B.1.\\nInference settings. As a default configuration, we assign the weight terms ISREL , ISSUP , ISUSE\\nvalues of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\\nto 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\\nup inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\\nFor a token-level generation, we use greedy decoding. By default, we use the top five documents\\nfrom Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\\nuse additional top five documents retrieved by a web search engine, following Luo et al. (2023);\\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\\nbaselines for a fair comparison.\\n5 R ESULTS AND ANALYSIS\\n5.1 M AIN RESULTS\\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\\nretrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\\niteratively prompts Llama265B to refine output.\\nComparison against baselines with retrieval.', mimetype='text/plain', start_char_idx=0, end_char_idx=3734, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='f09fc7f6-d8d2-4a91-a471-460aaa9df0a6', embedding=None, metadata={'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d90b082e-c862-4f45-ae5f-2309f813c977', node_type='4', metadata={'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6b52d61cecf425bda253fbc457b39c84aa8bb2c0138d6f75c787aa866ca7584d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4792f0a8-2e10-42ac-a003-a76567972fe0', node_type='1', metadata={'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6afa18e28c072547988d5b65f1abbfd4dac0ad3767c609fe93a83e6728e8fcd5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='5 R ESULTS AND ANALYSIS\\n5.1 M AIN RESULTS\\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\\nretrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\\niteratively prompts Llama265B to refine output.\\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF -RAG also\\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-\\nbased models on all tasks. Powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca)\\nshow large gains from their non-retrieval baselines. However, we found that these baselines provide\\nlimited solutions for tasks where we cannot simply copy or extract sub-strings of retrieved passages.\\nOn PubHealth and ARC-Challenge, baselines with retrieval do not improve performance notably\\nfrom their no-retrieval counterparts. We also observe that most baselines with retrieval struggle to\\nimprove citation accuracy. On ASQA, our model shows significantly higher citation precision and\\n5We report numbers using the results reported in the paper as the implementations are not available.\\n7', mimetype='text/plain', start_char_idx=3030, end_char_idx=4576, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='1920110d-c820-4832-ada7-4af0e8e4b1fb', embedding=None, metadata={'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1700df36-a424-43cf-bd5b-54121c9352cf', node_type='4', metadata={'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='90fb48cb3702acf19d168d93cf32962ed6667f405ac16fc3dfb03ee160e85ee5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='57c0fe83-c261-4069-bdb9-047fa049f0d8', node_type='1', metadata={}, hash='b4373839c56dd0dca2d2cafe163351591817dc1f7cce899910f37c2d617f36a1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\\nthey outperforms all non-proprietary models. ∗ indicates concurrent or recent results reported by\\nconcurrent work. – indicates numbers that are not reported by the original papers or are not applicable.\\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\\nShort-form Closed-set Long-form generations (with citations)\\nPopQA TQA Pub ARC Bio ASQA\\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\\nLMs with proprietary data\\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 – –\\nRet-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\\nChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 – –\\nRet-ChatGPT 50.8 65.7 54.7 75.3 – 40.7 39.9 79.7 65.1 76.6\\nPerplexity.ai – – – – 71.2 – – – – –\\nBaselines without retrieval\\nLlama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 – –\\nAlpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 – –\\nLlama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 – –\\nAlpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 – –\\nCoVE65B * – – – – 71.2 – – – – –\\nBaselines with retrieval\\nToolformer*6B – 48.8 – – – – – – – –\\nLlama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\\nAlpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\\nLlama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\\nSAIL*7B – – 69.2 48.4 – – – – – –\\nLlama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\\nOur SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\\nOur SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\\nPQA Med AS\\n(acc) (acc) (em)\\nSELF -RAG (50k) 45.5 73.5 32.1\\nTraining\\nNo Retriever R 43.', mimetype='text/plain', start_char_idx=0, end_char_idx=1947, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='57c0fe83-c261-4069-bdb9-047fa049f0d8', embedding=None, metadata={'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1700df36-a424-43cf-bd5b-54121c9352cf', node_type='4', metadata={'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='90fb48cb3702acf19d168d93cf32962ed6667f405ac16fc3dfb03ee160e85ee5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1920110d-c820-4832-ada7-4af0e8e4b1fb', node_type='1', metadata={'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='3ff73f17603129c622d044acc3f7f9694e823792cc7e7aef92bbcf62e7fd88d4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='7 2.3 3.6\\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\\nOur SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\\nOur SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\\nPQA Med AS\\n(acc) (acc) (em)\\nSELF -RAG (50k) 45.5 73.5 32.1\\nTraining\\nNo Retriever R 43.6 67.8 31.0\\nNo Critic C 42.6 72.0 18.1\\nTest\\nNo retrieval 24.7 73.0 –\\nHard constraints 28.3 72.6 –\\nRetrieve top1 41.8 73.1 28.6\\nRemove ISSUP 44.1 73.2 30.6\\n(a) Ablation\\n1 2\\n70.0\\n70.5Precision\\n1 2\\nWeight for IsSupport\\n90\\n95Mauve\\n (b) Customization\\n0.0 0.2 0.4 0.6\\n0.98\\n0.99\\n0.99\\n1.00Accuracy\\nPubHealth\\n0.0 0.2 0.4 0.6\\nRetrieval Threshold\\n0.6\\n0.8\\n1.0Accuracy\\nPopQA\\n0.0\\n0.5\\n1.0\\nFrequency\\n0.25\\n0.50\\n0.75\\n1.00\\nFrequency\\n (c) Retrieval\\nFigure 3: Analysis on SELF -RAG: (a) Ablation studies for key components of SELF -RAG training\\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\\nrecall than all models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits\\nsuperior efficacy in this particular task, surpassing smaller LMs. Our SELF -RAG bridges this\\nperformance gap, even outperforming ChatGPT in citation precision, which measures whether the\\nmodel-generated claim is fully supported by cited evidence. Llama2-FT 7B, which is the baseline\\nLM trained on the same instruction-output pairs as SELF -RAG without retrieval or self-reflection and\\nis retrieval-augmented at test time only, lags behind SELF -RAG. This result indicates SELF -RAG\\ngains are not solely from training data and demonstrate the effectiveness of SELF -RAG framework.\\n8', mimetype='text/plain', start_char_idx=1658, end_char_idx=3336, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='82dde819-017f-4dda-828b-897359d23c9a', embedding=None, metadata={'page_label': '9', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5e9e64fb-90fe-4f60-8cca-31840cd72551', node_type='4', metadata={'page_label': '9', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1bf3c459167a747167abed6650a91bf781ecd43d51dc416e48506778b1c6f640')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\n5.2 A NALYSIS\\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\\ntop one document only, similar to standard RAG approaches;Remove ISSUP indicates the model\\nperformance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this\\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\\nand the bottom part is for inference ablations. We see that all components play important roles. We\\nalso observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\\nSELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAG’s\\ncapabilities of carefully selecting generations based on fine-grained multiple criteria, instead of\\nnaively using all passages from the retrieval model or solely depending on relevance scores.\\nEffects of inference-time customization. One key benefit of our proposed framework is that it\\nenables us to control how much each critique type affects the final generation sampling. We analyze\\nthe effects of different parameter weights on the top of our 7B model during inference time on\\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\\nthe weighting term for ISSUP , which criticizes how supported the output is by the text passage. As\\nthe figure shows, increasing the weight leads to positive effects on the models’ citation precision\\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at\\ntest time by adjusting such parameters without requiring additional training.\\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\\nthe overall accuracy and frequency of retrieval, and we evaluate the performance with varying\\nnumbers of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows\\nthat the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,\\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\\n6 C ONCLUSION\\nThis work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n9', mimetype='text/plain', start_char_idx=0, end_char_idx=4756, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='3c5aa808-89e6-4a33-a772-33a4210f9461', embedding=None, metadata={'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6c0884f1-2b3d-4b4f-8b66-f084a72d85ba', node_type='4', metadata={'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d53a702097b2e66060cb73c6698de5b8903f4153c88e3e5e0f193f9e0a75b9db'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a5da3671-5292-4068-bf79-513e4a463a4f', node_type='1', metadata={}, hash='d6036331b5b7e4ec426f2afee65fd38cc6e05661583c36be14d9e08ff8957e6b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nETHICAL CONCERNS\\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\\nadvice). While our method shows significant improvements in terms of performance, factuality, and\\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\\nmodel outputs.\\nACKNOWLEDGMENTS\\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\\nREFERENCES\\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-\\ning to retrieve reasoning paths over wikipedia graph for question answering. In International\\nConference on Learning Representations, 2020. URL https://openreview.net/forum?\\nid=SJgVHkrYDH.\\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and appli-\\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh\\nHajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Associ-\\nation for Computational Linguistics , 2023b. URL https://aclanthology.org/2023.\\nfindings-acl.225.\\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\\nEvaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037,\\n2022. URL https://arxiv.org/abs/2212.08037.\\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt’s behavior changing over time? arXiv\\npreprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memory-\\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems,\\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. Chain-of-verification reduces hallucination in large language models.arXiv preprint\\narXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.', mimetype='text/plain', start_char_idx=0, end_char_idx=3363, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='a5da3671-5292-4068-bf79-513e4a463a4f', embedding=None, metadata={'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6c0884f1-2b3d-4b4f-8b66-f084a72d85ba', node_type='4', metadata={'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d53a702097b2e66060cb73c6698de5b8903f4153c88e3e5e0f193f9e0a75b9db'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3c5aa808-89e6-4a33-a772-33a4210f9461', node_type='1', metadata={'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='242871b2274d5fc53964072e75cdfb8110ff0ae03fd1069b8a7ec13712460983')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memory-\\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems,\\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. Chain-of-verification reduces hallucination in large language models.arXiv preprint\\narXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of\\nwikipedia: Knowledge-powered conversational agents. In International Conference on Learning\\nRepresentations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\\n10', mimetype='text/plain', start_char_idx=2779, end_char_idx=3816, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='1de2830b-855d-4656-96ba-3c642986e281', embedding=None, metadata={'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f8e885b2-f47a-43eb-b031-5e959a198913', node_type='4', metadata={'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='e8407f3df368ce6c07a38e2f76a93b9009054596d1a120cdace47ed050bdf8f8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ff6c2b57-05e9-4db0-a0fe-a529fabb9444', node_type='1', metadata={}, hash='f341be7c9edfe017e10363b7d48d78b839bf2ee0f6fe1382a9ead574d71ec494')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nlearn from human feedback. arXiv preprint arXiv:2305.14387, 2023. URL https://arxiv.\\norg/abs/2305.14387.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/abs/\\n2305.14627.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\\nlanguage model pre-training. In International Conference on Machine Learning , 2020. URL\\nhttps://dl.acm.org/doi/pdf/10.5555/3524938.3525306.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\\nTransactions on Machine Learning Research , 2022a. URL https://openreview.net/\\nforum?id=jKN1pXi7b0.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\\nretrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022b. URL https:\\n//arxiv.org/abs/2208.03299.\\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\\narXiv:2305.06983, 2023. URL https://arxiv.org/abs/2305.06983.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017. URL\\nhttps://aclanthology.org/P17-1147.\\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\\narXiv:1909.05858, 2019. URL https://arxiv.org/abs/1909.05858.\\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences.\\nIn International Conference on Machine Learning, 2023. URL https://openreview.net/\\nforum?id=AT8Iw8KOeC.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\\nSlav Petrov. Natural questions: A benchmark for question answering research. Transactions of\\nthe Association for Computational Linguistics, 2019. URL https://aclanthology.org/\\nQ19-1026.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\\nSystems Principles, 2023. URL https://arxiv.org/abs/2309.06180.', mimetype='text/plain', start_char_idx=0, end_char_idx=3039, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='ff6c2b57-05e9-4db0-a0fe-a529fabb9444', embedding=None, metadata={'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f8e885b2-f47a-43eb-b031-5e959a198913', node_type='4', metadata={'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='e8407f3df368ce6c07a38e2f76a93b9009054596d1a120cdace47ed050bdf8f8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1de2830b-855d-4656-96ba-3c642986e281', node_type='1', metadata={'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='dc9a510ded7f9cfdb2a0d54e45023765444176f00f5facb5de46ec95e98922cf')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Natural questions: A benchmark for question answering research. Transactions of\\nthe Association for Computational Linguistics, 2019. URL https://aclanthology.org/\\nQ19-1026.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\\nSystems Principles, 2023. URL https://arxiv.org/abs/2309.06180.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/\\n2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-\\naugmented dual instruction tuning, 2023. URL https://arxiv.org/abs/2310.01352.\\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\\narXiv preprint arXiv:2304.09848, 2023a. URL https://arxiv.org/abs/2304.09848.\\n11', mimetype='text/plain', start_char_idx=2533, end_char_idx=3906, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='8726931c-19b1-43fc-b49c-7669f0005848', embedding=None, metadata={'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6a41ca68-386b-4cf1-b76e-afea07dd0c00', node_type='4', metadata={'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='c4d02a99413e4b1c0f7732dc6588e00eb674f9fd102ff33f3106e80d3a217114'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1b9231b0-8a7f-4d60-8071-057b8e55b6da', node_type='1', metadata={}, hash='2cf5fce88f7b2daa39448da31026a0e66fb8e846c4640ed85c5b2fae3e2a1a49')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.\\nURL https://arxiv.org/abs/2303.16634.\\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\\nmanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.\\nIn Advances in Neural Information Processing Systems, 2022. URL https://openreview.\\nnet/forum?id=5HaIds3ux5O.\\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,\\nHelen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint\\narXiv:2305.15225, 2023. URL https://arxiv.org/abs/2305.15225.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\\nmemories. In Proceedings of the 61st Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , 2023. URL https://aclanthology.org/2023.\\nacl-long.546.\\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\\nURL https://arxiv.org/abs/2203.11147.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? a new dataset for open book question answering. InProceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, 2018. URL https://aclanthology.\\norg/D18-1260.\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach\\nfor weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natu-\\nral Language Processing (EMNLP-IJCNLP), 2019. URL https://aclanthology.org/\\nD19-1284.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. URL https:\\n//arxiv.org/abs/2305.14251.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https:\\n//arxiv.org/abs/2112.09332.\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.669.\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.', mimetype='text/plain', start_char_idx=0, end_char_idx=3154, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='1b9231b0-8a7f-4d60-8071-057b8e55b6da', embedding=None, metadata={'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6a41ca68-386b-4cf1-b76e-afea07dd0c00', node_type='4', metadata={'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='c4d02a99413e4b1c0f7732dc6588e00eb674f9fd102ff33f3106e80d3a217114'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8726931c-19b1-43fc-b49c-7669f0005848', node_type='1', metadata={'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='e3a0232384048f093274846f1d514a5d0eecba517197ad6a5396c2c1ccd0ccd8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Webgpt: Browser-assisted\\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https:\\n//arxiv.org/abs/2112.09332.\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.669.\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.\\norg/abs/2303.08774.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\\nRyan Lowe. Training language models to follow instructions with human feedback. InAdvances in\\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?\\nid=TG8KACxEON.\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt¨aschel,\\n12', mimetype='text/plain', start_char_idx=2602, end_char_idx=3858, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='4e8bf89b-c118-4373-9b98-4b35583b6237', embedding=None, metadata={'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='788e6ad2-fca6-4808-8cff-9187b541573b', node_type='4', metadata={'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='4917fb8ee292366112b7b33461fb04eec1a4dfa535cdb5f4063836a482d184a2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80894de4-d02c-40e0-bee9-e884b8f3acaa', node_type='1', metadata={}, hash='670e4bbbc434695e52f33376d53485e687cd9dbe2d35fc4d55d0c9a5b4ae05bd')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nand Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings\\nof the 2021 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, 2021. URL https://aclanthology.org/\\n2021.naacl-main.200.\\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\\ndivergence frontiers. In Advances in Neural Information Processing Systems, 2021. URL https:\\n//openreview.net/forum?id=Tqx7nJp7PR.\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\\ntoward training trillion parameter models. In Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis, 2020. URL https://dl.acm.\\norg/doi/10.5555/3433701.3433727.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\\nfor Computational Linguistics, 2023. URL https://arxiv.org/abs/2302.00083.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\\nenables zero-shot task generalization. In International Conference on Learning Representations,\\n2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\\nuse tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.\\n04761.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https://arxiv.org/\\nabs/1707.06347.\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\\nSch¨arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.566.\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\\nscale dataset for fact extraction and VERification.', mimetype='text/plain', start_char_idx=0, end_char_idx=3185, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='80894de4-d02c-40e0-bee9-e884b8f3acaa', embedding=None, metadata={'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='788e6ad2-fca6-4808-8cff-9187b541573b', node_type='4', metadata={'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='4917fb8ee292366112b7b33461fb04eec1a4dfa535cdb5f4063836a482d184a2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4e8bf89b-c118-4373-9b98-4b35583b6237', node_type='1', metadata={'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d0b72d93eb9a474c6d3b49c8870aa2f46867b1174a7947f7d49266085e73b8dd')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.566.\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\\nscale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long Papers), 2018. URL https://aclanthology.org/N18-1074.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.\\norg/abs/2307.09288.\\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\\nexploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.\\nURL https://arxiv.org/abs/2306.04751.\\n13', mimetype='text/plain', start_char_idx=2582, end_char_idx=4021, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='a263ce37-5714-48b6-aa7f-c9330c0305a3', embedding=None, metadata={'page_label': '14', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='003a4950-f58e-425d-9f4e-d813ddc94328', node_type='4', metadata={'page_label': '14', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='02846c2fec8ba141d300a811d34af3fc26e459916d33054f89134bc880f31faf')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. InInternational\\nConference on Learning Representations, 2022. URL https://openreview.net/forum?\\nid=gEZrGCozdqR.\\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\\nrewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https:\\n//arxiv.org/abs/2306.01693.\\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\\nattribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https:\\n//arxiv.org/abs/2305.06311.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\\narXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\\npreprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\\n14', mimetype='text/plain', start_char_idx=0, end_char_idx=1337, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='c7e6ef48-3fac-483e-adec-1e5dfe2e02e0', embedding=None, metadata={'page_label': '15', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='228d0462-aad4-4b47-8ffb-d72064835937', node_type='4', metadata={'page_label': '15', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='2c5a36d1938bba78ed0a23b427b4520fbecd2fe8020b69f4038eee9ce94e1e73')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nAPPENDIX\\nA S ELF -RAG Details 16\\nA.1 Reflection Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nA.2 Advantages of Learning-based Methods . . . . . . . . . . . . . . . . . . . . . . . 16\\nA.3 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.4 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB Experimental Details 19\\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nC Results 20\\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nC.2 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nD Full List of Instructions and Demonstrations for GPT-4 21\\n15', mimetype='text/plain', start_char_idx=0, end_char_idx=1022, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='4f3186d4-6c47-42a7-952f-354dd3283198', embedding=None, metadata={'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43782c07-d00e-4d83-b3e3-e2c561fe09ef', node_type='4', metadata={'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='578118977388ea76aee1adadce278947913025ff8aa93c650301b9350b5eba13'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='737b259b-ee51-4d7b-b479-09018e9d2bfa', node_type='1', metadata={}, hash='d01980f1d481daeda1a7d8a1c5a227cd7141bd2f76cc449979b62edf0208c814')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nA S ELF -RAG DETAILS\\nA.1 R EFLECTION TOKENS\\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\\nonly given at each output level.\\n• Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\\nto use evidence, which indicates that a model can continue to use the evidence retrieved\\npreviously. For instance, a passage may contain rich factual information, and thus SELF -RAG\\ngenerates multiple segments based on the passage.\\n• Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\\n• Supported ( ISSUP ): Attribution is the concept of whether the output is fully supported by\\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\\nsupported, Partially supported, and No support / Contradictory, follow-\\ning Yue et al. (2023); Nakano et al. (2021).\\n• Useful ( ISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\\nas whether the response is a helpful and informative answer to the query, independently from\\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\\noutputs with “##”. We use the temperature 1 and set the maximum output token counts to be 200. We\\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\\nfor ISSUP , 19,317 for relevance, and 3,831 for utility.\\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\\nthe same instruction, demonstrations, and test instances. We found our assessments show high\\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to\\nthe disagreement between 1 and 2 or 4 and 5. Compared to prior efforts on agreement of GPT-4\\npredictions and human annotators in pair-wise evaluations, we found our human annotators often\\nagree with GPT-4 predictions. We hypothesize this is because our fine-grained evaluation with\\nabsolute scoring systems, unlike such relative, overall pair-wise evaluation systems enables GPT-4\\nto generate more reliable and agreeable predictions. The effectiveness of GPT-4 evaluations in\\nfine-grained aspects has shown to be effective in prior work (Liu et al., 2023a).\\nA.2 A DVANTAGES OF LEARNING -BASED METHODS\\nWhile recent work (Jiang et al., 2023) proposes a prompting-based method to enable retrieval on-\\ndemand, we find a learning-based method is more suitable to enable fine-grained self-reflection\\nfeedback and inference-time control. First, Self-RAG requires careful multi-aspect fine-grained\\nself-evaluations at inference time. To make an LM to comprehend fine-grained aspects and scoring\\nsystems, precise and detailed instructions, as well as few-shot demonstrations, are necessary. This\\nsignificantly increases the input sequence length, resulting in higher costs and latency. Nevertheless,\\nwe briefly tried prompting-based approaches in our preliminary experiments and found it is nontrivial.', mimetype='text/plain', start_char_idx=0, end_char_idx=4216, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='737b259b-ee51-4d7b-b479-09018e9d2bfa', embedding=None, metadata={'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43782c07-d00e-4d83-b3e3-e2c561fe09ef', node_type='4', metadata={'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='578118977388ea76aee1adadce278947913025ff8aa93c650301b9350b5eba13'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4f3186d4-6c47-42a7-952f-354dd3283198', node_type='1', metadata={'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='4eceec67c31d2ef9e6b9ab9f7bf0dccb5c792bda5974495271a02c4b04fdc5c0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The effectiveness of GPT-4 evaluations in\\nfine-grained aspects has shown to be effective in prior work (Liu et al., 2023a).\\nA.2 A DVANTAGES OF LEARNING -BASED METHODS\\nWhile recent work (Jiang et al., 2023) proposes a prompting-based method to enable retrieval on-\\ndemand, we find a learning-based method is more suitable to enable fine-grained self-reflection\\nfeedback and inference-time control. First, Self-RAG requires careful multi-aspect fine-grained\\nself-evaluations at inference time. To make an LM to comprehend fine-grained aspects and scoring\\nsystems, precise and detailed instructions, as well as few-shot demonstrations, are necessary. This\\nsignificantly increases the input sequence length, resulting in higher costs and latency. Nevertheless,\\nwe briefly tried prompting-based approaches in our preliminary experiments and found it is nontrivial.\\nWhen we combine all instructions for all aspects and feed them to the target pre-trained LMs (GPT-3\\ndavinci-003 / 002, Llama2-13B-chat), all models struggle to precisely follow our evaluation scheme,\\noften generating output formats that do not suit our scheme or whose reflections are less accurate. To\\n16', mimetype='text/plain', start_char_idx=3357, end_char_idx=4522, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='df978b06-05c0-4e29-80fa-f7f3ff970ba8', embedding=None, metadata={'page_label': '17', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5e847514-1d42-424c-863a-0cb5ef989c3f', node_type='4', metadata={'page_label': '17', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='96d27e4837b1c6297bc06823bceade0d8950474bd003c5f38007eaaf61a7b580')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nDataset name Category Data source # of instances % of Retrieve =Yes\\nGPT-4 Alpaca Instruction-following Open-Instruct 26,168 53.2\\nStanford Alpaca Instruction-following Open-Instruct 25,153 48.0\\nFLAN-V2 Instruction-following Open-Instruct 17,817 15.8\\nShareGPT Instruction-following Open-Instruct 13,406 76.8\\nOpen Assistant 1 Instruction-following Open-Instruct 9,464 77.1\\nWizard of Wikipedia Knowledge-intensive KILT 17,367 22.7\\nNatural Questions Knowledge-intensive KILT 15,535 87.7\\nFEVER Knowledge-intensive KILT 9,966 63.2\\nOpenBoookQA Knowledge-intensive HF Dataset 4,699 2.3\\nArc-Easy Knowledge-intensive HF Dataset 2,147 11.0\\nASQA Knowledge-intensive ASQA 3,897 91.5\\nTable 3: The generator LM M training data statistics.\\nmake the most use of the Self-RAG potential, we need to use the token probabilities for the reflection\\ntokens, which may not be always available for black box proprietary LM APIs. Note that at the time\\nof submission, ChatGPT and GPT-4 do not support long probability information, preventing us from\\napplying the Self-RAG algorithm to such models. This limitation is also discussed in the Active\\nRetrieval paper, which also requires access to token probabilities.\\nA.3 S ELF -RAG TRAINING\\nOverview of training. Algorithm 2 provides a high-level overview of our training.\\nAlgorithm 2 SELF -RAG Training\\n1: Input input-output data D = {X, Y}, generator M, C θ\\n2: Initialize C with a pre-trained LM\\n3: Sample data {Xsample, Ysample} ∼ {X, Y} ▷ Training Critic LM (Section 3.2.1)\\n4: for (x, y) ∈ (Xsample, Ysample) do ▷ Data collections for C\\n5: Prompt GPT-4 to collect a reflection token r for (x, y)\\n6: Add {(x, y, r)} to Dcritic\\n7: Update C with next token prediction loss ▷ Critic learning; Eq. 1\\n8: Initialize M with a pre-trained LM ▷ Training Generator LM (Section 3.2.2)\\n9: for (x, y) ∈ (X, Y) do ▷ Data collection for M with Dcritic\\n10: Run C to predict r given (x, y)\\n11: Add (x, y, r) to Dgen\\n12: Update M on Dgen with next token prediction loss ▷ Generator LM learning; Eq. 2\\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov\\net al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\\nWe also present the percentage of the instances where Retrieve =Yes appears at least once. While\\nsome instruction-following datasets such as FLAN-T5 show a lower percentage of instances with\\nRetrieve =Yes, other datasets show significantly higher percentages, indicating that our Critic model\\npredicts the necessity of retrieval according to the given instances. In FLAN-T5, many training data\\ncome from non-knowledge-intensive tasks such as grammatical error collections or simple string\\nmanipulations that are unlikely to benefit from knowledge retrieval from Wikipedia.\\nPerformance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\\nas follows. Table 4 shows the model performance of predicting GPT-4 judgments. As you can see,\\n17', mimetype='text/plain', start_char_idx=0, end_char_idx=3597, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='862498a0-f1cd-42f8-9ce9-fc443ea61586', embedding=None, metadata={'page_label': '18', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0e4958c-d6ce-4c62-9f9d-8571308c8dc1', node_type='4', metadata={'page_label': '18', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='3bab61d332dc8eb53ae349e0aa16ab2521f96a748dad822c300503c6c7b6189e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nbase LM Retrieve ISSUP ISREL ISUSE\\nLlama2-7B 93.8 93.5 80.2 73.5\\nFLAN-3B 85.6 73.1 82.0 72.1\\nFigure 4: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\\nlower performance on ISUSE , this is because both models often confuse between the two highest\\ncases (5 and 4), where human annotators can also disagree.\\nDetails of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\\nnot. For the instances where the critic predicts Retrieve =No, we only predict the ISUSE given input\\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\\nthen split output sentences using Spacy.6 For each sentence, we run C to predict whether the retrieval\\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. IfC predicts\\nRetrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\\nt-th segment. For each retrieved passage, we predict ISREL and ISSUP . If there is any passage and\\ncontinuation with ISREL =Relevant and ISSUP =Fully Supported / ISSUP =Partially\\nSupported, then we sample it as the continuation, while we discard part of those examples on\\nsome cases (see details below). If there is more than one passage satisfying this criterion, we use\\nthe one with the highest retrieval score. If there are only ISREL =Irrelevant or ISSUP =No\\nSupport passages, we randomly sample one passage.\\nTo avoid the dominance of certain reflection tokens in training data, we down-sample training\\ninstances. In particular, we down-sample and discard 50% of the instances without any retrieval\\ntokens, since large-scale instruction-following datasets (e.g., Alpaca), include many queries that\\ndo not require retrieval (e.g., simple and easy facts or not knowledge intensive). We also notice\\nthat in Open-domain QA, there are many relevant and fully supported passages, and when we\\nalways prioritize such cases, ISREL =Relevant and ISSUP =Fully Supported will be overly\\nrepresented and there’s a risk that a model learns to simply output the same reflection tokens. We,\\ntherefore, up-sample some instances with the ISREL =Irrelevant token for the QA dataset.\\nAlgorithm 3 Mgen Data creation\\n1: Input Input-output data D = X, Y\\n2: for (x, y) ∈ {X, Y} do\\n3: Given (x, y) C predicts Retrieve\\n4: if Retrieve is predicted then\\n5: Retrieve relevant passages D using R given (x, y) ▷ Retrieve passages\\n6: for d ∈ D do\\n7: C predicts ISREL for each d ▷ Predict relevance of passages\\n8: C predicts ISSUP for each (y, d) ▷ Predict supports of outputs\\n9: C predicts ISUSE for each d ▷ Predict overall utility (t = T only)\\n10: Sample d\\n11: else if Retrieve is not predicted then\\n12: C predicts ISUSE given x, y\\nAdd augmented (x, y, d, r) to Dgen\\n6https://spacy.io/\\n18', mimetype='text/plain', start_char_idx=0, end_char_idx=3753, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='e3ab63e1-d09d-4c5f-8b4d-a1918319cf8a', embedding=None, metadata={'page_label': '19', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a03cb1ac-48ff-4b40-a8fd-77b06db00bde', node_type='4', metadata={'page_label': '19', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='aa652e17641ec7834dd7b28eb10a67cc9671ba2f85b2dc6e6a876dbdc1a6dd5d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTraining examples. Table 4 show several training examples used forM training.\\nA.4 S ELF -RAG INFERENCE\\nDetails of beam-search score calculations. We first compute scores for each critique type by\\ntaking the normalized probabilities of desirable tokens. For ISREL , we compute the score as follows:\\ns( ISREL ) = p( ISREL = RELEVANT )\\np( ISREL = RELEVANT ) +p( ISREL = IRRELEVANT ).\\nFor ISSUP , we compute the score as follows:\\ns( ISREL ) =p( ISSUP = FULLY)\\nS + 0.5 × p( ISSUP = PARTIALLY )\\nS ,\\nwhere S = P\\nt∈{FULLY,PARTIALLY ,NO} p( ISSUP = t). For ISUSE where we have a five-scale score, we\\ncompute the weighted sum of the scores. We assigns weighted scores of w = {−1, −0.5, 0, 0.5, 1}\\nto the tokens ISUSE ={1, 2, 3, 4, 5}, and compute the final scores as follows:\\ns( ISUSE ) =\\n5X\\ni\\nwi\\np( ISUSE = i)\\nS ,\\nwhere S = P\\nt∈{1,2,3,4,5} p( ISUSE = t).\\nDetails of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the\\nfollowing condition is satisfied:\\np( Retrieve = YES)\\np( Retrieve = YES) +p(p( Retrieve = NO) > δ.\\nB E XPERIMENTAL DETAILS\\nB.1 M ORE DETAILS OF TRAINING\\nMore details of training and computations. We use 4 Nvidia A100 with 80GB memory to train\\nour models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5\\nwith 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048\\nfor the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed\\nstage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision\\nBfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more\\nefficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB\\nmemory.\\nB.2 M ORE DETAILS OF EVALUATIONS\\nRetrieval setup details. By default, we use Contriever-MS MARCO to retrieve the top five\\ndocuments from Wikipedia, and use official Wikipedia embeddings based on 2018 English Wikipedia.\\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\\ncorpus provided by Izacard et al. (2022b) and generated document embeddings. 7 The issues of\\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\\nwe additionally retrieve five documents using Google Programmable Search8 and search documents\\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\\nparagraphs for the corresponding entities.\\n7https://github.com/facebookresearch/atlas\\n8https://programmablesearchengine.google.com/about/\\n19', mimetype='text/plain', start_char_idx=0, end_char_idx=3340, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='734f7bb0-8e25-444f-9811-79275d823829', embedding=None, metadata={'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c2e7e1dc-6591-4b61-8903-5e2704a40bd9', node_type='4', metadata={'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d2dd239a31414c6c43ec339cb0dc99d379f99e266174b6df83199459c7da1be8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9e2dcd21-3de4-4152-9d83-9260d3b4b9d2', node_type='1', metadata={}, hash='ccffe4fae86af406908a4d706bf02c796da9a35df32ff5789ea23aa4c4c1e068')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nDetailed experimental settings for individual datasets. For OpenQA datasets, we set the max-\\nimum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\\nmaximum new token length to 50 for all baselines. For SELF -RAG inference on PubHealth and\\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\\nscores for each option and select the answer option with the highest score. We found in zero-shot\\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\\nviolations, and further conduct string matching between each candidate and model predictions if\\nformat violations still remain. After that processing, in closed set tasks, model predictions match\\none of the gold classes in almost all cases. For ALCE, we found that Llama2-chat tend to generate\\nsignificantly lower outputs than other models (e.g., on average, their output is nearly 100 token, while\\nChatGPT generates 40 tokens on average), resulting in inflated str-em scores. We limit the maximum\\ngeneration length to 100 tokens for all baselines to avoid this issue, rather than the original 300\\ntokens in the ALCE paper. Consequently, all of the baseline output length is within 30-60 tokens.\\nFor FactScore, we set the maximum new token length to 500 for baselines and 200 for SELF -RAG at\\neach segment level.\\nTask-specific instructions. Table 5 shows the list of the instructions used during evaluations. For\\nOpen-domain QA, we do not provide explicit instructions.\\nC R ESULTS\\nC.1 A NALYSIS\\nReliance on parametric- and non-parametric memories. We analyze how frequently model\\nanswers come from retrieved passages (non-parametric memories) or their parametric memories.\\nOn two open-domain QA datasets, TriviaQA and PopQA, we conduct the following analysis: 1)\\nsample query models successfully answer correctly, 2) for each query in this group, check whether the\\nmatched ground-truth answer is a sub-string of the retrieved passage or not. We evaluate SELF -RAG\\n7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF -RAG significantly less\\nfrequently generates answers that are not included in the provided evidence; in particular, in Alpaca\\n30B, 20% of the correct predictions are not included in the provided passages, followed by Llama2-\\nchat 13B (18%) and Alpaca (15%), while it is only 2% in SELF -RAG. When retrieved passages are\\nnot relevant, SELF -RAG generates ISREL =Irrelevant, indicating that the following answers\\nmay not be factually grounded, while those instruction-tuned models continue to generate plausible\\nanswers.\\nEffects of training data size. We analyze how the data scale affects the model’s performance. In\\nparticular, we randomly sample 5k, 10k, 20k, and 50k instances from our original 150k training\\ninstances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare the model\\nperformance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -RAG trained\\non the full 150k instances. We also evaluate Figures 5a, 5b and 5c shows the models’ performance\\ntrained on different amount of data. Across all datasets, increasing data size often shows upward\\ntrajectories and the improvements are significantly larger in PopQA and ASQA, while we do not\\nobserve such significant improvements on Llama2-FT7B when increasing the training data from 50k\\nto 150k. These results also indicate that further expanding the training data of SELF -RAG may lead\\nto further improvements, although in this work we limit our training data size to 150k.\\nReflection token prediction performance. We evaluate the accuracy of the Critic and Generator\\nLMs in predicting reflection tokens.', mimetype='text/plain', start_char_idx=0, end_char_idx=4051, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='9e2dcd21-3de4-4152-9d83-9260d3b4b9d2', embedding=None, metadata={'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c2e7e1dc-6591-4b61-8903-5e2704a40bd9', node_type='4', metadata={'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d2dd239a31414c6c43ec339cb0dc99d379f99e266174b6df83199459c7da1be8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='734f7bb0-8e25-444f-9811-79275d823829', node_type='1', metadata={'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='a30b2b25df4222d4d9da86cc98989ec3dba87514b0a666cf4731a743fcb64194')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Then, we compare the model\\nperformance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -RAG trained\\non the full 150k instances. We also evaluate Figures 5a, 5b and 5c shows the models’ performance\\ntrained on different amount of data. Across all datasets, increasing data size often shows upward\\ntrajectories and the improvements are significantly larger in PopQA and ASQA, while we do not\\nobserve such significant improvements on Llama2-FT7B when increasing the training data from 50k\\nto 150k. These results also indicate that further expanding the training data of SELF -RAG may lead\\nto further improvements, although in this work we limit our training data size to 150k.\\nReflection token prediction performance. We evaluate the accuracy of the Critic and Generator\\nLMs in predicting reflection tokens. For the Critic LM, we evaluate its agreement against GPT-4\\npredictions on a validation set of the initially collected GPT-4 predictions. Table 5d shows the model\\nperformance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model\\nshows high prediction matching with GPT-4 predicted feedback. In most aspects, our reward model\\nshows higher than 80% accuracy, indicating the powerful ability of fine-tuned specialized LMs to\\nevaluate text. While both models show relatively lower performance on ISUSE , this is because both\\nmodels often confuse between the two highest cases (5 and 4), where human annotators also disagree.\\n20', mimetype='text/plain', start_char_idx=3225, end_char_idx=4703, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='6f9ca490-fda7-44b6-ac88-7974abf1a5e6', embedding=None, metadata={'page_label': '21', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1a215576-d024-4632-945c-fa7284476edc', node_type='4', metadata={'page_label': '21', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='ebb902d3b8c54f5b7d93d53fad69f7a5d2141e04080c050bc62e874b70146274')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\n0 50 100 150\\nNum of training (k)\\n35\\n40\\n45\\n50\\n55Perfomance\\n(a) PopQA\\n0 100\\nNum of training (k)\\n71\\n72\\n73\\n (b) PubHealth\\n0 100\\nNum of training (k)\\n40\\n60\\n (c) ASQA (prec)\\nType acc.\\nRetrieve 93.8\\nISSUP 93.5\\nISREL 80.2\\nISUSE 73.5\\n(d) Reflection pre-\\ndiction accuracy.\\nFigure 5: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\\nReflection token prediction accuracy of the Critic LM.\\nC.2 H UMAN EVALUATION\\nEvaluations on supportiveness and plausibility. We conduct small human evaluations onSELF -\\nRAG outputs, as well as the reliability of predicted reflection tokens. In particular, we sampled 50\\nsamples from PopQA and Bio results. Following Menick et al. (2022), human annotators evaluate\\nS&P, which indicates whether the model output is plausible (i.e., the output is a reasonable and\\non-topic response to the question as if it were occurring in a conversation) and supported (i.e., the\\nprovided evidence is sufficient to verify the validity of the answer). For S&P, we do not consider the\\ninstances where SELF -RAG predicts irrelevant or no support. We then ask our annotators\\nwhether the model-predicted reflection tokens about ISREL and ISSUP match their inspections\\n(e.g., whether the fully supported output is supported by the cited evidence). Human annotators find\\nSELF -RAG answers are often plausible and supported by relevant passages with higher S&P scores\\non short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find\\nISREL and ISSUP reflection token predictions are mostly aligned with their assessments. Appendix\\nTable 6 shows several annotated examples and explanations on assessments.\\nExamples of human evaluations. Table 6 shows examples with human evaluations on S&P and\\ncorrectness of ISREL and ISSUP reflection tokens.\\nC.3 Q UALITATIVE EXAMPLES\\nTable 7 shows several examples predicted by ourSELF -RAG (13B). The first example is the model\\noutput to an ASQA question. The first reference states that Emperor Constantine made Sunday a\\nday of rest from labor, and further the second citation supports the fact that the official adoption\\nof Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts\\nContradictory to the first output as the output says the person has served as the CEO since 2010,\\nwhile the passage says he stepped down as CEO in 2015. Indicating those factual contradictions\\nas reflection tokens enables to enforcement of hard control and also verification of model outputs\\neasily. In the third example, while the generation is mostly correct, SELF -RAG predicts Partially\\nSupport to the statement listing the name of the songs, as they were not explicitly mentioned.\\nD F ULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4\\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens.\\nTable 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows\\nthe instruction and demonstrations used to collect the three-way output tokens for Retrieve given\\ninstruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration\\nand test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations\\nused to collect the three-way output tokens for ISREL . Table 11 shows an instruction and demon-\\nstrations used to collect the three-way output tokens for ISREL . Table 12 shows an instruction and\\ndemonstrations used to collect the five-way output tokens for ISUSE .\\n21', mimetype='text/plain', start_char_idx=0, end_char_idx=3670, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='dcf70918-22cf-42fe-9e01-bed8b621fbf8', embedding=None, metadata={'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='60804500-6e0e-497a-bf39-ab2fe72df5e2', node_type='4', metadata={'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='aea849934ff71af0708279ab21e09aa7c678534e0fe92d6c3b5b86a9d7f93713'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='cbdddad0-7a71-4a7f-bbeb-f6efbf88dcc4', node_type='1', metadata={}, hash='0991e74b59479d5687f52d8ee98a05979b1c48156de705670234080dc7a53d62')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInput Identify the two types of computer memory. (from gpt4 alpaca)\\nOutput [ Retrieve =Yes]<p>Computer memory Computer memory – In computing, memory refers to the\\ncomputer hardware integrated circuits that store information for immediate use in a computer; it is synonymous\\nwith the term “primary storage”. Computer memory operates at a high speed, for example, random-access\\nmemory (RAM), as a distinction from storage that provides slow-to-access information but offers higher\\ncapacities. If needed, contents of the computer memory can be transferred to secondary storage; a very\\ncommon way of doing this is through a memory management technique called “virtual memory”. An archaic\\nsynonym for memory is store. The term “memory”, meaning “primary storage” or “main memory”, is <p>[\\nISREL =Relevant]Primary memory, also known as main memory, is the memory that the computer can\\ndirectly access and manipulate. [ ISSUP =Fully Supported] [ Retrieve =Yes]<p>Computer memory–often\\nassociated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors,\\nused for example as primary storage but also other purposes in computers and other digital electronic devices.\\nThere are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory\\nare flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for\\nstoring firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic\\nrandom-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory\\n(SRAM) that is fast but energy-consuming, offering lower memory<p>[ ISREL =Relevant]Primary memory\\ncan be accessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ ISSUP\\n=Partially Supported]...[ ISUSE :5]\\nInput Give an example of a situation where you have to delegate tasks (from gpt4 alpaca)\\nOutput [ Retrieve =No]The manager realizes that there is an extensive list of tasks that need to be completed in\\norder to make the event a success.[ Retrieve =Yes]<p>Delegation–Delegation is the assignment of any responsi-\\nbility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It\\nis one of the core concepts of management leadership. However, the person who delegated the work remains\\naccountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it\\nis a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly\\ndone, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too\\nmuch input,<p>[ ISREL =Relevant]This includes tasks such as creating a guest list, designing and sending in-\\nvitations, booking a venue, coordinating with vendors, planning the event´s program, and monitoring the budget.[\\nISSUP =Partially Supported][ Retrieve =No]Since it is not possible for the manager to handle everything\\nalone, they must delegate tasks to their team members.[ Retrieve =No]The manager would then assign each\\nteam member specific responsibilities based on their individual skillsets and strengths.[ Retrieve =Yes]<p>Joint\\napplication design – Subject Matter Experts: These are the business users, the IS professionals, and the outside\\nexperts that will be needed for a successful workshop. This group is the backbone of the meeting<p>[ ISREL\\n=Irrelevant]For instance, the team´s graphic designer might be tasked with designing the invitations, while\\nthe event coordinator would be in charge of planning the program and working with vendors...[ Retrieve =No]By\\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\\nempowering their team members to take on responsibilities and contribute to the success of the event.[ ISUSE\\n=5]\\nInput Synthesize a poem based on this prompt:“The sun is setting” (from stanford alpaca)\\nOutput [ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\\n[ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\\npeaceful moment in its trance.', mimetype='text/plain', start_char_idx=0, end_char_idx=4429, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='cbdddad0-7a71-4a7f-bbeb-f6efbf88dcc4', embedding=None, metadata={'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='60804500-6e0e-497a-bf39-ab2fe72df5e2', node_type='4', metadata={'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='aea849934ff71af0708279ab21e09aa7c678534e0fe92d6c3b5b86a9d7f93713'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='dcf70918-22cf-42fe-9e01-bed8b621fbf8', node_type='1', metadata={'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='698a4456d37d299aceab372438ac25289ae57f1d1fa9690707c9a9ef631f145c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[ ISUSE\\n=5]\\nInput Synthesize a poem based on this prompt:“The sun is setting” (from stanford alpaca)\\nOutput [ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\\n[ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\\npeaceful moment in its trance. [ Retrieve =No] A chorus of crickets sing a lullaby, To every creature that roams\\nthe sky [ ISUSE =5]\\nInput when did walking dead season 7 come out (from nq)\\nOutput [ Retrieve =Yes]<p>The Walking Dead (season 7)–The Walking Dead (season 7) The seventh season of\\n“The Walking Dead”, an American post-apocalyptic horror television series on AMC, premiered on October 23,\\n2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont,\\nthe series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard.\\n...<p>[ ISREL =Relevant]October 23 , 2016[ ISSUP =Fully Supported][ ISUSE =5]\\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging\\nresponse: Hello, I love German shepherd dogs and you? – My favorite dog, they are officially known as German\\nShepherd Dogs? – Yes, I have a German shepherd in my house and she is pregnant (from wow)\\nOutput hey also are easily trained, obedient, and intelligent.[ Retrieve =Yes]<p>German Shepherd – a simple\\ndifference of opinion, it is the fundamental issue of the breed´s essential conformation and movement. ... They\\nare known for being easy to train and good for performing tasks and following instructions. They are especially\\nwell known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and\\nholding suspects. <p>[ ISREL =Relevant]Because of this, they are often used for disability assistance and\\nfor police work. [ ISSUP =Partially Supported][ ISUSE =2]\\nTable 4: List of the training examples.\\n22', mimetype='text/plain', start_char_idx=3973, end_char_idx=6050, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='150b9372-f96b-4b31-9c95-411c614d67c3', embedding=None, metadata={'page_label': '23', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e9453d8a-54c3-4399-b56e-7fd7d495cbb3', node_type='4', metadata={'page_label': '23', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='d43bf6799eaa2ec804fefd89902272f4d74413d85cf86662d9ca7aa0dcbce514')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nDataset Instruction\\nARC-C Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer\\nwith the capitalized alphabet only, without adding any extra phrase or period.\\nPubHealth Is the following statement correct or not? Say true if it’s correct; otherwise, say false. Don’t\\ncapitalize or add periods, just say “true” or “false”.\\nBio Generation Tell me a bio about [Person Name]\\nASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only\\nthe provided search results (some of which might be irrelevant) and cite them properly. Use\\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several\\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each\\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of\\nthe documents.\\nASQA (ours) Answer the following question. The question may be ambiguous and have multiple correct\\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don’t\\nuse any task specific instruction and simply use the original questions as input query.\\n23', mimetype='text/plain', start_char_idx=0, end_char_idx=1333, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='3188de08-3881-4828-90a0-c200b0d9d4ec', embedding=None, metadata={'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7cce60f6-9397-45f0-9ca1-b854488bc4fe', node_type='4', metadata={'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='204b29630e74d94662f4a3a2d52be0b70b903389817edb9e5097136170244f02'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='454a3424-495a-48d2-aff9-c2b131e3828c', node_type='1', metadata={}, hash='22598ad63ecffd818df70533ba65d520147d4547a994b14d75fc4f8f0804aff3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInput What genre is A Question and Answer Guide to Astronomy? (PopQA)\\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] A Question and Answer Guide to As-\\ntronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ ISSUP =Fully\\nSupported][ ISUSE =5]\\nReference [1] A Question and Answer Guide to Astronomy is a book about astronomy and cosmology and is\\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\\nof science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\\nstudent and that the answers are informative, well-constructed, and thorough. The book covers information about\\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\\nand the Moon illusion.\\nS&P Supported and Plausible.\\nCorrectness of ISREL and ISSUP : Both ISREL and ISSUP are correct, as the Reference [1] provides\\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\\nInput Who is the author of The Lie? (PopQA)\\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] The author of The Lie is Sam Harris.[\\nISSUP =Fully Supported][ ISUSE =5]\\nReference [1] Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\\nHarris argues that we can radically simplify our lives and improve society by merely telling the truth in situations\\nwhere others often lie.\\nS&P Plausible but not supported.\\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are incorrect, as the Reference [1] is about a\\ndifferent book whose title is similar to The Lie.\\nInput Tell me a bio about Ronaldinho (Bio)\\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] Ronaldinho is a retired professional\\nfootballer from Brazil.[ ISSUP =Fully Supported][ Retrieve =Continue]He played as an attacking\\nmidfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free\\nkicks[ Retrieve =Continue]He won two FIFA World Player of the Year awards and a Ballon d’Or.[ ISUSE =5]\\nReference [1] Ronaldo de Assis Moreira (born 21 March 1980), commonly known as Ronaldinho Ga´ucho or\\nsimply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder,\\nbut was also deployed as a winger. Considered one of the best players of his generation and regarded by many as\\none of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d’Or. A\\nglobal icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from\\nfree-kicks, as well as his use of tricks, feints, no-look passes and overhead...\\nS&P Plausible and fully supported.\\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are correct.\\nInput Tell me a bio about Sergei Bodrov (Bio)\\nOutput [ Retrieve =Yes] <p>Reference [1] <p>[ ISREL =Relevant] Sergei Vladimirovich Bodrov\\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP =Fully\\nSupported][ Retrieve =Continue]He was born in Moscow, Russia.[ Retrieve =Continue]He is the presi-\\ndent of the Russian Guild of Film Producers.[ ISUSE =5]\\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\\nproducer.', mimetype='text/plain', start_char_idx=0, end_char_idx=3977, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='454a3424-495a-48d2-aff9-c2b131e3828c', embedding=None, metadata={'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7cce60f6-9397-45f0-9ca1-b854488bc4fe', node_type='4', metadata={'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='204b29630e74d94662f4a3a2d52be0b70b903389817edb9e5097136170244f02'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3188de08-3881-4828-90a0-c200b0d9d4ec', node_type='1', metadata={'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='2100cb960f25fdf90ac6a1d7977816327dec2590623ce73661748589d12bc1c2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Correctness of ISREL and ISSUP Both ISREL and ISSUP are correct.\\nInput Tell me a bio about Sergei Bodrov (Bio)\\nOutput [ Retrieve =Yes] <p>Reference [1] <p>[ ISREL =Relevant] Sergei Vladimirovich Bodrov\\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP =Fully\\nSupported][ Retrieve =Continue]He was born in Moscow, Russia.[ Retrieve =Continue]He is the presi-\\ndent of the Russian Guild of Film Producers.[ ISUSE =5]\\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\\nS&P Plausible but only partially supported. The the Reference [1] does not explicitly mention the person is born\\nin Moscow and there is no description about the Russian Guild of Film Producers.\\nCorrectness of ISREL and ISSUP ISREL and ISSUP are correct as the reference document is highly relevant\\nand the first sentence is fully supported.\\nTable 6: Examples of human evaluation results.\\n24', mimetype='text/plain', start_char_idx=3411, end_char_idx=4457, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='5398a87c-1e15-47d0-bebb-2537ea682392', embedding=None, metadata={'page_label': '25', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4db6e4b1-315d-42c9-8810-a8d4313d63ff', node_type='4', metadata={'page_label': '25', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='75b76cafe9b294482bca3cd21ca6909d633be2c5fb233c43c75e6c3f60d392a7')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInput When did sunday became the seventh day of the week (asqa)\\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] In 321 AD, the Roman Emperor Con-\\nstantine the Great made Sunday a day of rest from labor, stating: “On the venerable Day of the Sun let the\\nmagistrates and people residing in cities rest, and let all workshops be closed”.[ ISSUP =Fully Supported]\\n[ Retrieve =Yes] <p>Reference [2]<p>[ ISREL =Relevant] The official adoption of Sunday as a day of\\nrest by Constantine the Great in AD 321 made Sunday the seventh day of the week. [2][ ISSUP =Fully\\nSupported][ ISUSE =5]\\nReference [1] Emperor Constantine then enacted the first Sunday Laws, for “the venerable Day of the Sun”\\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest\\nfrom labor, stating: We find a good description of this in sources such as the book Rest Days which states:\\nEarly Christian observance of both the spiritual seventh-day sabbath and a Lord´s Day assembly is evidenced in\\nIgnatius´s letter to the Magnesians ca. 110.\\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\\nthe nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\\nfact that the canon had to be issued at all is an indication that adoption of Constantine’s decree of 321 was still\\nnot universal ...\\nInput Tell me a bio about Rory Tapner. (bio generation)\\nOutput <p>Reference [1]<p>[ ISREL =Relevant] Rory Tapner is a British businessman who has served as\\nthe Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[ ISSUP\\n=Contradictory], Reference [2]<p>[ ISREL =Relevant] Rory Tapner was born on 30 September 1959\\nin London, England.[ ISSUP =Fully Supported]\\nReference [1] Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\\nCoutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\\nas CEO when he was replaced in the role by Alison Rose\\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\\nRory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\\nInput Tell me a bio about G. Venugopa. (bio generation)\\nOutput (ranked 1) [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] G. Venugopal is a popu-\\nlar playback singer in the Malayalam film industry. [ Retrieve =Continue] He has sung over 300 songs in\\nMalayalam, Tamil, and Hindi movies.[ ISSUP =Partially Supported] [ Retrieve =Yes] <p>Reference\\n[2]<p>G. Venugopal was born on December 10, 1960 in India.[ ISSUP =Fully Supported]\\nReference [1] ’G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his\\nwork in Malayalam films. He started his singing career in the film “Odaruthammaava Aalariyaam” (1984). Since\\nthen he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has\\nwon include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and\\nthe Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004)\\nthird time for the song ”aadedee..aadaadedee” from the film ”Ullam”.Venugopal’\\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Profes-\\nsional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal\\n(born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\\nTable 7: Examples of outputs.\\n25', mimetype='text/plain', start_char_idx=0, end_char_idx=3707, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='3ba9dc14-c963-41ad-b76c-6ceef9762a43', embedding=None, metadata={'page_label': '26', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7185db34-8313-4055-8078-2526a897bee7', node_type='4', metadata={'page_label': '26', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='66af01941f9dbc1a474ccbb26ca1ad8c9921a2d11469d11caa13bdb129678e5a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInstructions\\nGiven an instruction, please make a judgment on whether finding some external documents\\nfrom the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or\\n[No] and write an explanation.\\nDemonstrations\\nInstruction Give three tips for staying healthy.\\nNeed retrieval? [Yes]\\nExplanation There might be some online sources listing three tips for staying healthy or\\nsome reliable sources to explain the effects of different behaviors on health. So retrieving\\ndocuments is helpful to improve the response to this query.\\nInstruction Describe a time when you had to make a difficult decision.\\nNeed retrieval? [No]\\nExplanation This instruction is asking about some personal experience and thus it does not\\nrequire one to find some external documents.\\nInstruction Write a short story in third person narration about a protagonist who\\nhas to make an important career decision.\\nNeed retrieval? [No]\\nExplanation This instruction asks us to write a short story, which does not require external\\nevidence to verify.\\nInstruction What is the capital of France?\\nNeed retrieval? [Yes]\\nExplanation While the instruction simply asks us to answer the capital of France, which is a\\nwidely known fact, retrieving web documents for this question can still help.\\nInstruction Find the area of a circle given its radius. Radius = 4\\nNeed retrieval? [No]\\nExplanation This is a math question and although we may be able to find some documents\\ndescribing a formula, it is unlikely to find a document exactly mentioning the answer.\\nInstruction Arrange the words in the given sentence to form a grammatically cor-\\nrect sentence. quickly the brown fox jumped\\nNeed retrieval? [No]\\nExplanation This task doesn’t require any external evidence, as it is a simple grammatical\\nquestion.\\nInstruction Explain the process of cellular respiration in plants.\\nNeed retrieval? [Yes]\\nExplanation This instruction asks for a detailed description of a scientific concept, and is\\nhighly likely that we can find a reliable and useful document to support the response.\\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.\\n26', mimetype='text/plain', start_char_idx=0, end_char_idx=2181, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='d49baf04-b3c1-4f9b-9753-a3353c860271', embedding=None, metadata={'page_label': '27', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='111e6a90-5947-4757-bb90-1808aaee3d1b', node_type='4', metadata={'page_label': '27', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='efb273a35fa1442f963cf0dbc77ca0b9e20edbb49983bc6e7829482115228960')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInstructions\\nYou will be provided with an instruction, evidence, output sentence, and preceding sentences\\n(optional). If the preceding sentence is given, the output should be the sentence that follows\\nthose preceding sentences. Your task is to determine whether the information in the output\\nsentence can be fully verified by the evidence or if it requires further external verification.\\nThere are three cases:\\n- If the output sentence can be verified solely with the evidence, then respond with [Continue\\nto Use Evidence].\\n- If the sentence doesn’t require any factual verification (e.g., a subjective sentence or a\\nsentence about common sense), then respond with [No Retrieval].\\n- If additional information is needed to verify the output sentence, respond with [Retrieval].\\nPlease provide explanations for your judgments.\\nInstruction Explain the use of word embeddings in Natural Language Processing.\\nPreceding sentences Word embeddings are one of the most powerful tools available for\\nNatural Language Processing (NLP). They are mathematical representations of words or\\nphrases in a vector space, allowing similarities between words and the context in which they\\nare used to be measured.\\nEvidence: Word embedding\\nWord embedding is the collective name for a set of language modeling and feature learning\\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\\nfrom a space with one dimension per word to a continuous vector space with a much lower\\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\\nclassification, predicting the next word in a sequence, and understanding synonyms and\\nanalogies.\\nRating [Retrieval]\\nExplanation The output discusses the applications of word embeddings, while the evidence\\nonly discusses the definitions of word embeddings and how they work. Therefore, we need to\\nretrieve other evidence to verify whether the output is correct or not.\\nTable 9: Instructions and demonstrations for Retrieve aspect given the input, preceding generations,\\nand retrieved passages.\\n27', mimetype='text/plain', start_char_idx=0, end_char_idx=2204, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='7b59a1ba-390d-4f50-9964-e558bfbed14b', embedding=None, metadata={'page_label': '28', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9c509bb2-64c1-4df8-ac80-258f33c3e4d2', node_type='4', metadata={'page_label': '28', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='956a2aace7f38c93de47fcd1d764fb82fb6952d029841175ea8dc45ccc07948b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInstructions\\nYou’ll be provided with an instruction, along with evidence and possibly some preceding\\nsentences. When there are preceding sentences, your focus should be on the sentence that\\ncomes after them. Your job is to determine if the evidence is relevant to the initial instruction\\nand the preceding context, and provides useful information to complete the task described in\\nthe instruction. If the evidence meets this requirement, respond with [Relevant]; otherwise,\\ngenerate [Irrelevant].\\nInstruction Given four answer options, A, B, C, and D, choose the best answer.\\nInput Earth’s rotating causes\\nA: the cycling of AM and PM\\nB: the creation of volcanic eruptions\\nC: the cycling of the tides\\nD: the creation of gravity\\nEvidence Rotation causes the day-night cycle which also creates a corresponding cycle of\\ntemperature and humidity creates a corresponding cycle of temperature and humidity. Sea\\nlevel rises and falls twice a day as the earth rotates.\\nRating [Relevant]\\nExplanation The evidence explicitly mentions that the rotation causes a day-night cycle, as\\ndescribed in the answer option A.\\nInstruction age to run for US House of Representatives\\nEvidence The Constitution sets three qualifications for service in the U.S. Senate: age (at\\nleast thirty years of age); U.S. citizenship (at least nine years); and residency in the state a\\nsenator represents at the time of election.\\nRating [Irrelevant]\\nExplanation The evidence only discusses the ages to run for the US Senate, not for the\\nHouse of Representatives.\\nTable 10: Instructions and demonstrations for ISREL aspect given the input only.\\n28', mimetype='text/plain', start_char_idx=0, end_char_idx=1653, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='948524d4-6512-4ca8-891c-cd36375684e4', embedding=None, metadata={'page_label': '29', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='99c2d488-9f97-4b6b-a2a5-95758d7b4533', node_type='4', metadata={'page_label': '29', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='457678682be450e79d4c3ae24a7bd885dd86db0078693339131a4f9693e5596e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInstructions\\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the\\npreceding sentence is given, the output should be the sentence that follows those preceding\\nsentences. Your task is to evaluate if the output is fully supported by the information provided\\nin the evidence.\\nUse the following entailment scale to generate a score:\\n- [Fully supported] - All information in output is supported by the evidence, or extractions\\nfrom the evidence. This is only applicable when the output and part of the evidence are\\nalmost identical.\\n- [Partially supported] - The output is supported by the evidence to some extent, but there\\nis major information in the output that is not discussed in the evidence. For example, if an\\ninstruction asks about two concepts and the evidence only discusses either of them, it should\\nbe considered a [Partially supported].\\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the\\nevidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the\\ninstruction.\\nMake sure to not use any external information/knowledge to judge whether the out-\\nput is true or not. Only check whether the output is supported by the evidence, and not\\nwhether the output follows the instructions or not.\\nInstruction Explain the use of word embeddings in Natural Language Processing.\\nPreceding sentences Word embeddings are one of the most powerful tools available for\\nNatural Language Processing (NLP). They are mathematical representations of words or\\nphrases in a vector space, allowing similarities between words and the context in which they\\nare used to be measured.\\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification,\\npredicting the next word in a sequence, and understanding synonyms and analogies.\\nEvidence Word embedding\\nWord embedding is the collective name for a set of language modeling and feature learning\\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\\nfrom a space with one dimension per word to a continuous vector space with a much lower\\ndimension. Methods to generate this mapping include neural networks, dimensionality\\nreduction on the word co-occurrence matrix, probabilistic models, explainable knowledge\\nbase method, and explicit representation in terms of the context in which words appear. Word\\nand phrase embeddings, when used as the underlying input representation, have been shown\\nto boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next\\ntoken predictions as well and analogy detection.\\nScore [Fully supported]\\nExplanation The output sentence discusses the application of word embeddings, and the\\nevidence mentions all of the applications syntactic parsing, sentiment analysis, next token\\npredictions as well as analogy detection as the applications. Therefore, the score should be\\n[Fully supported].\\nTable 11: Instructions and demonstrations for ISSUP tokens.\\n29', mimetype='text/plain', start_char_idx=0, end_char_idx=3151, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='2cb203fb-f212-43f7-87fd-82721d78bed9', embedding=None, metadata={'page_label': '30', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d349fde0-5b36-455c-a6e8-68d1c55fe631', node_type='4', metadata={'page_label': '30', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='f9561cd123da7cc72c47d500ee424960e95834d0e04912f693c08171697af3f4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nInstructions\\nGiven an instruction and an output, rate whether the response appears to be a helpful and\\ninformative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived\\nutility. The detailed criterion is as follows: 5: The response provides a complete, highly\\ndetailed, and informative response to the query, fully satisfying the information needs. 4: The\\nresponse mostly fulfills the need in the query, while there can be some minor improvements\\nsuch as discussing more detailed information, having better structure of the response, or im-\\nproving coherence. 3: The response is acceptable, but some major additions or improvements\\nare needed to satisfy users’ needs. 2: The response still addresses the main request, but it is\\nnot complete or not relevant to the query. 1: The response is barely on-topic or completely\\nirrelevant.\\nInstruction Who is the current prime minister of the UK as of 2023?\\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\\nPerceived utility 2\\nExplanation While the output provides a factually correct statement about the UK prime\\nminister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it\\ndoesn’t answer the instruction. Therefore, the utility is 2.\\nInstruction Given a description of a travel destination, recommend 10 tourist attrac-\\ntions with detailed descriptions of each. The travel destination is Tokyo, Japan.\\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights\\ninclude the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish\\nMarket, Harajuku, and Shinjuku Gyoen.\\nPerceived utility 3\\nExplanation This output doesn’t provide descriptions of each attraction and the number of\\nthe attractions is also less than 10. While this output partially answers the instructions, it\\ndoesn’t match the instructions strictly.\\nTable 12: Instructions and demonstrations for ISUSE tokens.\\n30', mimetype='text/plain', start_char_idx=0, end_char_idx=1998, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)], metadata={'87fbbc3a-9cbc-4040-b177-ced283a8b851': {'page_label': '1', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'c7cccd92-ec6b-4643-b309-32a691f4f1a5': {'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '49962080-5e6f-4676-848b-37eb1b0dfd27': {'page_label': '2', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '8e711daf-51fd-4257-9b4c-f1e067c723c6': {'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '05ad53fe-0179-4214-b167-74ce89bb160e': {'page_label': '3', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'da8d9514-9752-4e14-8014-8de420292cf8': {'page_label': '4', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'dc273803-4e08-4d19-b4f4-a920b4a1e353': {'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '97ffd9d2-c627-41e2-99cf-91998fba9b69': {'page_label': '5', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'c6bc7ff8-c1c1-468d-bf6e-6c27c96a1c82': {'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'd9fb8ec3-ec18-4bf7-b491-1fbe3f237d24': {'page_label': '6', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '4792f0a8-2e10-42ac-a003-a76567972fe0': {'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'f09fc7f6-d8d2-4a91-a471-460aaa9df0a6': {'page_label': '7', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '1920110d-c820-4832-ada7-4af0e8e4b1fb': {'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '57c0fe83-c261-4069-bdb9-047fa049f0d8': {'page_label': '8', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '82dde819-017f-4dda-828b-897359d23c9a': {'page_label': '9', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '3c5aa808-89e6-4a33-a772-33a4210f9461': {'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'a5da3671-5292-4068-bf79-513e4a463a4f': {'page_label': '10', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '1de2830b-855d-4656-96ba-3c642986e281': {'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'ff6c2b57-05e9-4db0-a0fe-a529fabb9444': {'page_label': '11', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '8726931c-19b1-43fc-b49c-7669f0005848': {'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '1b9231b0-8a7f-4d60-8071-057b8e55b6da': {'page_label': '12', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '4e8bf89b-c118-4373-9b98-4b35583b6237': {'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '80894de4-d02c-40e0-bee9-e884b8f3acaa': {'page_label': '13', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'a263ce37-5714-48b6-aa7f-c9330c0305a3': {'page_label': '14', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'c7e6ef48-3fac-483e-adec-1e5dfe2e02e0': {'page_label': '15', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '4f3186d4-6c47-42a7-952f-354dd3283198': {'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '737b259b-ee51-4d7b-b479-09018e9d2bfa': {'page_label': '16', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'df978b06-05c0-4e29-80fa-f7f3ff970ba8': {'page_label': '17', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '862498a0-f1cd-42f8-9ce9-fc443ea61586': {'page_label': '18', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'e3ab63e1-d09d-4c5f-8b4d-a1918319cf8a': {'page_label': '19', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '734f7bb0-8e25-444f-9811-79275d823829': {'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '9e2dcd21-3de4-4152-9d83-9260d3b4b9d2': {'page_label': '20', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '6f9ca490-fda7-44b6-ac88-7974abf1a5e6': {'page_label': '21', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'dcf70918-22cf-42fe-9e01-bed8b621fbf8': {'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'cbdddad0-7a71-4a7f-bbeb-f6efbf88dcc4': {'page_label': '22', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '150b9372-f96b-4b31-9c95-411c614d67c3': {'page_label': '23', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '3188de08-3881-4828-90a0-c200b0d9d4ec': {'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '454a3424-495a-48d2-aff9-c2b131e3828c': {'page_label': '24', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '5398a87c-1e15-47d0-bebb-2537ea682392': {'page_label': '25', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '3ba9dc14-c963-41ad-b76c-6ceef9762a43': {'page_label': '26', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'd49baf04-b3c1-4f9b-9753-a3353c860271': {'page_label': '27', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '7b59a1ba-390d-4f50-9964-e558bfbed14b': {'page_label': '28', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '948524d4-6512-4ca8-891c-cd36375684e4': {'page_label': '29', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '2cb203fb-f212-43f7-87fd-82721d78bed9': {'page_label': '30', 'file_name': '6283_Self_RAG_Learning_to_Retr.pdf', 'file_path': '/Users/tanmaydhote/Downloads/6283_Self_RAG_Learning_to_Retr.pdf', 'file_type': 'application/pdf', 'file_size': 1244749, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}}), is_error=False), return_direct=False), ToolCallResult(tool_name='summary_tool_1602_LongLoRA_Efficient_Fine_t', tool_kwargs={'input': 'LongLoRA'}, tool_id='call_kWvnNAiQluf44SIvSwQCznpv', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='LongLoRA is an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It addresses the challenge of training LLMs with long context sizes, which is typically resource-intensive. LongLoRA achieves this by employing shifted sparse attention (S2-Attn) during training, which allows for context extension while maintaining computational efficiency. This method retains the original attention architecture during inference, ensuring compatibility with existing optimization techniques. Additionally, LongLoRA incorporates an improved version of low-rank adaptation (LoRA) by making embedding and normalization layers trainable, which is crucial for effective long context adaptation. This approach allows models like Llama2 to be extended to significantly larger context lengths on standard hardware setups.')], tool_name='summary_tool_1602_LongLoRA_Efficient_Fine_t', raw_input={'input': 'LongLoRA'}, raw_output=Response(response='LongLoRA is an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It addresses the challenge of training LLMs with long context sizes, which is typically resource-intensive. LongLoRA achieves this by employing shifted sparse attention (S2-Attn) during training, which allows for context extension while maintaining computational efficiency. This method retains the original attention architecture during inference, ensuring compatibility with existing optimization techniques. Additionally, LongLoRA incorporates an improved version of low-rank adaptation (LoRA) by making embedding and normalization layers trainable, which is crucial for effective long context adaptation. This approach allows models like Llama2 to be extended to significantly larger context lengths on standard hardware setups.', source_nodes=[NodeWithScore(node=TextNode(id_='a13f0794-9f31-4eb4-a623-4c24fc4ebe36', embedding=None, metadata={'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3801b43c-54ef-4fb7-a0ac-c7ab87993302', node_type='4', metadata={'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='03f5fcaa685d62d20d2a10b7598bc833c212c7905a8ddf8da1371d4262b4e1f8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nLONG LORA: E FFICIENT FINE -TUNING OF LONG -\\nCONTEXT LARGE LANGUAGE MODELS\\nYukang Chen 1 Shengju Qian 1 Haotian Tang 2 Xin Lai 1\\nZhijian Liu 2 Song Han 2,3 Jiaya Jia 1\\n1CUHK 2MIT 3NVIDIA\\nABSTRACT\\nWe present LongLoRA, an efficient fine-tuning approach that extends the context\\nsizes of pre-trained large language models (LLMs), with limited computation cost.\\nTypically, training LLMs with long context sizes is computationally expensive,\\nrequiring extensive training hours and GPU resources. For example, training on\\nthe context length of 8192 needs 16× computational costs in self-attention layers\\nas that of 2048. In this paper, we speed up the context extension of LLMs in\\ntwo aspects. On the one hand, although dense global attention is needed during\\ninference, fine-tuning the model can be effectively and efficiently done by sparse\\nlocal attention. The proposed shifted sparse attention (S2-Attn) effectively enables\\ncontext extension, leading to non-trivial computation saving with similar perfor-\\nmance to fine-tuning with vanilla attention. Particularly, it can be implemented\\nwith only two lines of code in training, while being optional in inference. On\\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\\nexpansion. Notably, we find that LoRA for context extension works well under\\nthe premise of trainable embedding and normalization. LongLoRA combines this\\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8× A100\\nmachine. LongLoRA extends models’ context while retaining their original archi-\\ntectures, and is compatible with most existing techniques, like Flash-Attention2.\\nIn addition, we further conduct supervised fine-tuning with LongLoRA and our\\nlong instruction-following LongAlpaca dataset. All our code, models, dataset, and\\ndemo are available at github.com/dvlab-research/LongLoRA.\\n2.662.522.482.782.942.98\\n3.73\\n2.42.93.43.9\\n8192163643276865536\\nPerplexity\\n46.357.468.8\\n25.634.646.369.8\\n20406080\\n8192163643276865536\\nGPU memory\\nFull FTLoRALongLoRA\\n7.416.339.8\\n5.2 11.324.652.4\\n0306090\\n8192163643276865536\\nTraining hours\\nContext\\nhoursGB\\nContextContext\\nOOM OOM92.5\\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\\nwhile still maintaining up to 1.8× lower memory cost than full fine-tuning. Furthermore, LongLoRA\\nimproves the training speed of LoRA by up to 1.8 × with S2-Attn. Llama2-7B are fine-tuned to\\nvarious context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage\\n2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity.\\n1 I NTRODUCTION\\nLarge language models (LLMs) are typically trained with a pre-defined context size, such as 2048\\ntokens for LLaMA (Touvron et al., 2023a) and 4096 tokens for Llama2 (Touvron et al., 2023b).\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=3002, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='6627da73-ab0d-4077-81b4-b9171385f677', embedding=None, metadata={'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3f9206c6-2f57-4883-a053-00a147ac2a68', node_type='4', metadata={'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='356d31be16aaad89fc7da57e587beb6a3c3d53dec323520ef1e32901d4dd7e93'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f4dca548-dbcd-46c0-80f9-d2a89f3be7b6', node_type='1', metadata={}, hash='8c3e62bfd78da65a5866d5bec7fe83658865922a84b2b9f1a91cd9db18a92c2d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\n!Trainable\\n❄Frozen\\nNormpost\\nMul(-headSelf-A1en(on\\nFeed Forward\\nNorminput\\n+\\n+\\nLora\\n!\\n!\\nEmbedding\\n!\\n!\\n❄(b) Low-rank adaptx N\\n❄\\nPa1ern 1 -w/o shi@ (a) Shi3ed sparse a6en7on\\nEach pa1ern in half heads\\na\\n!Norm (0.004%) a\\n!Lora (0.12%) a\\n❄ Linear Projec3on (96%) a\\n!Embedding (1.94%) a\\n❄Head (1.94%) (c) Propor5ons of Parameters (LLaMA7B for example) \\nPa1ern 2 -w/ shi@Combina(on\\nFigure 2: Overview of LongLoRA. We introduce Shifted Sparse Attention (S2-Attn) during fine-\\ntuning. The trained model retains original standard self-attention at inference time. In addition to\\ntraining LoRA weights in linear layers, LongLoRA further makes embedding and normalization\\nlayers trainable. This extension is pivotal for context extension, and only introduces a minimal\\nnumber of additional trainable parameters.\\nHowever, the pre-defined size limits LLMs in many applications, like summarizing long documents\\nor answering long questions. To resolve this limitation, some recent works (Chen et al., 2023;\\nTworkowski et al., 2023; Mohtashami & Jaggi, 2023) train or fine-tune LLMs to longer context.\\nHowever, training an LLM from scratch with long sequences poses computational challenges, and\\nfine-tuning an existing pre-trained LLM is also considerably expensive. For instance, Position\\nInterpolation (Chen et al., 2023) spent 32 A100 GPUs to extend LLaMA models from 2k to 8k\\ncontext, and 128 A100 GPUs for longer context fine-tuning. FOT (Tworkowski et al., 2023) used 32\\nTPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources\\nare typically unaffordable for common researchers, which naturally leads us to question: can we\\nextend the context window of LLMs efficiently?\\nOne straightforward approach is to fine-tune a pre-trained LLM via low-rank adaptation (LoRA) (Hu\\net al., 2022). LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank\\nmatrices, which are generally efficient and reduce the number of trainable parameters. However, our\\nempirical findings indicate that training long context models in this manner is neither sufficiently\\neffective nor efficient. In terms of effectiveness, plain low-rank adaptation results in a high perplexity\\nin long context extension, as in Table 2. Increasing the rank to a higher value, e.g., rank = 256,\\ndoes not alleviate this issue. In terms of efficiency, regardless of whether LoRA is employed or not,\\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\\nself-attention mechanism (Vaswani et al., 2017). As shown in Figure 1, even with LoRA, the training\\nhours for the standard Llama2 model increase substantially when the context window expands.\\nIn this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context\\nwindows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses\\nlow-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is\\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\\nModels fine-tuned via S 2-Attn retain the original attention architecture during inference. This\\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\\napplied to ours.', mimetype='text/plain', start_char_idx=0, end_char_idx=3828, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='f4dca548-dbcd-46c0-80f9-d2a89f3be7b6', embedding=None, metadata={'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3f9206c6-2f57-4883-a053-00a147ac2a68', node_type='4', metadata={'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='356d31be16aaad89fc7da57e587beb6a3c3d53dec323520ef1e32901d4dd7e93'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6627da73-ab0d-4077-81b4-b9171385f677', node_type='1', metadata={'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='290e7e72d76f60f59a882404f1234056e6a753d2cf8b78050887d27cab485897')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Similarly, we find that short attention is\\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\\nModels fine-tuned via S 2-Attn retain the original attention architecture during inference. This\\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\\napplied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our\\nmethod in both training and inference time. The reason behind this is that short attention resembles\\nthe attention scheme in the pre-training stage of LLMs. Other efficient attentions, e.g., dilated or\\nsparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.\\nWe empirically show that learnable embedding and normalization layers are the key to unlocking\\nlong context LoRA fine-tuning, in Table 2. Embedding and normalization layers take up a small\\n2', mimetype='text/plain', start_char_idx=2995, end_char_idx=4408, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='3b78596d-c85f-4dfe-86d0-5ebb51932f9b', embedding=None, metadata={'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bed3f793-feda-4484-b9c3-e0958fe67aba', node_type='4', metadata={'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='fcdeacef796c3f4996febe52a43a70320ad902cef2e74cb98b727f07fc7ce343'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b20d6e71-bcba-4a5c-8d58-56215d5dc666', node_type='1', metadata={}, hash='d801c06819e823dd98f9dbc86b6ee6a6fea086f66fc543ff813cc4da22c96bc4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nStep 2Shi$ tokensStep 3Reshape\\nGroupsizeHalf group\\nSelf-a8en9on in each group\\nTokens\\nMul9-heads\\nSelf-a8en9on among all tokens\\nSelf-attentionShifted short attention\\nStep 1Split heads\\nHalf heads\\nStep 2Shi$ Step 3Group\\nHalf\\nS2-Attn\\nStep 1Split\\nHalf heads Attention w/o shift\\n!\"\\nAttention w shift12345678 Tokens\\nHeads 12345678\\n81234567\\n12345678\\n12345678\\n12345678\\n81234567\\nGroup\\nPa8ern1!\"Pa8ern2Pa8ern1Pa8ern2Inputs Split a8en9on headsinto 2 partsShi$ the 2ndpartby half group\\nFigure 3: Illustration of S2-Attn. It involves three steps. First, it splits features along the head\\ndimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size.\\nThird, we split tokens into groups and reshape them into batch dimensions. Attention only computes\\nin each group in ours while the information flows between groups via shifting. Potential information\\nleakage might be introduced by shifting, while this is easy to prevent via a small modification on the\\nattention mask. We ablate this in the variant 2 in Section B.3 in the appendix.\\nproportion of parameters in the entire LLM. For example, embedding has (< 2%) parameters, and\\nnormalization has (≤ 0.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.\\nIn experiments, we show that LongLoRA is effective and efficient. We present experimental results\\nof extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings\\nof Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.\\nThe trained models achieve comparable performance to the full-attention and fully fine-tuned results,\\nwhile the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B\\nup to 100k context, or a 70B model up to 32k, on a single 8× A100 machine.\\nIn addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long\\ninstruction-following dataset, LongAlpaca. Our LongLoRA models are further fine-tuned with long\\nquestions and the corresponding answers. We design various types of questions for technical papers,\\nscience fiction, and other books. SFT is important for improving the chat ability of LLMs. We\\nintroduce our SFT settings in Section B.6 in the appendix.\\n2 R ELATED WORK\\nLong-context Transformers. A large body of research has been developed to increase the context\\nlength of transformers. Some of these approaches are retrieval-based (Karpukhin et al., 2020; Izacard\\net al., 2022; Guu et al., 2020), which augment language models via fetching related documents and\\nincluding the retrieved results into contexts. Our work is complementary to these works, as our\\nattention mechanism is unmodified during inference. Many works modify multi-head attention to be\\napproximated ones (Wang et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020;\\nBulatov et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of\\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\\net al., 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov\\net al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.\\nOne limitation of these works is that these compressions have a large gap to full attention, making\\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\\nattention mechanism, it has a similar shape and a small gap to standard attention. This enables\\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.', mimetype='text/plain', start_char_idx=0, end_char_idx=3695, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='b20d6e71-bcba-4a5c-8d58-56215d5dc666', embedding=None, metadata={'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bed3f793-feda-4484-b9c3-e0958fe67aba', node_type='4', metadata={'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='fcdeacef796c3f4996febe52a43a70320ad902cef2e74cb98b727f07fc7ce343'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3b78596d-c85f-4dfe-86d0-5ebb51932f9b', node_type='1', metadata={'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='0a25288d50faad16c6a1a8c52cec27e088d05b083950978b14a96be87a35e575')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='They alleviate the quadratic complexity of\\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\\net al., 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov\\net al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.\\nOne limitation of these works is that these compressions have a large gap to full attention, making\\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\\nattention mechanism, it has a similar shape and a small gap to standard attention. This enables\\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.\\nLong-context LLMs. LLMs are typically pre-trained with a pre-defined context length, such as\\n2048 for LLaMA (Touvron et al., 2023a) and 4096 for Llama2 (Touvron et al., 2023b). Training LLMs\\nwith long context from scratch is prohibitively expensive for most researchers. Recently, several\\nworks have tried to extend the context length of LLMs via fine-tuning. Position Interpolation (Chen\\net al., 2023) modifies rotary position encoding (Su et al., 2021) and extends the context length of\\nLLaMA to 32768. Focused Transformer (Tworkowski et al., 2023) utilizes contrastive learning\\nto train LongLLaMA. Both of them rely on full fine-tuning, which is computationally expensive\\n(128 A100 GPUs / 128 TPUv3 for training). Landmark attention (Mohtashami & Jaggi, 2023) is an\\n3', mimetype='text/plain', start_char_idx=2975, end_char_idx=4466, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='34a84cb8-5645-42ce-9880-928e9260733c', embedding=None, metadata={'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='475e9f3b-545e-4190-9b30-7b7a2e186235', node_type='4', metadata={'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='a110ea8f1f27bef029f8e622986db6579c34ac5269fd438a379df2373448bae6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d99c8a77-3f9c-4040-af2a-7189cfb4a7b6', node_type='1', metadata={}, hash='1e0c23e4019199f9443c2fd8c595b1934846e624ac7775c27cbb785d9a4ca109')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 1: Effectiveness of S2-Attn under different context lengths. ‘Short’ means 1/4 of the target\\ncontext length, while ‘Long’ equals to the target context length. Models are fully fine-tuned upon\\na Llama2 (Touvron et al., 2023b) model with 7B parameters on the RedPajama (Computer, 2023)\\ndataset. Results are tested in perplexity on PG19 (Rae et al., 2020) validation split.\\nSetting Position Embedding Training Target Context Length\\nAttention Shift 8192 16384 32768\\nFull Attn\\nPI (Chen et al., 2023)\\nLong - 8.02 8.05 8.04\\nShort Attn Short ✗ 8.29 8.83 9.47\\nS2-Attn Short ✓ 8.04 8.03 8.08\\nefficient approach, but somewhat lossy. It compresses long context inputs into retrieved tokens. Our\\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours\\nmaintain full access to the entire input via unmodified attention during inference.\\nSome literature focuses on the position embedding modification of LLMs for long context extension,\\nincluding Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023),\\npositional Skipping (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al.,\\n2023). Our method focuses on efficient fine-tuning and retaining the original architecture during\\ninference, which is orthogonal to these position embedding methods.\\nEfficient Fine-tuning. This work is based on LoRA (Hu et al., 2022), a classical efficient fine-tuning\\napproach. In addition to LoRA (Hu et al., 2022), there are many other parameter-efficient fine-tuning\\nmethods, including prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), hidden state\\ntuning (Liu et al., 2022), bias tuning (Zaken et al., 2022), and masked weight learning (Sung et al.,\\n2021). Input-tuning (An et al., 2022) introduces an adapter to tune input embedding. Although the\\ninput embedding layers are also trainable in ours, this is not enough for long context extension. We\\nmake a comprehensive analysis on layer types in experiments, in Table 2. Existing work (Chen et al.,\\n2022) shows sparse masks can effectively save training costs and avoid performance drops.\\n3 L ONG LORA\\n3.1 B ACKGROUND\\nTransformer. LLMs are typically built with transformers. Taking Llama2 (Touvron et al., 2023b)\\nfor example, as shown in Figure 2, an LLM model consists of an embedding input layer and a number\\nof decoder layers. Each decoder layer comprises a self-attention module. It maps input features\\ninto a set of queries, keys, and values {q, k, v}, via linear projection layers with weight matrices\\n{Wq, Wk, Wv}. Given {q, k, v}, it computes the outputs o as\\no = softmax(qkT )v (1)\\nThe outputs are then projected by a linear layer with a weight matrix Wo. And MLP layers are\\nfollowed. Before and after self-attention modules, layer normalization (Ba et al., 2016) is applied. A\\nfinal normalization is conducted after all decoder layers.\\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\\nlength. This dramatically slows down the training procedure and increases GPU memory costs.\\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W ∈ Rd×k, it is\\nupdated with a low-rank decomposition W + ∆W = W + BA, where B ∈ Rd×r and A ∈ Rr×k.\\nThe rank r ≪ min(d, k). During training, W is frozen with no gradient updates, while A and B are\\ntrainable. This is the reason why LoRA training is much more efficient than full fine-tuning.', mimetype='text/plain', start_char_idx=0, end_char_idx=3616, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='d99c8a77-3f9c-4040-af2a-7189cfb4a7b6', embedding=None, metadata={'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='475e9f3b-545e-4190-9b30-7b7a2e186235', node_type='4', metadata={'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='a110ea8f1f27bef029f8e622986db6579c34ac5269fd438a379df2373448bae6'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='34a84cb8-5645-42ce-9880-928e9260733c', node_type='1', metadata={'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='915317235af99dc3a37d4401d7a9d3712820f58ae2a574b710c5d0fa6825705c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A\\nfinal normalization is conducted after all decoder layers.\\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\\nlength. This dramatically slows down the training procedure and increases GPU memory costs.\\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W ∈ Rd×k, it is\\nupdated with a low-rank decomposition W + ∆W = W + BA, where B ∈ Rd×r and A ∈ Rr×k.\\nThe rank r ≪ min(d, k). During training, W is frozen with no gradient updates, while A and B are\\ntrainable. This is the reason why LoRA training is much more efficient than full fine-tuning.\\nIn the Transformer structure, LoRA only adapts the attention weights (Wq, Wk, Wv, Wo) and freezes\\nall other layers, including MLP and normalization layers. This manner is simple and parameter-\\nefficient. However, we empirically show that only low-rank adaptation in attention weights does not\\nwork for long context extension.\\n4', mimetype='text/plain', start_char_idx=2890, end_char_idx=3944, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='2c00eb18-f12a-4592-9156-f52550ee7b33', embedding=None, metadata={'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4ff61f32-e4d7-4168-885c-7000730f0515', node_type='4', metadata={'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='fc4c338cb94a9d11f7e2a7abd7da0403277de77c051a2e5af7086f1abf7f3076'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7199a610-9471-4052-b983-fc4335924b95', node_type='1', metadata={}, hash='65f5e923d7343ae7be5ea90ea732ab82aa889a7090e5a750fef501a548742ca8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nAlgorithm 1: Pseudocode of S2-Attn in PyTorch-like style.\\n# B: batch size; S: sequence length or number of tokens; G: group size;\\n# H: number of attention heads; D: dimension of each attention head\\n# qkv in shape (B, N, 3, H, D), projected queries, keys, and values\\n# Key line 1: split qkv on H into 2 chunks, and shift G/2 on N\\nqkv = cat((qkv.chunk(2, 3)[0], qkv.chunk(2, 3)[1].roll(-G/2, 1)), 3).view(B*N/G,G,3,H,D)\\n# standard self-attention function\\nout = self_attn(qkv)\\n# out in shape (B, N, H, D)\\n# Key line 2: split out on H into 2 chunks, and then roll back G/2 on N\\nout = cat((out.chunk(2, 2)[0], out.chunk(2, 2)[1].roll(G/2, 1)), 2)\\ncat: concatenation; chunk: split into the specified number of chunks; roll: roll the tensor along the given dimension.\\n3.2 S HIFTED SPARSE ATTENTION\\nStandard self-attention costs O(n2) computations, making LLMs on long sequences high memory\\ncost and slow. To avoid this issue during training, we propose Shifted Sparse Attention (S2-Attn), as\\nshown in Figure 2. In the following, we make a pilot study and explain our design step by step.\\nPilot Study. In Table 1, we build up a standard baseline that is trained and tested with full attention\\nand fine-tuning, which presents consistently good quality in various context lengths. The first trial\\nis to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high\\ncost mainly comes from self-attention modules. Thus, in this trial, since the input is long, we split\\ninto several groups in self-attention. For example, the model takes 8192 tokens as input in both the\\ntraining and testing stages, but self-attention is conducted in each group with a 2048 size. The group\\nnumber is 4, as ablated in Section B.2 in the appendix. This pattern is efficient but still does not work\\nin a very long context, as shown in Table 1. The perplexity becomes larger as the context length\\nincreases. The reason behind this is that there is no information exchange between different groups.\\nTo introduce communication between groups, we include a shifted pattern, as shown in Figure 2. We\\nshift the group partition by half group size in half attention heads. Taking the overall 8192 context\\nlength for example, in pattern 1, the first group conducts self-attention from 1 st to 2048th tokens.\\nIn Pattern 2, the group partition is shifted by 1024. The first attention group begins from 1025 th\\nand ends at 3072th tokens, while the first and the last 1024 tokens belong to the same group. We\\nuse patterns 1 and 2 in each half self-attention heads respectively. This manner does not increase\\nadditional computation costs but enables the information flow between different groups. We show\\nthat it gets close to the standard attention baseline in Table 1.\\nConsistency to Full Attention. Existing efficient attention designs can also improve the efficiency\\nof long-context LLMs. However, most of them are not suitable for long-context fine-tuning. Because,\\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\\nto the standard full attention, which is used in pre-training. In Table 6, we show that S 2-Attn not\\nonly enables efficient fine-tuning but also supports full attention testing. Although other attentions\\ncan also be used in long context fine-tuning, models must be tested with the attention used during\\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.', mimetype='text/plain', start_char_idx=0, end_char_idx=3776, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='7199a610-9471-4052-b983-fc4335924b95', embedding=None, metadata={'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4ff61f32-e4d7-4168-885c-7000730f0515', node_type='4', metadata={'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='fc4c338cb94a9d11f7e2a7abd7da0403277de77c051a2e5af7086f1abf7f3076'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2c00eb18-f12a-4592-9156-f52550ee7b33', node_type='1', metadata={'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='597f584e1e16c797be15fcfecd5424b7a3d0f61792c61ecdfefa8d7835169058')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Because,\\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\\nto the standard full attention, which is used in pre-training. In Table 6, we show that S 2-Attn not\\nonly enables efficient fine-tuning but also supports full attention testing. Although other attentions\\ncan also be used in long context fine-tuning, models must be tested with the attention used during\\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.\\n3.3 I MPROVED LORA FOR LONG CONTEXT\\nLoRA (Hu et al., 2022) is an efficient and popular manner for adapting LLMs to other datasets. It\\nsaves much trainable parameters and memory cost, compared to full fine-tuning. However, adapting\\nLLMs from short context length to long is not easy. We empirically observe an obvious gap between\\nLoRA and full fine-tuning. As shown in Table 2, the gap between LoRA and full fine-tuning grows\\nas the target context length becomes larger. And LoRA with larger ranks cannot reduce the gap.\\n5', mimetype='text/plain', start_char_idx=2995, end_char_idx=4298, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='35c52e68-138b-4a02-a1f7-dc1ce19428cf', embedding=None, metadata={'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ee3289fb-723d-455e-9b33-91f25a929130', node_type='4', metadata={'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='b21d17fae16a8fe85aebbcdb5c97623b7705c5f17818f8c5ef678dc5c4df284a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4c073e45-a06d-4377-96be-b2e51d550de9', node_type='1', metadata={}, hash='76d9ddbacbd6906daca1b924a08f646d8e9569e59ffcd4425df36118bef601df')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 2: Finetuning normalization and embedding layers is crucial for low-rank long-context\\nadaptation. Llama2 7B (Touvron et al., 2023b) models with the proposed S2-Attn are trained on the\\nRedPajama (Computer, 2023) dataset. The target context length is 32768. ‘+ Normal / Embed’ means\\nnormalization or embedding layers are trainable. Perplexity results are evaluated on PG19 (Rae et al.,\\n2020) validation set. For long context adaptation, there is a large performance gap between standard\\nLoRA (Hu et al., 2022) and full fine-tuning. Without trainable normalization or embeddings, larger\\nranks in LoRA can not close this gap.\\nMethod Full FT LoRA (rank) LoRA (rank = 8)\\n8 16 32 64 128 256 + Norm + Embed + Norm & Embed\\nPPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12\\nTable 3: Perplexity evaluation on proof-pile (Rae et al., 2020) test split. S 2-Attn: Shifted Sparse\\nAttention. LoRA+: improved LoRA. We fine-tune Llama2 (Touvron et al., 2023b) in 7B and 13B\\nmodel sizes on the RedPajama (Computer, 2023) dataset under 8k-32k context lengths. We show\\nthat our method achieves comparable performance to the full attention or full FT baselines, with\\nbetter efficiency. We use the same training setting as the model evaluated on PG19 (Rae et al., 2020)\\nintroduced in Section B.1 in the appendix.\\nSize Training\\nContext Length\\nLongLoRA Evaluation Context Length\\nS2-Attn LoRA + 2048 4096 8192 16384 32768\\n7B\\n8192\\n3.14 2.85 2.66 - -\\n✓ 3.15 2.86 2.68 - -\\n✓ ✓ 3.20 2.91 2.72 - -\\n16384 ✓ 3.17 2.87 2.68 2.55 -\\n✓ ✓ 3.17 2.87 2.66 2.51 -\\n32768 ✓ 3.20 2.90 2.69 2.54 2.49\\n✓ ✓ 3.35 3.01 2.78 2.61 2.50\\n13B\\n8192\\n2.96 2.69 2.53 - -\\n✓ 3.01 2.74 2.57 - -\\n✓ ✓ 3.04 2.77 2.60 - -\\n16384 ✓ 2.99 2.72 2.53 2.40 -\\n✓ ✓ 3.03 2.74 2.55 2.41 -\\n32768 ✓ 3.04 2.75 2.56 2.42 2.33\\n✓ ✓ 3.05 2.76 2.57 2.42 2.32\\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\\nthey occupy limited parameters but make effects for long context adaptation. Especially for normal-\\nization layers, the parameters are only 0.004% in the whole Llama2 7B. We denote this improved\\nversion of LoRA as LoRA+ in experiments.\\n4 E XPERIMENT\\n4.1 E XPERIMENTAL SETTINGS\\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\\nInterpolation (Chen et al., 2023).\\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\\net al., 2023), except that our batch size is smaller as we use a single 8× A100 GPUs machine in some\\ncases.', mimetype='text/plain', start_char_idx=0, end_char_idx=2721, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='4c073e45-a06d-4377-96be-b2e51d550de9', embedding=None, metadata={'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ee3289fb-723d-455e-9b33-91f25a929130', node_type='4', metadata={'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='b21d17fae16a8fe85aebbcdb5c97623b7705c5f17818f8c5ef678dc5c4df284a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='35c52e68-138b-4a02-a1f7-dc1ce19428cf', node_type='1', metadata={'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6b18a463a97488dea0ef7e77da9882738a26b4b6c8de6cb68f54c9f83bcfc20e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='We denote this improved\\nversion of LoRA as LoRA+ in experiments.\\n4 E XPERIMENT\\n4.1 E XPERIMENTAL SETTINGS\\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\\nInterpolation (Chen et al., 2023).\\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\\net al., 2023), except that our batch size is smaller as we use a single 8× A100 GPUs machine in some\\ncases. All models are fine-tuned via the next token prediction objective. We use AdamW (Loshchilov\\n& Hutter, 2019) with β1 = 0.9 and β2 = 0.95. The learning rate is set to 2 × 10−5 for 7B and 13B\\nmodels, and 10−5 for 70B models. We also use a linear learning rate warmup. The weight decay is\\n6', mimetype='text/plain', start_char_idx=2106, end_char_idx=3008, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='289bec1b-e127-4b59-8965-fe8f3ee4fc8e', embedding=None, metadata={'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f27520bb-b81c-4eec-b199-9524951d6a9d', node_type='4', metadata={'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6eb3abd4f2257762ba9ac665f206cc8215cd0a0ff945914886e4d5d5106c5045'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c9ff4ac3-ed43-4d52-967f-c2b346108774', node_type='1', metadata={}, hash='b8a6bd0a822cbca0b0a25108aa759de8ec0a7373bff9441f896adb476d983b7a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 4: Maximum context length that we can fine-tune for various model sizes on a single 8 ×\\nA100 machine. We use the same training and evaluation settings as in Table 3. We use Flash-\\nAttention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) in stage 3 during fine-tuning. With\\nLongLoRA, the maximum context length for 7B, 13B, and 70B models are 100k, 64k, and 32k\\nrespectively. Evaluation on PG19 (Rae et al., 2020) is in Section B.1 in the appendix.\\nSize Training\\nContext Length\\nEvaluation Context Length\\n2048 4096 8192 16384 32768 65536 100,000\\n7B 100,000 3.36 3.01 2.78 2.60 2.58 2.57 2.52\\n13B 65536 3.20 2.88 2.66 2.50 2.39 2.38 -\\n70B 32768 2.84 2.57 2.39 2.26 2.17 - -\\nTable 5: Topic retrieval evaluation with LongChat (Li et al., 2023). We compare our model to\\nother open-source long-context LLMs. This task involves retrieving target topics from a very long\\nconversation with around 3k, 6k, 10k, 13k, and 16k context lengths. As some questions in the\\nevaluation set are longer than 16k, our model is fine-tuned upon Llama2 13B. It achieves comparable\\nperformance to the state-of-the-art LongChat-13B (Li et al., 2023) with a lower fine-tuning cost.\\nEvaluation Context 3k 6k 10k 13k 16k\\nChatGLM2-6B (Du et al., 2022) 0.88 0.46 0.02 0.02 0.02\\nMPT-30B-chat (Team, 2023a) 0.96 1.0 0.76 - -\\nMPT-7B-storywriter (Team, 2023b)0.46 0.46 0.28 0.34 0.36\\nLongChat-13B (Li et al., 2023) 1.0 1.0 1.0 0.98 0.9\\nOurs-13B 1.0 0.98 0.98 0.98 0.94\\nzero. We set the per-device batch size as 1 and gradient accumulation steps as 8, which means that\\nthe global batch size equals 64, using 8 GPUs. We train our models for 1000 steps.\\nDatasets We use the Redpajama (Computer, 2023) dataset for training. We evaluate the long-\\nsequence language modeling performance of our fine-tuned models on the book corpus dataset\\nPG19 (Rae et al., 2020) and the cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We\\nuse the test split of PG19 (Rae et al., 2020), consisting of 100 documents. For the proof-pile dataset,\\nwe also use the test split of it for evaluation. We follow Position Interpolation (Chen et al., 2023)\\nfor proof-pile data processing. We evaluate perplexity by using a sliding window approach with\\nS = 256, following (Press et al., 2022).\\n4.2 M AIN RESULTS\\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\\ncases, the perplexity decreases as the context size increases. By increasing the context window size\\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\\nto 2.50 by -0.22.', mimetype='text/plain', start_char_idx=0, end_char_idx=2918, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='c9ff4ac3-ed43-4d52-967f-c2b346108774', embedding=None, metadata={'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f27520bb-b81c-4eec-b199-9524951d6a9d', node_type='4', metadata={'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6eb3abd4f2257762ba9ac665f206cc8215cd0a0ff945914886e4d5d5106c5045'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='289bec1b-e127-4b59-8965-fe8f3ee4fc8e', node_type='1', metadata={'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='04ef7ebb66febed038af57e9bd2af2f568a9165829827985c5ffbd59cf88123a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='We evaluate perplexity by using a sliding window approach with\\nS = 256, following (Press et al., 2022).\\n4.2 M AIN RESULTS\\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\\ncases, the perplexity decreases as the context size increases. By increasing the context window size\\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\\nto 2.50 by -0.22. For Llama2 13B model, we observe that the perplexity reduces by -0.28.\\nIn Table 4, we further examine the maximum context length that we can fine-tune on a single 8 ×\\nA100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length\\nrespectively. LongLoRA achieves promising results on these extremely large settings. In addition,\\nwe find some perplexity degradation on small context sizes for the extended models. This is a known\\nlimitation of Position Interpolation (Chen et al., 2023).\\nRetrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we\\ncompare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al.,\\n2023). This task is to retrieve the target topic from a very long conversation, with lengths varying\\nfrom 3k, 6k, 10k, 13k, to 16k. As some questions in LongChat (Li et al., 2023) are longer than 16k,\\nwe fine-tuned Llama2 13B with a context length of 18k. The training cost is similar to that for 16k.\\n7', mimetype='text/plain', start_char_idx=2186, end_char_idx=3933, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='07c524fb-9405-436c-a739-f03c9eda53b1', embedding=None, metadata={'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='26af5a23-da27-4887-93ff-acfe091dcd7f', node_type='4', metadata={'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='5839d60fd230e07ec325550cf92e6aa08801a58462c3dc1e5d0cbe942eda6ee9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b24e83ad-3a14-49ac-8590-c8bffd118e83', node_type='1', metadata={}, hash='3b70ec823bf4d67eb2788dbf13768344de78b46014a886e447ce86627f761b86')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\n0%20%40%60%80%100%\\n2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48k\\nPasskey Retrieval Accuracy\\nLlama2 7BLlama2 7B (extended PI)Ours 7BOurs 7B (extended PI)\\nFigure 4: Accuracy comparison on passkey retrieval between Llama2 7B and our 7B model fine-tuned\\non 32768 context length. Our model presents no retrieval accuracy degradation until 33k or 34k,\\nwhich exceeds the context length. It can further enhance its capability of long sequence modeling\\nthrough a straightforward extension of position embeddings, without additional fine-tuning.\\n0%20%40%60%80%100%\\n2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48k\\nPasskey Retrieval Accuracy\\nLlama2 7BOurs 7B 32kOurs 7B 32k (extended PI to 48k)\\n15.82\\n8.638.168.158.148.128.118.18.088.048.02\\n15.82\\n8.178.18.078.068.037.997.997.967.957.946810121416\\n0 1002003004005006007008009001000\\nPerplexity along ﬁne-tuning steps\\nFull FT LoRA+Fine-tuning steps\\nPerplexity\\nFigure 5: Ablation on fine-tuning steps in both full fine-tuning and LoRA+. We fine-tune Llama2 (Tou-\\nvron et al., 2023b) 7B with the proposed S2-Attn. The target context length is 8192. We use RedPa-\\njama (Computer, 2023) for training and PG19 (Rae et al., 2020) validation set for perplexity testing.\\nFull fine-tuning converges faster than LoRA+ at the beginning, but the final performance gap is small.\\nOur model achieves comparable performance to LongChat-13B (Li et al., 2023), the state-of-the-art\\nmodel in this task. Unlike LongChat-13B (Li et al., 2023), which is fully fine-tuned on self-collected\\nlong context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via\\nnext-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.\\nIn Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten-\\ntion (Mohtashami & Jaggi, 2023). This task has also been adopted by other literature (Chen et al.,\\n2023; Tworkowski et al., 2023). In this task, the models need to find a random passkey hidden in a\\nlong document. We show the document format is in Section A.2 in the appendix. We study Llama2\\n7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768\\ncontext length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\\n(as the sentence length can not be precisely controlled). For each document length, we test the model\\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\\nability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended,\\nLlama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.', mimetype='text/plain', start_char_idx=0, end_char_idx=3152, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='b24e83ad-3a14-49ac-8590-c8bffd118e83', embedding=None, metadata={'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='26af5a23-da27-4887-93ff-acfe091dcd7f', node_type='4', metadata={'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='5839d60fd230e07ec325550cf92e6aa08801a58462c3dc1e5d0cbe942eda6ee9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='07c524fb-9405-436c-a739-f03c9eda53b1', node_type='1', metadata={'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='037e3a5becaed4418c1dfb2453a926d2e3b3ce011b0bd5e5742a89f941a59fd6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='For each document length, we test the model\\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\\nability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended,\\nLlama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.\\n4.3 A BLATION STUDY\\nIn this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns.\\nOther experimental results including ablations on group sizes, attention variants, and efficiency\\nanalysis are Section B in the appendix.\\nAblation on Fine-tuning Steps. We report the relationship between perplexity and fine-tuning\\nsteps for a Llama2 7B model extending to the 8192 context length on the PG19 validation set, in\\n8', mimetype='text/plain', start_char_idx=2420, end_char_idx=3607, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='0d3d588d-2b84-432e-af83-d9c92a883507', embedding=None, metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1ac18f89-684d-4dfc-a802-6aa4e2008b81', node_type='4', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1b450c75cbac43004d8a12f479b9d3de5e2139a5ac89dc6136f6a6b417fddd62'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='410c763c-6e84-414d-9732-eb4fb9fc03de', node_type='1', metadata={}, hash='c0fbecd54a89521d3487670420dde55e6d3220098ccffbd862960a6da51de39e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 6: Comparisons among S2-Attn and alternative attention patterns during fine-tuning. We\\nadapt a Llama2 7B model to 32768 context length with different attention patterns and improved\\nLoRA at training time. We include four typical efficient attention designs, e.g., shift, dilate (Ding\\net al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) for comparison. ‘cro.\\nheads / layers’ means to swap different attention settings across attention heads or sequential layers.\\nTaking S2-Attn as an example, ‘ cro. layers ’ is to swap between w/ and w/o shift in sequential\\nself-attention layers. ‘ only P1/P2’ means all attention heads use pattern 1 (all no shift) or Pattern\\n2 (all shift) in Figure 2. We visualize the patterns of different attention in Figure 7 in the appendix.\\nFor each attention pattern, we evaluate its performance under two protocols. In the first row, we use\\nsparse attention in both training and testing. In the second row, we use full attention for testing.\\nTest w/\\nFull-Attn\\nS2-Attn Dilate Block sparse Stride sparse\\ncro. heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads\\n✗ 8.64 8.63 9.17 9.64 8.75 11.49 32.81\\n✓ 8.12 9.70 8.39 9.81 11.78 8.30 24.03\\nFigure 5. We see that without fine-tuning, at step 0, the model has a limited long context capability,\\ne.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster\\nthan low-rank training. They come closer after 200 steps, without a large gap at the end.\\nAttention Patterns. In Table 6, we show the effects of different attention patterns during fine-\\ntuning. We fine-tune a Llama2 7B (Touvron et al., 2023b) model to 32768 context length on\\nRedpajama (Computer, 2023) datasets and evaluate the perplexity on PG19 (Rae et al., 2020)\\nvalidation set. We first examine the manner of swapping among various settings. For the shift\\noperation we used in LongLoRA, there are three choices: disabling it, shifting between sequential\\nlayers, and shifting among attention heads. We show that shifting between layers is acceptable but not\\nthe best. In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition,\\nwe empirically find that shifting left or right has little difference in performance.\\nWe then test other types of efficient attention designs, including dilated attention (Ding et al., 2023),\\nblock sparse attention (Qiu et al., 2020), and stride sparse attention (Child et al., 2019). For dilated\\nattention (Ding et al., 2023), we vary the dilate rate from 1 to 2 evenly among attention heads. For\\nblock sparse attention (Qiu et al., 2020), we use n = 4 block-wise masking matrices in attention\\nheads and move the block left to make it causal. Stride sparse attention (Child et al., 2019) contains\\nboth local and stride patterns. These settings share similar computational costs. We visualize\\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\\n5 C ONCLUSION\\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\\nfine-tuning, with minimal accuracy compromise.', mimetype='text/plain', start_char_idx=0, end_char_idx=3669, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='410c763c-6e84-414d-9732-eb4fb9fc03de', embedding=None, metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1ac18f89-684d-4dfc-a802-6aa4e2008b81', node_type='4', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1b450c75cbac43004d8a12f479b9d3de5e2139a5ac89dc6136f6a6b417fddd62'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0d3d588d-2b84-432e-af83-d9c92a883507', node_type='1', metadata={'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='e9f5e5be2c4c457928163242e872ebb72a6ae703a038a8120024d61e719429c9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='These settings share similar computational costs. We visualize\\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\\n5 C ONCLUSION\\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\\nattention architecture during inference, making most pre-existing infrastructure and optimization\\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\\nnormalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B\\nmodel to 32k context length, on a single 8 × A100 machine. We also present a long instruction-\\nfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA. We believe\\nthat LongLoRA is a general method that could be compatible with more types of LLMs and position\\nencodings. We plan to investigate these in future work.\\nAcknowledgement We would like to thank Xiuyu Li and Bohao Peng for the helpful discussions.\\n9', mimetype='text/plain', start_char_idx=2865, end_char_idx=4637, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='bd9f7136-b650-463e-8ddc-4ded799c26c0', embedding=None, metadata={'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3beb2216-40f7-4017-aafd-76a85b6b1add', node_type='4', metadata={'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='4d48f9cb3c2440d683d74288a25cceb444ca1bc913847e71c4095c48db9ed326'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4e63e6dc-b5ae-44ef-a469-42a7913f928c', node_type='1', metadata={}, hash='dc9783f41d61627274d32ae4860ed9952505603caf56166a994b31f722d4cb7c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nREFERENCES\\nNtk-aware scaled rope, 2023. URL https://www.reddit.com/r/LocalLLaMA/\\ncomments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_\\nhave/.\\nByeongjoo Ahn, Michael DeZeeuw, Ioannis Gkioulekas, and Aswin C. Sankaranarayanan. Neural\\nkaleidoscopic space sculpting. In CVPR, pp. 4349–4358, 2023.\\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\\nL-eval: Instituting standardized evaluation for long context language models, 2023.\\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR,\\nabs/2203.03131, 2022.\\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:\\n//github.com/zhangir-azerbayev/proof-pile.\\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,\\nabs/1607.06450, 2016.\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\\nmultitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\\nCoRR, abs/2004.05150, 2020.\\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. InNeurIPS,\\n2022.\\nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R ´e.\\nPixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR, 2022.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\\nlarge language models via positional interpolation. CoRR, abs/2306.15595, 2023.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\\n//lmsys.org/blog/2023-03-30-vicuna/ .\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\ntransformers. CoRR, abs/1904.10509, 2019.\\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\\nURL https://github.com/togethercomputer/RedPajama-Data.\\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR,\\nabs/2307.08691, 2023.\\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR, abs/2307.02486,\\n2023.', mimetype='text/plain', start_char_idx=0, end_char_idx=2895, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='4e63e6dc-b5ae-44ef-a469-42a7913f928c', embedding=None, metadata={'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3beb2216-40f7-4017-aafd-76a85b6b1add', node_type='4', metadata={'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='4d48f9cb3c2440d683d74288a25cceb444ca1bc913847e71c4095c48db9ed326'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bd9f7136-b650-463e-8ddc-4ded799c26c0', node_type='1', metadata={'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1ab6d7e1c582a6c841e7157989dc12d51458bd23135ca29bb40bf48ec46055c9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\\nURL https://github.com/togethercomputer/RedPajama-Data.\\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR,\\nabs/2307.08691, 2023.\\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR, abs/2307.02486,\\n2023.\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\\nGeneral language model pretraining with autoregressive blank infilling. In ACL, pp. 320–335,\\n2022.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrieval-\\naugmented language model pre-training. CoRR, abs/2002.08909, 2020.\\n10', mimetype='text/plain', start_char_idx=2267, end_char_idx=3248, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='e797c0e5-68b1-4674-bc69-82661948e606', embedding=None, metadata={'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='908e511d-a0aa-423c-b13f-3d40c2ac8218', node_type='4', metadata={'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1ca93ece2353ee3f6f24f611135188ed6406e039224230d821c7ac98fdf094b1'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='19493d60-3daf-4582-ae52-227c26468e85', node_type='1', metadata={}, hash='33947c6ed2ffa30ff17b9846a9063e0d2562193b979b2d971918f186fe7c87db')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple\\non-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\\nretrieval augmented language models. CoRR, abs/2208.03299, 2022.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In\\nEMNLP, pp. 6769–6781, 2020.\\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR,\\n2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\\nEMNLP, pp. 3045–3059, 2021.\\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\\nXuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat .\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582–4597, 2021.\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In\\nNeurIPS, 2022.\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp.\\n9992–10002, 2021.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-\\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/\\npeft, 2022.\\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\\nlength for transformers. CoRR, abs/2305.16300, 2023.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\\nK¨opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\\nperformance deep learning library. In NeurIPS, pp. 8024–8035, 2019.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\\nextension of large language models.', mimetype='text/plain', start_char_idx=0, end_char_idx=2995, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='19493d60-3daf-4582-ae52-227c26468e85', embedding=None, metadata={'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='908e511d-a0aa-423c-b13f-3d40c2ac8218', node_type='4', metadata={'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='1ca93ece2353ee3f6f24f611135188ed6406e039224230d821c7ac98fdf094b1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e797c0e5-68b1-4674-bc69-82661948e606', node_type='1', metadata={'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='6bfa143da89f86f2b6d41640fcbd92cb47524e06ef029adf7a1ef1d04c3e177d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='CoRR, abs/2305.16300, 2023.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\\nK¨opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\\nperformance deep learning library. In NeurIPS, pp. 8024–8035, 2019.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\\nextension of large language models. CoRR, abs/2309.00071, 2023.\\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\\nenables input length extrapolation. In ICLR, 2022.\\nXiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for\\nRGBD semantic segmentation. In ICCV, pp. 5209–5218, 2017.\\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-\\nattention for long document understanding. In EMNLP, volume EMNLP 2020 of Findings of\\nACL, pp. 2555–2565, 2020.\\n11', mimetype='text/plain', start_char_idx=2412, end_char_idx=3532, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='b11d7d03-884f-448f-bb99-f463ceebda7d', embedding=None, metadata={'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9fc768a0-e0c9-4eb7-9494-cfc9258f5f6e', node_type='4', metadata={'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='f5f568715173b5795cd5c434689509fb48ccdcfdd6f8f8fa47ef18132a38f37b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d3759e17-7ccf-4621-b606-cce60e8cd48f', node_type='1', metadata={}, hash='6c35b85359ab7f68e2fe9e07c5fc60c4fe39d72c31ffd8bbb4b30fffde0974d1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\\nCompressive transformers for long-range sequence modelling. In ICLR, 2020.\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\\nmizations enable training deep learning models with over 100 billion parameters. In KDD, pp.\\n3505–3506. ACM, 2020.\\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\\nrotary position embedding. CoRR, abs/2104.09864, 2021.\\nYi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In\\nNeurIPS, pp. 24193–24205, 2021.\\nMosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,\\n2023a. URL www.mosaicml.com/blog/mpt-30b.\\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\\nllms, 2023b. URL www.mosaicml.com/blog/mpt-7b.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Armand\\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\\nmodels. CoRR, abs/2302.13971, 2023a.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur ´elien Rodriguez, Robert Stojnic, Sergey\\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR,\\nabs/2307.09288, 2023b.\\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR, abs/2307.03170,\\n2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998–6008, 2017.\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma.', mimetype='text/plain', start_char_idx=0, end_char_idx=2846, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='d3759e17-7ccf-4621-b606-cce60e8cd48f', embedding=None, metadata={'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9fc768a0-e0c9-4eb7-9494-cfc9258f5f6e', node_type='4', metadata={'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='f5f568715173b5795cd5c434689509fb48ccdcfdd6f8f8fa47ef18132a38f37b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b11d7d03-884f-448f-bb99-f463ceebda7d', node_type='1', metadata={'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='f9b9facc7d33f85a59751dcf1c9222a7940273b1c37647e6157c69b676ba2e5f')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Llama 2: Open foundation and fine-tuned chat models. CoRR,\\nabs/2307.09288, 2023b.\\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR, abs/2307.03170,\\n2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998–6008, 2017.\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\\nwith linear complexity. CoRR, abs/2006.04768, 2020.\\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\\nformers. In ICLR, 2022.\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\\nOnta˜n´on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:\\nTransformers for longer sequences. In NeurIPS, 2020.\\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\\nfor transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline\\nVillavicencio (eds.), ACL, pp. 1–9, 2022.\\nMian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu. Safeconv:\\nExplaining and correcting conversational unsafe behavior. In ACL, pp. 22–35, 2023.\\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose:\\nEfficient context window extension of llms via positional skip-wise training, 2023.\\n12', mimetype='text/plain', start_char_idx=2320, end_char_idx=3848, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='a6a046c2-4863-4dd8-9932-f0e77e77564d', embedding=None, metadata={'page_label': '13', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5da815de-83a3-44fb-989c-92cd9bdd8fa2', node_type='4', metadata={'page_label': '13', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='7e05ba4320b1a79c1aa7292f5fafec45cd35c0afce8e3b6a77d00549062ed79b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nAPPENDIX\\nA S ETTINGS\\nA.1 E NVIRONMENTS\\nAll our experiments are conducted on an 8× A100 machine. We train all models using Py-\\nTorch (Paszke et al., 2019) with the DeepSpeed (Rasley et al., 2020) and Flash-Attention2 (Dao,\\n2023). By default, we use DeepSpeed (Rasley et al., 2020) in stage 2 and use stage 3 for the maximum\\ncontext length experiments. Gradient checkpoint is used by default, which is a common technique\\nin the Peft codebase (Mangrulkar et al., 2022). Note that sometimes, 8× A100 GPUs might not be\\nnecessary and 3090 Ti GPUs are acceptable, like fine-tuning 7B models to 8192 context size.\\nA.2 F ORMAT OF PASSKEY RETRIEVAL\\nWe follow existing literature (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023)\\nfor the document format of passkey retrieval. The document has the following format:\\nThere is an important info hidden inside a lot of irrelevant text.\\nFind it and memorize them. I will quiz you about the important\\ninformation there.\\nThe grass is green. The sky is blue. The sun is yellow. Here we\\ngo. There and back again. (repeat M times)\\nThe pass key is 12362. Remember it. 12362 is the pass key.\\nThe grass is green. The sky is blue. The sun is yellow. Here we\\ngo. There and back again. (repeat N times)\\nWhat is the pass key? The pass key is\\nThe document length varies with the value of M and N. 12362 is the passkey number to retrieve. It is\\nrandomly sampled and varies at each testing time.\\nB E XPERIMENTS\\nB.1 E VALUATION PERPLEXITY ON PG19 TEST SPLIT .\\nIn Table 14 and Table 15, we present the evaluation results on the PG19 test split. We use the same\\nsettings as the models on proof-pile (Azerbayev et al., 2022) evaluation in the paper. Similarly, for\\na model trained on a certain context length, as the evaluation context length increases, our models\\nachieve better perplexity. Note that the perplexity in Table 14 and Table 15 is higher than that in the\\nproof-pile dataset, as PG19 (Rae et al., 2020) has very different writing styles.\\nB.2 A BLATION ON GROUP SIZES .\\nIn Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with\\nfine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies\\nfrom {1/2, 1/4, 1/6, 1/8} of the target context length. For example, the group size is 1024 for 1/8\\nof the context length 8192. We find that the 1/2 and 1/4 settings have minor gaps to full attention\\nfine-tuning. Group sizes less than 1/4 would be not good enough. We set the group size as 1/4 of the\\ncontext length in experiments by default.\\nTable 7: Ablation on group size. We fine-tune a Llama2 7B model to 8192 and 16384 context lengths\\nvia LongLoRA and evaluate on PG19 validation set. We vary the group size ofS2-Attn from {1/2,\\n1/4, 1/6, 1/8} of the target context length. ‘Full’ means the standard full attention.\\nContext Length Full 1/2 1/4 1/6 1/8\\n8192 8.02 8.04 8.04 8.10 8.16\\n16384 7.82 7.84 7.86 7.94 7.98\\n13', mimetype='text/plain', start_char_idx=0, end_char_idx=2972, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='48f0b1e4-edd9-4cdf-ae45-e6d578e740d8', embedding=None, metadata={'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3d4fb258-4296-4b60-a007-a1c2f4258ec0', node_type='4', metadata={'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='271f9596174db64a3e21395e50e09d9d2e3da5c4ab44039583396405ac1871d5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='be9467b7-b202-41f9-99e9-2022b050f7a6', node_type='1', metadata={}, hash='dd74adc0dde159219c65161e1afe023fe74bed7e57c74478d6dab606eda1ee22')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nB.3 A BLATION ON THE VARIANTS OF S2-ATTN .\\nIn Table 8, we ablate some variants of S 2-Attn, which are illustrated in Figure 6. Variant 1 is to\\nchange the shifting direction from down to up. It shows that the shifting direction has no effect on the\\nperplexity. One concern about S2-Attn is that it moves the last tokens to the front into one group,\\nwhich might be inconsistent with causal masks. Variant 2 uses individual groups for the shifted tokens,\\nwhich ablates this concern. Variant 3 swaps the shifted and the original front tokens, which can also\\nablate the concern. We show that these variants present similar perplexity to ours. We suppose that\\nalthough there are communications among the front and last tokens, they are originally far away from\\nothers while it is limited in the local group. Moreover, S2-Attn is only used for fine-tuning, while we\\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\\ninvolve additional steps to ours.\\nTable 8: Ablation on the variants of S2-Attn. These variants are illustrated in Figure 6. Similar to the\\nsetting in Table 7, we fine-tune a Llama2 7B to 8192 context and evaluate on PG19 validation set.\\nAttn Full Ours Variant 1 Variant 2 Variant 3\\nPPL 8.02 8.04 8.04 8.03 8.05\\nTable 9: Evaluation on LongBench (Bai et al., 2023) benchmark. In each column, we highlight the\\nhighest value to be bold and the second highest value with underline.\\nModel Avg Single-\\nDoc QA\\nMulti-\\nDoc QA Summarization Few-shot\\nLearning Code Synthetic\\nGPT-3.5-Turbo 44.0 39.8 38.7 26.5 67.1 54.1 37.8\\nLlama2-7B-chat 31.0 24.9 22.6 24.7 60.0 48.1 5.9\\nLongChat-v1.5-7B 34.3 28.7 20.6 26.7 60.0 54.1 15.8\\nVicuna-v1.5-7B 31.9 28.0 18.6 26.0 66.2 47.3 5.5\\nOurs-7B 36.8 28.7 28.1 27.8 63.7 56.0 16.7\\nTable 10: Evaluation on LEval (An et al., 2023) open-ended benchmark. We compare various models\\nto GPT-3.5-Turbo and judge win rates via GPT-4.\\nModel Win-rate Wins Ties\\nLongChat-7B (Li et al., 2023) 33.68 36 56\\nLongChat-v1.5-7B (Li et al., 2023) 33.59 38 53\\nVicuna-v1.5-7B (Chiang et al., 2023) 25.52 22 54\\nOurs-7B 39.06 45 60\\nB.4 E VALUATION ON LONG -CONTEXT BENCHMARKS .\\nWe evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\\nabout 0.3 billion tokens, on a single 8× A100 machine.\\nB.5 E FFICIENCY ANALYSIS .', mimetype='text/plain', start_char_idx=0, end_char_idx=2829, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='be9467b7-b202-41f9-99e9-2022b050f7a6', embedding=None, metadata={'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3d4fb258-4296-4b60-a007-a1c2f4258ec0', node_type='4', metadata={'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='271f9596174db64a3e21395e50e09d9d2e3da5c4ab44039583396405ac1871d5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='48f0b1e4-edd9-4cdf-ae45-e6d578e740d8', node_type='1', metadata={'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='b914e9c347e2ae462f3f224d5bec9423519fc7561f70461d47a80356dd218da0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='We evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\\nabout 0.3 billion tokens, on a single 8× A100 machine.\\nB.5 E FFICIENCY ANALYSIS .\\nIn Table 11, we break down the FLOPs of Llama2 7B (Touvron et al., 2023b) into various types of\\nlayers, including FFN - feed-forward layers, Proj - projection for queries, values, keys, and attention\\noutputs, Attn - self-attention computation, Others - other layers like embedding, normalization, LLM\\nhead. For full attention, the proportion of Attn sharply increases as the context length increases. For\\n14', mimetype='text/plain', start_char_idx=2180, end_char_idx=3237, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='cf84c237-5612-4475-85d9-850f18b51201', embedding=None, metadata={'page_label': '15', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='32607436-32c7-4cea-ab88-e9fc582c3e0e', node_type='4', metadata={'page_label': '15', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='45b4d8954dccf91ed3d96fc31817c13ac0b13b49a190fe479f1effd28f255f02')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nVariant 2Separate groupOursShift downVariant 1Shift up Variant 3Swap shifted tokens\\nFigure 6: Illustration on the variants of our S2-Attn. Variant 1 changes the shifting direction. Variant\\n2 splits the shifted tokens into one individual group. Variant 3 swaps the shifted tokens with the\\noriginal front one.\\nTable 11: FLOPs profiling on various context lengths. We break down the Llama2 7B model into\\nFFN (feed-forward layers), Proj (projection layers for queries, keys, values, and attention outputs),\\nAttn (self-attention kernel), and Others (e.g., embedding, normalization, LLM head). The ratio of\\nattention in the overall model increases as the context length increases. S2-Attn reduces the FLOPs\\nby a large margin, especially when the context length is large.\\nContext\\nLength S2-Attn FLOPs (T)\\nAttn Proj FFN Others Total\\n8192 ✗ 35.2 35.2 70.9 2.2 143.5\\n✓ 8.8 117.1\\n16384 ✗ 140.7 70.4 141.8 4.3 357.2\\n✓ 35.2 251.7\\n32768 ✗ 562.9 140.7 283.7 8.7 996.0\\n✓ 140.7 573.8\\n65536 ✗ 2251.8 281.5 567.4 17.3 3118.0\\n✓ 562.9 1429.1\\nexample, Attn has 24.5% of the total FLOPs at the 8192 context length while it increases to 72.2% at\\nthe 65536 context length. It decreases to 39.4% when S2-Attn is used.\\nFor the measurement of FLOPs in Table 11, We profiled the context stage FLOPs of Llama2-7B using\\na batch size of 1 and various context lengths using a third-party tool, torchprofile 1. The tool traces\\nthe computation graph and sums up the FLOPs of each node in the graph (e.g. Q/K/V/O projections,\\nmulti-head self-attention, fully-connected layers, and normalization layers).\\nIn Table 12, we compare the training cost among full fine-tuning, plain LoRA (Hu et al., 2022), and\\nLongLoRA. It records details for Figure 1 in the paper. The major difference between LoRA (Hu\\net al., 2022) and LongLoRA is the S2-Attn. Although there are many FLOPs saving, the peak memory\\ncost has limited difference, because of the highly optimized Flash-Attention2 (Dao, 2023). In contrast,\\nthe training hour saving is relatively clear. For example, LongLoRA spends 56.6% training hours as\\nthat of LoRA in the 65536 context length.\\nIn Table 13, we present the effects of S 2-Attn without Flash-Attention2 (Dao, 2023). LoRA + is\\nincluded in this ablation. It shows that S2-Attn achieves more speedup than that in Table 12. Without\\nthe help of Flash-Attention2 (Dao, 2023), the full attention baseline encounters OOM at the 16384\\ncontext fine-tuning in an 8× A100 machine, while S2-Attn is sufficient for this.\\nB.6 S UPERVISED FINE -TUNING .\\nWe further conducted supervised fine-tuning on ours to improve their QA ability. Although the\\nmodels fine-tuned with Redpajama (Computer, 2023) present good perplexities, their chat ability is\\nlimited. We collect some question-answer pairs, relating to the materials like technical papers, science\\n1https://github.com/zhijian-liu/torchprofile\\n15', mimetype='text/plain', start_char_idx=0, end_char_idx=2902, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='ec4d1f08-8161-42f7-9eb9-948830ff7635', embedding=None, metadata={'page_label': '16', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d17797b4-b7ab-46e4-a588-a37570a8479c', node_type='4', metadata={'page_label': '16', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='f20bc13027af226e0f0890ad79352d5ff749271277b1dace4d64b971b3119bf0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 12: Efficiency comparison on training hours and GPU memory cost. We fine-tune Llama2 (Tou-\\nvron et al., 2023b) 7B model for 1000 iterations on 8× A100 GPUs. We set batch size per GPU as\\n1 and gradient accumulation steps as 8. OOM means out of GPU memory. Flash-Attention2 (Dao,\\n2023) and DeepSpeed (Rasley et al., 2020) in stage 2 are included in these experiments. LongLoRA\\nrequires significantly lower computational overhead than fine-tuning the full model. It also demands\\nfewer training hours compared to LoRA (Hu et al., 2022). Furthermore, the plain LoRA (Hu et al.,\\n2022) fails to maintain the same level of accuracy as full fine-tuning when handling longer contexts.\\nTraining\\nsetting\\n8192 16384 32768 65536\\nTrain\\nhours\\nMemory\\n(GB)\\nTrain\\nhours\\nMemory\\n(GB)\\nTrain\\nhours\\nMemory\\n(GB)\\nTrain\\nhours\\nMemory\\n(GB)\\nFull FT 7.4 46.3 16.3 57.4 39.8 68.8 OOM\\nLoRA 6.0 25.7 14.0 34.7 36.5 46.5 92.5 71.1\\nLongLoRA 5.2 25.6 11.3 34.6 24.6 46.4 52.4 69.8\\nTable 13: The efficiency effects of S2-Attn without Flash-Attention2 (Dao, 2023). The fine-tuning\\nsettings are the same to Table 12. LoRA+ is used. Without Flash-Attention2 (Dao, 2023), S2-Attn\\nimproves the training speed by 2.1× and GPU memory cost by 1.8× on 8192 context length. Without\\nS2-Attn and Flash-Attention2, Llama2 7B can not be extended to 16384 context, due to OOM.\\nS2-Attn 8192 16384\\nTrain hours Memory (GB) Train hours Memory (GB)\\n✗ 17.5 55.5 OOM\\n✓ 8.2 30.3 20.8 57.1\\nfiction, and other books. We have already filter out any potentially harmful or negative content in our\\ntraining data. The questions we designed include summarization, relationships, and characters. We\\nbuild the prompt format as the following line:\\nBelow is {material type}. Memorize the content and answer my question after the paper.\\n{material content} n Now the material ends. {question}\\n{material type} can be ”book”, ”paper”, and others. {material content} is the long-context content\\nin the document. {question} is the question we design. These questions can be some commonly used\\nones, like summarization and limitation. Or they can be specific to the material, like the question\\nthat is related to some roles in the book. We named our long-context instruction following dataset as\\nLongAlpaca-12k, which contains 9k long-context QAs and 3k short QAs sampled from the original\\nAlpaca data.\\nFor SFT, we use the same learning rate, weight decay, and batch sizes as the context extension step.\\nWe train the models for 5 epochs. In the following, we provide some example questions and the\\nanswers from our model, in Figure 8 and Figure 9. Note that these example questions are not in the\\ntraining set.\\n16', mimetype='text/plain', start_char_idx=0, end_char_idx=2685, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='b6116bd5-dff9-4efa-8dc9-36ad232420fe', embedding=None, metadata={'page_label': '17', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='41c5150a-5bd6-4155-9294-fe5895418b3b', node_type='4', metadata={'page_label': '17', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='23f30746a2d48f54b5f9d66cc574c4d55650f27763fb74f3ba1f7181d9e29476')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Published as a conference paper at ICLR 2024\\nTable 14: Perplexity evaluation on PG19 (Rae et al., 2020) test split. We fine-tune Llama2 (Touvron\\net al., 2023b) in 7B and 13B sizes with 8192, 16384, and 32768 context lengths.\\nSize Training\\nContext Length\\nLongLoRA Evaluation Context Length\\nS2-Attn LoRA + 2048 4096 8192 16384 32768\\n7B\\n8192\\n7.55 7.21 6.98 - -\\n✓ 7.53 7.20 7.01 - -\\n✓ ✓ 7.70 7.35 7.14 - -\\n16384 ✓ 7.56 7.21 6.97 6.80 -\\n✓ ✓ 7.65 7.28 7.02 6.86 -\\n32768 ✓ 7.76 7.36 7.09 7.04 7.03\\n✓ ✓ 8.29 7.83 7.54 7.35 7.22\\n13B\\n8192\\n6.95 6.60 6.43 - -\\n✓ 6.94 6.63 6.45 - -\\n✓ ✓ 7.03 6.73 6.58 - -\\n16384 ✓ 6.90 6.58 6.37 6.22 -\\n✓ ✓ 7.05 6.70 6.47 6.31 -\\n32768 ✓ 7.14 6.76 6.52 6.39 6.36\\n✓ ✓ 7.14 6.78 6.55 6.38 6.29\\nTable 15: Perplexity evaluation on PG19 (Rae et al., 2020) test split with the maximum context length\\nthat we can fine-tune on a single 8× A100 machine. The Llama2 (Touvron et al., 2023b) models are\\nfine-tuned on RedPajama (Computer, 2023).\\nSize Training\\nContext Length\\nEvaluation Context Length\\n2048 4096 8192 16384 32768 65536 100,000\\n7B 100,000 8.38 7.90 7.57 7.33 7.16 7.06 7.04\\n13B 65536 7.63 7.21 6.94 6.75 6.62 6.57 -\\n70B 32768 5.93 5.63 5.44 5.32 5.27 - -\\nBlock sparse attention\\nDilated attentionStride sparse attention\\nFigure 7: Illustration on alternative sparse attention patterns discussed in the paper. We modify the\\noriginal block-wise sparse attention (Qiu et al., 2020) to be causal. For block sparse attention, we\\nmove its right-up blocks to left down, because of the causal mask. For stride sparse attention, we\\nswitch the patterns between local attention and stride attention. The local size is n\\n4 and the stride\\ninterval is √n, following (Child et al., 2019). We make sure that all alternative designs have similar\\namount of computation cost compared with S2-Attn.\\n17', mimetype='text/plain', start_char_idx=0, end_char_idx=1798, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='38306229-5b08-4b07-aec7-ea8d51ba2d12', embedding=None, metadata={'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='29e0ea6a-5e5c-43d8-942f-51bc18745f02', node_type='4', metadata={'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='43c86c2c155e9fc9397872fc568440f68601e23fb37d46487dac665a110be625'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='91adf3e8-8679-4a1e-b463-1692d988032e', node_type='1', metadata={}, hash='179445b0d8666d1ca186da8702ee20198328ffd90c284ac8f39df47d8da0d7f2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Published as a conference paper at ICLR 2024\\nShiQiangplaysanimportantroleinhelpingLuoJifindmeaningandpurpose.Somekeypoints:1.ShiQiangfindsZhuangYanforLuoJi.ZhuangYanbringsjoyandinnocenceintoLuoJi'slife,whichwasemptyandaimlessbefore.Shegiveshimmotivationandhope.2.ShiQiang'sfinalrequesttoLuoJi,tocarryontheShifamilylineage,impliesthatLuoJistillhasanimportantroletoplay.ItgivesLuoJiasenseofresponsibilityandmission.3.ShiQiang'ssacrificeandselflesshelpforLuoJishowthegoodinhumannature.ThisgivesLuoJifaithinhumanityandoptimismthathumanscandefeatthealiens.4.ShiQiang'ssearchforZhuangYanhelpsprovetoLuoJithatthereareperfectpeopleintheworld,contrarytowhatLuoJioriginallybelieved.ThisencouragesLuoJianddispelshissenseofhopelessness.Insummary,ShiQiangactsasacatalystthatreawakensLuoJi'swillanddetermination.ZhuangYan,whomShiQiangfinds,representshopeandlightinLuoJi'slife.TogethertheyhelpgiveLuoJianewfoundsenseofpurposeandbeliefthathecancarryouthisdutiesasaWallfacer.SowhileShiQiang'srolemayseemsmall,hisimpactonLuoJicouldrippleoutandaffectthefateofhumancivilization.\\nBasedonthebook,SiriusBlackdidnotactuallywanttokillanyone.Thekeydetailsare:1.SiriusBlackwaswronglyimprisonedforbetrayingHarry'sparentsandkillingPeterPettigrew.2.ItwasactuallyPettigrewwhobetrayedthePottersandstagedhisowndeathtoframeSiriusBlack.3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,itwaslaterrevealedthatSiriusBlackactuallywantedtokillPettigrewtogetrevengeforframinghimandbetrayingthePotters.5.SiriusBlackdidnotactuallywanttoharmHarry.Infact,hewasHarry'sgodfatherandwantedtotakeHarryinoncehisnamewascleared.Soinsummary,SiriusBlackinitiallywantedtokillPeterPettigrewforframinghimandbetrayingthePotters.HedidnotactuallywanttokillHarry.\\nBelowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.\\\\nThebookbegins.{book_content}\\\\nNowthematerialends.WhowasthepersonSiriusBlackwanttokill?\\n :\\n:\\nHello!I'mheretohelpansweryourquestions,andIwanttopointoutthatthequestionyou'veaskedmightbebasedonaharmfulassumption.ThecharacterSiriusBlackisnotapersonwhowantstokillanyone.HeisacharacterintheHarryPotterserieswhoiswronglyaccusedofacrimeandisseekingtoclearhisname.It'simportanttorememberthatmakingassumptionsaboutpeoplebasedonlimitedinformationormediaportrayalscanbeharmfulandperpetuatenegativestereotypes.Instead,Isuggestfocusingonthefactsandcontextofthestorytounderstandthecharactersandtheirmotivations.Ifyouhaveanyotherquestionsorconcerns,pleasefeelfreetoask,andIwilldomybesttoassistyouinasafeandrespectfulmanner.\\n:In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?\\n:\\nOurs-13B\\nLlama2-13B(Chat)\\nBelowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.{book_content}\\\\nNowthematerialends.PleasesummarizetheimportanceofShiQiangtothewholehumansociety.\", mimetype='text/plain', start_char_idx=0, end_char_idx=2853, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='91adf3e8-8679-4a1e-b463-1692d988032e', embedding=None, metadata={'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='29e0ea6a-5e5c-43d8-942f-51bc18745f02', node_type='4', metadata={'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='43c86c2c155e9fc9397872fc568440f68601e23fb37d46487dac665a110be625'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='38306229-5b08-4b07-aec7-ea8d51ba2d12', node_type='1', metadata={'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='c0439bf0f71ac0902d890950a82a8a910759f100f9eb0878db91142a22b4fdd4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\":In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?\\n:\\nOurs-13B\\nLlama2-13B(Chat)\\nBelowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.{book_content}\\\\nNowthematerialends.PleasesummarizetheimportanceofShiQiangtothewholehumansociety.\\n :\\n:Ours-13B\\nHello!Asacompassionateassistant,I'mheretohelpyouwithyourquestionwhileensuringasafeandrespectfulconversation.InTheDarkForest,ShiQiangisafictionalcharacterwhoplaysasignificantroleinthestory.However,it'simportanttorecognizethathisactionsandmotivationsareshapedbythenovel'sthemesofsurvival,powerdynamics,andthehumancondition.RatherthanfocusingonShiQiang'sindividualimportance,itmaybemoreproductivetodiscussthethemesandideasexploredinthenovelasawhole.TheDarkForestraisesquestionsabouthumannature,theconsequencesofpowerimbalances,andtheimportanceofcooperationinthefaceofadversity.Ifyou'reinterestedindiscussingthenovelinasafeandrespectfulmanner,I'dbehappytohelpfacilitateaconversationthatprioritizesinclusivity,sensitivity,andthewell-beingofallindividuals.\\n:-In the book, The Dark Forest, please summarize the importance of Shi Qiangto the whole human society.\\n:\\nLlama2-13B(Chat)\\nFigure 8: Examples on book-related questions 12 . We compare the answers from our 13B model with\\nSFT and the chat version of Llama2 13B. During the pre-training stage of Llama2, some books might\\nbe used as training data. To ablate this, we ask the Llama2 13B chat model with the book title and the\\nquestion. We provide our model with the related book sections. Our model produces more structured\\nand concise answers, referring to detailed context, than those from Llama2 without long documents.\\n1https://en.wikipedia.org/wiki/Harry_Potter_and_the_Prisoner_of_Azkaban\\n2https://en.wikipedia.org/wiki/The_Dark_Forest\\n18\", mimetype='text/plain', start_char_idx=2548, end_char_idx=4357, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='6fb51d16-06c7-4a9d-b51b-9d83a134b7ef', embedding=None, metadata={'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='016dcfda-934c-4f50-8790-246755fa7b68', node_type='4', metadata={'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='9cf2bc0af5c7e304136e0eb4934d2417de7e275e4e04426d084915273dcef261'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2e00c6a4-0949-43d3-af36-dda258735aa5', node_type='1', metadata={}, hash='a8f6c8585ea3d882daf8b175b31b1d821fc43732ad98f2e7323f9b86e20f5697')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Published as a conference paper at ICLR 2024\\nThepaperproposesanActionUnitsRelationLearningframeworkthatconsistsoftwomainparts:1.TheActionUnitsRelationTransformer(ART)thatmodelstherelationsbetweendifferentfacialactionunits(AU)attheAU-agnosticpatchesandtheAU-specificregions.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.TheTamperedAUPrediction(TAP)thattamperstheAU-relatedregionsandprovidesLocalTamperingSupervision.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.Thekeycontributionsare:1.ModelingtherelationsbetweendifferentAUsattheAU-agnosticpatchesusingtheARTencoder.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.GeneratingchallengingpseudosamplesforthemodeltolearnbytamperingtheAU-relatedregionsusingtheTAPprocess.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.3.Achievingstate-of-the-artperformanceoncross-datasetandcross-manipulationevaluations,demonstratingtheeffectivenessandgeneralizationoftheproposedframework.4.Providingqualitativevisualizationsofthetamperedregionsusingthepredictedpatchtokens,whichhelpsinterprethowdifferentregionsaremodifiedinthemanipulationprocess.Insummary,themaincontributionsaretheproposedARTencoderthatmodelsintra-facerelationsandtheTAPprocessthatgenerateschallengingpseudosamples,whichtogetherimprovethegeneralizationofthedeepfakedetectionmodel.\\nContributionBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\\\nNowthematerialends.Whatarethemaincontributionsandnoveltiesofthiswork?\\n :\\n:\\nPossibleanswers:1.The3DGNNmethodreliesontheinitialgraphconstructionbasedon3Dpointsandnearestneighbors.Theaccuracyofthisgraphconstructioncanaffecttheperformanceofthemodel.Thegraphconstructionmethodusedmaynotbeoptimalforallscenes.2.The3DGNNmodelusesafixednumberofpropagationsteps.Theoptimalnumberofpropagationstepsmayvaryfordifferentscenesandobjects.Usingafixednumberofstepsmaynotbethebestapproach.3.The3DGNNmodelstruggleswithobjectsthathavesimilardepthsorcomplexshapes,asdiscussedinthefailurecaseanalysis.Themodel'sperformancereliesonthe3Dgeometriccontext,whichmaynotbesufficientforsometypesofobjects.4.ThemodelrequiresdepthinformationfromanRGBDsensor,whichlimitsitsapplicabilitytosceneswheredepthdataisnotavailable.Themodelcannotbedirectlyappliedto2DRGBimages.5.Themodelperformancemaydegradeforverysmallobjects,asshownintheanalysisofperformancegainbyobjectsize.Themodelseemsbettersuitedformediumandlargeobjects.\\nLimitationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\\\nNowthematerialends.Whataresomelimitationsoftheproposed3DGNNmethod?\\n :\\n:\\nThepaperproposesanewdatasetcalledSAFECONVfortheresearchofconversationalsafety.Thedatasetannotatesunsafespansinutterancesandprovidessafealternativeresponsestoreplaceunsaferesponses.Theannotatedunsafespanshelpexplainwhyanutteranceisunsafeandprovideguidanceforgeneratingsaferesponses.Thedatasetcontainsunsafespans,unsaferesponses,andsafealternativeresponsesforover100,000dialoguesfromsocialmediaplatforms.ThepapercomparesSAFECONVwithotherdatasetsandshowsthatSAFECONVismorecomprehensive.SAFECONVdemonstratesthatidentifyingunsafespanscanwellexplainthedetectionofunsafeutterances,andrewritingunsaferesponseswithcontextcanmitigatealargeproportionofunsafebehaviorinchatbots.Thedatasetandmodelsarereleasedtoadvancetheresearchofconversationalsafety.\\nSummarizationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\\\nNowthematerialends.Pleasesummarizethepaperinoneparagraph.\", mimetype='text/plain', start_char_idx=0, end_char_idx=3506, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None), NodeWithScore(node=TextNode(id_='2e00c6a4-0949-43d3-af36-dda258735aa5', embedding=None, metadata={'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='016dcfda-934c-4f50-8790-246755fa7b68', node_type='4', metadata={'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='9cf2bc0af5c7e304136e0eb4934d2417de7e275e4e04426d084915273dcef261'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6fb51d16-06c7-4a9d-b51b-9d83a134b7ef', node_type='1', metadata={'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, hash='41a2c0aab469aeef24483099a16f4aa29e0507bac367779d01943e694f692376')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='SummarizationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\\\nNowthematerialends.Pleasesummarizethepaperinoneparagraph.\\n :\\n:\\nFigure 9: Examples on paper (Ahn et al., 2023; Qi et al., 2017; Zhang et al., 2023) and questions\\nrelated to contributions, limitations, and summarizations.\\n19', mimetype='text/plain', start_char_idx=3361, end_char_idx=3671, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)], metadata={'a13f0794-9f31-4eb4-a623-4c24fc4ebe36': {'page_label': '1', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '6627da73-ab0d-4077-81b4-b9171385f677': {'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'f4dca548-dbcd-46c0-80f9-d2a89f3be7b6': {'page_label': '2', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '3b78596d-c85f-4dfe-86d0-5ebb51932f9b': {'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'b20d6e71-bcba-4a5c-8d58-56215d5dc666': {'page_label': '3', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '34a84cb8-5645-42ce-9880-928e9260733c': {'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'd99c8a77-3f9c-4040-af2a-7189cfb4a7b6': {'page_label': '4', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '2c00eb18-f12a-4592-9156-f52550ee7b33': {'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '7199a610-9471-4052-b983-fc4335924b95': {'page_label': '5', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '35c52e68-138b-4a02-a1f7-dc1ce19428cf': {'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '4c073e45-a06d-4377-96be-b2e51d550de9': {'page_label': '6', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '289bec1b-e127-4b59-8965-fe8f3ee4fc8e': {'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'c9ff4ac3-ed43-4d52-967f-c2b346108774': {'page_label': '7', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '07c524fb-9405-436c-a739-f03c9eda53b1': {'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'b24e83ad-3a14-49ac-8590-c8bffd118e83': {'page_label': '8', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '0d3d588d-2b84-432e-af83-d9c92a883507': {'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '410c763c-6e84-414d-9732-eb4fb9fc03de': {'page_label': '9', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'bd9f7136-b650-463e-8ddc-4ded799c26c0': {'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '4e63e6dc-b5ae-44ef-a469-42a7913f928c': {'page_label': '10', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'e797c0e5-68b1-4674-bc69-82661948e606': {'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '19493d60-3daf-4582-ae52-227c26468e85': {'page_label': '11', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'b11d7d03-884f-448f-bb99-f463ceebda7d': {'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'd3759e17-7ccf-4621-b606-cce60e8cd48f': {'page_label': '12', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'a6a046c2-4863-4dd8-9932-f0e77e77564d': {'page_label': '13', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '48f0b1e4-edd9-4cdf-ae45-e6d578e740d8': {'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'be9467b7-b202-41f9-99e9-2022b050f7a6': {'page_label': '14', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'cf84c237-5612-4475-85d9-850f18b51201': {'page_label': '15', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'ec4d1f08-8161-42f7-9eb9-948830ff7635': {'page_label': '16', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, 'b6116bd5-dff9-4efa-8dc9-36ad232420fe': {'page_label': '17', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '38306229-5b08-4b07-aec7-ea8d51ba2d12': {'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '91adf3e8-8679-4a1e-b463-1692d988032e': {'page_label': '18', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '6fb51d16-06c7-4a9d-b51b-9d83a134b7ef': {'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}, '2e00c6a4-0949-43d3-af36-dda258735aa5': {'page_label': '19', 'file_name': '1602_LongLoRA_Efficient_Fine_t.pdf', 'file_path': '/Users/tanmaydhote/Downloads/1602_LongLoRA_Efficient_Fine_t.pdf', 'file_type': 'application/pdf', 'file_size': 1168720, 'creation_date': '2025-11-19', 'last_modified_date': '2025-11-19'}}), is_error=False), return_direct=False)]\n",
            "  Tool 1: summary_tool_6283_Self_RAG_Learning_to_Retr with args: {'input': 'Self-RAG'}\n",
            "  Tool output: Self-Reflective Retrieval-Augmented Generation (SELF-RAG) is a framework designed to enhance the quality and factual accuracy of large language models (LLMs) by incorporating retrieval and self-reflection. It trains an LLM to retrieve relevant passages on-demand and generate outputs while reflecting on the quality and factual support of these outputs using special tokens called reflection tokens. This approach allows the model to adaptively decide when retrieval is necessary and to critique its own outputs, improving overall generation quality, factuality, and citation accuracy. SELF-RAG outperforms other models, including ChatGPT, in various tasks by providing more accurate and verifiable outputs.\n",
            "  Tool 2: summary_tool_1602_LongLoRA_Efficient_Fine_t with args: {'input': 'LongLoRA'}\n",
            "  Tool output: LongLoRA is an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It addresses the challenge of training LLMs with long context sizes, which is typically resource-intensive. LongLoRA achieves this by employing shifted sparse attention (S2-Attn) during training, which allows for context extension while maintaining computational efficiency. This method retains the original attention architecture during inference, ensuring compatibility with existing optimization techniques. Additionally, LongLoRA incorporates an improved version of low-rank adaptation (LoRA) by making embedding and normalization layers trainable, which is crucial for effective long context adaptation. This approach allows models like Llama2 to be extended to significantly larger context lengths on standard hardware setups.\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nprint(\"HANDLER INFORMATION:\")\\nprint(\"=\" * 60)\\nprint(f\"Handler type: {type(handler)}\")\\nprint(f\"Handler context available: {hasattr(handler, \\'ctx\\')}\")\\nprint()\\n\\n\\n# You can also access memory to see the full conversation\\nprint(\"=\" * 60)\\nprint(\"CONVERSATION HISTORY (from memory):\")\\nprint(\"=\" * 60)\\nmessages = await chat_memory.aget()\\nfor i, msg in enumerate(messages):\\n    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\\n'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"Give me a summary of both Self-RAG and LongLoRA\",\n",
        "    memory=chat_memory,\n",
        ")\n",
        "response = await handler\n",
        "\n",
        "# Explore what's available in the response object\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE CONTENT:\")\n",
        "print(\"=\" * 60)\n",
        "print(str(response))  # or response.response.content\n",
        "print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE OBJECT PROPERTIES:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Response type: {type(response)}\")\n",
        "print(f\"Agent name: {response.current_agent_name}\")\n",
        "print(f\"Response message: {response.response}\")\n",
        "print(f\"Response content: {response.response.content}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Number of tool calls: {len(response.tool_calls)}\")\n",
        "if response.tool_calls:\n",
        "    print(f\"Tool calls: {response.tool_calls}\")\n",
        "    for i, tool_call in enumerate(response.tool_calls):\n",
        "        print(f\"  Tool {i+1}: {tool_call.tool_name} with args: {tool_call.tool_kwargs}\")\n",
        "        if hasattr(tool_call, \"tool_output\"):\n",
        "            print(f\"  Tool output: {tool_call.tool_output}\")\n",
        "print()\n",
        "\n",
        "# Access the handler to see execution details\n",
        "print(\"=\" * 60)\n",
        "\"\"\"\n",
        "print(\"HANDLER INFORMATION:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Handler type: {type(handler)}\")\n",
        "print(f\"Handler context available: {hasattr(handler, 'ctx')}\")\n",
        "print()\n",
        "\n",
        "\n",
        "# You can also access memory to see the full conversation\n",
        "print(\"=\" * 60)\n",
        "print(\"CONVERSATION HISTORY (from memory):\")\n",
        "print(\"=\" * 60)\n",
        "messages = await chat_memory.aget()\n",
        "for i, msg in enumerate(messages):\n",
        "    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-19 16:35:43,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESPONSE CONTENT:\n",
            "============================================================\n",
            "Self-RAG employs datasets for reasoning, long-form generation, and retrieval accuracy analysis, such as PubHealth and PopQA, whereas LongLoRA uses the LongAlpaca dataset for long instruction-following tasks in supervised fine-tuning.\n",
            "\n",
            "============================================================\n",
            "RESPONSE OBJECT PROPERTIES:\n",
            "============================================================\n",
            "Response type: <class 'llama_index.core.agent.workflow.workflow_events.AgentOutput'>\n",
            "Agent name: Agent\n",
            "Response message: assistant: Self-RAG employs datasets for reasoning, long-form generation, and retrieval accuracy analysis, such as PubHealth and PopQA, whereas LongLoRA uses the LongAlpaca dataset for long instruction-following tasks in supervised fine-tuning.\n",
            "Response content: Self-RAG employs datasets for reasoning, long-form generation, and retrieval accuracy analysis, such as PubHealth and PopQA, whereas LongLoRA uses the LongAlpaca dataset for long instruction-following tasks in supervised fine-tuning.\n",
            "============================================================\n",
            "Number of tool calls: 0\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nprint(\"HANDLER INFORMATION:\")\\nprint(\"=\" * 60)\\nprint(f\"Handler type: {type(handler)}\")\\nprint(f\"Handler context available: {hasattr(handler, \\'ctx\\')}\")\\nprint()\\n\\n\\n# You can also access memory to see the full conversation\\nprint(\"=\" * 60)\\nprint(\"CONVERSATION HISTORY (from memory):\")\\nprint(\"=\" * 60)\\nmessages = await chat_memory.aget()\\nfor i, msg in enumerate(messages):\\n    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\\n'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = agent.run(\n",
        "    user_msg=\"Compare the datasets used in Self-RAG and LongLoRA. Give the answer in one sentence.\",\n",
        "    memory=chat_memory,\n",
        ")\n",
        "response = await handler\n",
        "\n",
        "# Explore what's available in the response object\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE CONTENT:\")\n",
        "print(\"=\" * 60)\n",
        "print(str(response))  # or response.response.content\n",
        "print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESPONSE OBJECT PROPERTIES:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Response type: {type(response)}\")\n",
        "print(f\"Agent name: {response.current_agent_name}\")\n",
        "print(f\"Response message: {response.response}\")\n",
        "print(f\"Response content: {response.response.content}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Number of tool calls: {len(response.tool_calls)}\")\n",
        "if response.tool_calls:\n",
        "    print(f\"Tool calls: {response.tool_calls}\")\n",
        "    for i, tool_call in enumerate(response.tool_calls):\n",
        "        print(f\"  Tool {i+1}: {tool_call.tool_name} with args: {tool_call.tool_kwargs}\")\n",
        "        if hasattr(tool_call, \"tool_output\"):\n",
        "            print(f\"  Tool output: {tool_call.tool_output}\")\n",
        "print()\n",
        "\n",
        "# Access the handler to see execution details\n",
        "print(\"=\" * 60)\n",
        "\"\"\"\n",
        "print(\"HANDLER INFORMATION:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Handler type: {type(handler)}\")\n",
        "print(f\"Handler context available: {hasattr(handler, 'ctx')}\")\n",
        "print()\n",
        "\n",
        "\n",
        "# You can also access memory to see the full conversation\n",
        "print(\"=\" * 60)\n",
        "print(\"CONVERSATION HISTORY (from memory):\")\n",
        "print(\"=\" * 60)\n",
        "messages = await chat_memory.aget()\n",
        "for i, msg in enumerate(messages):\n",
        "    print(f\"Message {i+1} ({msg.role}): {msg.content[:100]}...\")\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
