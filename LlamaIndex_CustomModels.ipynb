{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhnKf0iVR0h_"
      },
      "outputs": [],
      "source": [
        "import os, requests\n",
        "from typing import Any, ClassVar\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "\n",
        "\n",
        "class OpenAIResponsesLLM(CustomLLM):\n",
        "    \"\"\"\n",
        "    Wrapper for the preview `/v1/responses` endpoint.\n",
        "    Handles the three payload shapes seen so far:\n",
        "      • {\"choices\": …, \"message\": …}   (chat-like)\n",
        "      • {\"choices\": …, \"delta\": …}     (chat stream)\n",
        "      • {\"output\":  […{\"content\": […{\"type\": \"output_text\"} ] } ]}\n",
        "    \"\"\"\n",
        "\n",
        "    # mark as `ClassVar` so pydantic ignores them\n",
        "    context_window: ClassVar[int] = 16_384\n",
        "    num_output:     ClassVar[int] = 512\n",
        "    model_name:     ClassVar[str] = \"gpt-4o\"\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:                    # noqa: D401\n",
        "        return LLMMetadata(\n",
        "            context_window=self.context_window,\n",
        "            num_output=self.num_output,\n",
        "            model_name=self.model_name,\n",
        "        )\n",
        "\n",
        "    # ---------- blocking completion ----------\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
        "        url = \"https://api.openai.com/v1/responses\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer 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\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"input\": [\n",
        "                {\"role\": \"user\",\n",
        "                 \"content\": [{\"type\": \"input_text\", \"text\": prompt}]}\n",
        "            ],\n",
        "            \"max_output_tokens\": kwargs.get(\"max_tokens\", self.num_output),\n",
        "            \"temperature\":      kwargs.get(\"temperature\", 0.7),\n",
        "        }\n",
        "\n",
        "        r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "\n",
        "        # -------- unified extraction ----------\n",
        "        text = \"\"\n",
        "\n",
        "        # 1) chat/completions-style\n",
        "        if data.get(\"choices\"):\n",
        "            choice = data[\"choices\"][0]\n",
        "            if (msg := choice.get(\"message\")):\n",
        "                text = msg.get(\"content\", \"\")\n",
        "            elif (delta := choice.get(\"delta\")):\n",
        "                text = delta.get(\"content\", \"\")\n",
        "\n",
        "        # 2) new \"output\" array shape\n",
        "        elif data.get(\"output\"):\n",
        "            first = data[\"output\"][0]           # one assistant msg\n",
        "            if first.get(\"content\"):\n",
        "                for part in first[\"content\"]:\n",
        "                    # look for the output_text segment\n",
        "                    if part.get(\"type\") in (\"output_text\", \"text\"):\n",
        "                        text = part.get(\"text\", \"\")\n",
        "                        break\n",
        "\n",
        "        # 3) very early beta shape\n",
        "        elif \"output_text\" in data:\n",
        "            text = data[\"output_text\"]\n",
        "\n",
        "        if not text:\n",
        "            raise ValueError(f\"Unrecognised /v1/responses payload:\\n{data}\")\n",
        "        # --------------------------------------\n",
        "\n",
        "        return CompletionResponse(text=text)\n",
        "\n",
        "    # ---------- streaming (single-chunk stub) ----------\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
        "        full = self.complete(prompt, **kwargs).text\n",
        "        yield CompletionResponse(text=full, delta=full)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59e8f047",
        "outputId": "d2182626-c3a0-4221-d9da-d77e3663eb11"
      },
      "source": [
        "%pip install llama_index\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.11/dist-packages (0.12.45)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.12)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.45 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.12.46)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.7.7)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.9)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama_index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.93.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (1.0.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.45->llama_index) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.45->llama_index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud==0.1.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (0.1.26)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.26->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.6.15)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (4.13.4)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (5.7.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama_index) (0.6.34)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.45->llama_index) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.45->llama_index) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.45->llama_index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.45->llama_index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.45->llama_index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.45->llama_index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.45->llama_index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.45->llama_index) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.45->llama_index) (0.2.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.32 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (0.6.34)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.45->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.45->llama_index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.45->llama_index) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.45->llama_index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.45->llama_index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.45->llama_index) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.45->llama_index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.45->llama_index) (3.26.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.1.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.45->llama_index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.45->llama_index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.45->llama_index) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests\n",
        "from typing import Any, List, Optional\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from pydantic import FieldValidationInfo\n",
        "\n",
        "\n",
        "class OpenAI3SmallEmbeddings(BaseEmbedding):\n",
        "    model_config = {\"extra\": \"allow\"}     # <-- add this\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"text-embedding-3-small\",\n",
        "        api_key: Optional[str] = None,\n",
        "        dimensions: Optional[int] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "        self.endpoint = \"https://api.openai.com/v1/embeddings\"\n",
        "        self.dimensions = dimensions            # e.g. 512 to shorten vectors\n",
        "\n",
        "    # ---------- internal helper ----------\n",
        "    def _embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer 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\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        payload: dict[str, Any] = {\"model\": self.model_name, \"input\": texts}\n",
        "        if self.dimensions:\n",
        "            payload[\"dimensions\"] = self.dimensions\n",
        "\n",
        "        r = requests.post(self.endpoint, headers=headers, json=payload)\n",
        "        r.raise_for_status()\n",
        "        return [item[\"embedding\"] for item in r.json()[\"data\"]]\n",
        "\n",
        "    # ---------- synchronous API ----------\n",
        "    def _get_text_embedding(self, text: str) -> List[float]:\n",
        "        return self._embed([text])[0]\n",
        "\n",
        "    def _get_query_embedding(self, query: str) -> List[float]:\n",
        "        return self._get_text_embedding(query)\n",
        "\n",
        "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self._embed(texts)\n",
        "\n",
        "    # ---------- asynchronous API ----------\n",
        "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
        "        return self._get_text_embedding(text)\n",
        "\n",
        "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
        "        return self._get_query_embedding(query)\n",
        "\n",
        "    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self._get_text_embeddings(texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d0o4npQTc7G",
        "outputId": "013edc38-7893-4fad-d8a0-12d12c7e38a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-181424562.py:4: DeprecationWarning: Importing FieldValidationInfo from `pydantic` is deprecated. This feature is either no longer supported, or is not public.\n",
            "  from pydantic import FieldValidationInfo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -O data/paul_graham_essay.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RFAvAVTZOKc",
        "outputId": "0dc9592e-896d-44d5-dbac-1962a9ed5851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-03 03:48:47--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham_essay.txt’\n",
            "\n",
            "data/paul_graham_es 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-03 03:48:48 (4.47 MB/s) - ‘data/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "\n",
        "Settings.embed_model = OpenAI3SmallEmbeddings()\n",
        "\n",
        "llm = OpenAIResponsesLLM()\n",
        "Settings.llm = llm\n",
        "\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# build index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# build / reuse your query_engine\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=8,          # tweak how many chunks come back\n",
        "    # any other kwargs…\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=8)  # adjust k here\n",
        "resp = query_engine.query(\"What is mentioned about Sam Altman\")\n",
        "\n",
        "print(\"=== ANSWER ===\")\n",
        "print(resp)\n",
        "\n",
        "print(\"\\n=== CHUNKS USED ===\")\n",
        "for i, node_with_score in enumerate(resp.source_nodes, 1):\n",
        "    node   = node_with_score.node\n",
        "    score  = node_with_score.score          # cosine-sim in embedding space\n",
        "    text   = node.text.replace(\"\\n\", \" \")   # single-line for brevity\n",
        "    print(f\"{i:02d} | score={score:.4f} | {text[:120]}…\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVXTGZkFXlTW",
        "outputId": "3cbdbb40-63c3-4142-d4ff-321a468ec02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANSWER ===\n",
            "Sam Altman is mentioned as one of the impressive members of the first batch of Y Combinator startups. He later became the second president of YC after Paul Graham decided to step down. Initially, Sam said no to the offer because he wanted to start a startup to make nuclear reactors, but he eventually agreed in October 2013.\n",
            "\n",
            "=== CHUNKS USED ===\n",
            "01 | score=0.4219 | But while I continued to work a good deal in Arc, I gradually stopped working on Arc, partly because I didn't have time …\n",
            "02 | score=0.4183 | [13]  Once again, ignorance worked in our favor. We had no idea how to be angel investors, and in Boston in 2005 there w…\n",
            "03 | score=0.3767 | Publishing online means you treat the online version as the (or at least a) primary version.  [12] There is a general le…\n",
            "04 | score=0.3681 | So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees y…\n",
            "05 | score=0.3530 | Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting …\n",
            "06 | score=0.3476 | They were an impressive group. That first batch included reddit, Justin Kan and Emmett Shear, who went on to found Twitc…\n",
            "07 | score=0.3437 | What I Worked On  February 2021  Before college the two main things I worked on, outside of school, were writing and pro…\n",
            "08 | score=0.3353 | Meanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me w…\n"
          ]
        }
      ]
    }
  ]
}