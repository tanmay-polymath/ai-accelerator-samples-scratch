{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhnKf0iVR0h_"
      },
      "outputs": [],
      "source": [
        "import os, requests\n",
        "from typing import Any, ClassVar\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "\n",
        "\n",
        "class OpenAIResponsesLLM(CustomLLM):\n",
        "    \"\"\"\n",
        "    Wrapper for the preview `/v1/responses` endpoint.\n",
        "    Handles the three payload shapes seen so far:\n",
        "      • {\"choices\": …, \"message\": …}   (chat-like)\n",
        "      • {\"choices\": …, \"delta\": …}     (chat stream)\n",
        "      • {\"output\":  […{\"content\": […{\"type\": \"output_text\"} ] } ]}\n",
        "    \"\"\"\n",
        "\n",
        "    # mark as `ClassVar` so pydantic ignores them\n",
        "    context_window: ClassVar[int] = 16_384\n",
        "    num_output:     ClassVar[int] = 512\n",
        "    model_name:     ClassVar[str] = \"gpt-4o\"\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:                    # noqa: D401\n",
        "        return LLMMetadata(\n",
        "            context_window=self.context_window,\n",
        "            num_output=self.num_output,\n",
        "            model_name=self.model_name,\n",
        "        )\n",
        "\n",
        "    # ---------- blocking completion ----------\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
        "        url = \"https://api.openai.com/v1/responses\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer 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\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"input\": [\n",
        "                {\"role\": \"user\",\n",
        "                 \"content\": [{\"type\": \"input_text\", \"text\": prompt}]}\n",
        "            ],\n",
        "            \"max_output_tokens\": kwargs.get(\"max_tokens\", self.num_output),\n",
        "            \"temperature\":      kwargs.get(\"temperature\", 0.7),\n",
        "        }\n",
        "\n",
        "        r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "\n",
        "        # -------- unified extraction ----------\n",
        "        text = \"\"\n",
        "\n",
        "        # 1) chat/completions-style\n",
        "        if data.get(\"choices\"):\n",
        "            choice = data[\"choices\"][0]\n",
        "            if (msg := choice.get(\"message\")):\n",
        "                text = msg.get(\"content\", \"\")\n",
        "            elif (delta := choice.get(\"delta\")):\n",
        "                text = delta.get(\"content\", \"\")\n",
        "\n",
        "        # 2) new \"output\" array shape\n",
        "        elif data.get(\"output\"):\n",
        "            first = data[\"output\"][0]           # one assistant msg\n",
        "            if first.get(\"content\"):\n",
        "                for part in first[\"content\"]:\n",
        "                    # look for the output_text segment\n",
        "                    if part.get(\"type\") in (\"output_text\", \"text\"):\n",
        "                        text = part.get(\"text\", \"\")\n",
        "                        break\n",
        "\n",
        "        # 3) very early beta shape\n",
        "        elif \"output_text\" in data:\n",
        "            text = data[\"output_text\"]\n",
        "\n",
        "        if not text:\n",
        "            raise ValueError(f\"Unrecognised /v1/responses payload:\\n{data}\")\n",
        "        # --------------------------------------\n",
        "\n",
        "        return CompletionResponse(text=text)\n",
        "\n",
        "    # ---------- streaming (single-chunk stub) ----------\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
        "        full = self.complete(prompt, **kwargs).text\n",
        "        yield CompletionResponse(text=full, delta=full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59e8f047",
        "outputId": "d2182626-c3a0-4221-d9da-d77e3663eb11"
      },
      "outputs": [],
      "source": [
        "%pip install llama_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d0o4npQTc7G",
        "outputId": "013edc38-7893-4fad-d8a0-12d12c7e38a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-14-181424562.py:4: DeprecationWarning: Importing FieldValidationInfo from `pydantic` is deprecated. This feature is either no longer supported, or is not public.\n",
            "  from pydantic import FieldValidationInfo\n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from typing import Any, List, Optional\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from pydantic import FieldValidationInfo\n",
        "\n",
        "\n",
        "class OpenAI3SmallEmbeddings(BaseEmbedding):\n",
        "    model_config = {\"extra\": \"allow\"}     # <-- add this\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"text-embedding-3-small\",\n",
        "        api_key: Optional[str] = None,\n",
        "        dimensions: Optional[int] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "        self.endpoint = \"https://api.openai.com/v1/embeddings\"\n",
        "        self.dimensions = dimensions            # e.g. 512 to shorten vectors\n",
        "\n",
        "    # ---------- internal helper ----------\n",
        "    def _embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer add your key here\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        payload: dict[str, Any] = {\"model\": self.model_name, \"input\": texts}\n",
        "        if self.dimensions:\n",
        "            payload[\"dimensions\"] = self.dimensions\n",
        "\n",
        "        r = requests.post(self.endpoint, headers=headers, json=payload)\n",
        "        r.raise_for_status()\n",
        "        return [item[\"embedding\"] for item in r.json()[\"data\"]]\n",
        "\n",
        "    # ---------- synchronous API ----------\n",
        "    def _get_text_embedding(self, text: str) -> List[float]:\n",
        "        return self._embed([text])[0]\n",
        "\n",
        "    def _get_query_embedding(self, query: str) -> List[float]:\n",
        "        return self._get_text_embedding(query)\n",
        "\n",
        "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self._embed(texts)\n",
        "\n",
        "    # ---------- asynchronous API ----------\n",
        "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
        "        return self._get_text_embedding(text)\n",
        "\n",
        "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
        "        return self._get_query_embedding(query)\n",
        "\n",
        "    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self._get_text_embeddings(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RFAvAVTZOKc",
        "outputId": "0dc9592e-896d-44d5-dbac-1962a9ed5851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-03 03:48:47--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham_essay.txt’\n",
            "\n",
            "data/paul_graham_es 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-03 03:48:48 (4.47 MB/s) - ‘data/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -O data/paul_graham_essay.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVXTGZkFXlTW",
        "outputId": "3cbdbb40-63c3-4142-d4ff-321a468ec02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ANSWER ===\n",
            "Sam Altman is mentioned as one of the impressive members of the first batch of Y Combinator startups. He later became the second president of YC after Paul Graham decided to step down. Initially, Sam said no to the offer because he wanted to start a startup to make nuclear reactors, but he eventually agreed in October 2013.\n",
            "\n",
            "=== CHUNKS USED ===\n",
            "01 | score=0.4219 | But while I continued to work a good deal in Arc, I gradually stopped working on Arc, partly because I didn't have time …\n",
            "02 | score=0.4183 | [13]  Once again, ignorance worked in our favor. We had no idea how to be angel investors, and in Boston in 2005 there w…\n",
            "03 | score=0.3767 | Publishing online means you treat the online version as the (or at least a) primary version.  [12] There is a general le…\n",
            "04 | score=0.3681 | So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees y…\n",
            "05 | score=0.3530 | Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting …\n",
            "06 | score=0.3476 | They were an impressive group. That first batch included reddit, Justin Kan and Emmett Shear, who went on to found Twitc…\n",
            "07 | score=0.3437 | What I Worked On  February 2021  Before college the two main things I worked on, outside of school, were writing and pro…\n",
            "08 | score=0.3353 | Meanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me w…\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "\n",
        "Settings.embed_model = OpenAI3SmallEmbeddings()\n",
        "\n",
        "llm = OpenAIResponsesLLM()\n",
        "Settings.llm = llm\n",
        "\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# build index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# build / reuse your query_engine\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=8,          # tweak how many chunks come back\n",
        "    # any other kwargs…\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=8)  # adjust k here\n",
        "resp = query_engine.query(\"What is mentioned about Sam Altman\")\n",
        "\n",
        "print(\"=== ANSWER ===\")\n",
        "print(resp)\n",
        "\n",
        "print(\"\\n=== CHUNKS USED ===\")\n",
        "for i, node_with_score in enumerate(resp.source_nodes, 1):\n",
        "    node   = node_with_score.node\n",
        "    score  = node_with_score.score          # cosine-sim in embedding space\n",
        "    text   = node.text.replace(\"\\n\", \" \")   # single-line for brevity\n",
        "    print(f\"{i:02d} | score={score:.4f} | {text[:120]}…\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
